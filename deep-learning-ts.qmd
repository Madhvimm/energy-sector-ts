---
title: "Deep Learning for TS"
output: html

---




```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false

library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)

```





```{python, warning=FALSE, message=FALSE, echo=FALSE}


#!pip install scikit-learn 
#!pip install tensorflow
#!pip install plotly
#!pip install statsmodels 
#!pip install IPython 
#!pip install matplotlib 
#!pip install seaborn 
#!pip install jupyter 
#!pip install scikeras

import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import pandas as pd 
import plotly.express as px 
import statsmodels.api as sm 
from IPython.display import IFrame
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error 
from tensorflow import keras 
from tensorflow.keras import layers 
from tensorflow.keras import initializers 
from tensorflow.keras import regularizers 
from keras.layers import Dense, SimpleRNN, LSTM, GRU, Dropout
# Import L1 and L2 regularizers from Python
import tensorflow.keras.regularizers
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Input
from keras.optimizers import RMSprop

from keras.callbacks import EarlyStopping
from keras.layers import LSTM, Dropout
from scikeras.wrappers import KerasRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import RandomizedSearchCV

```


```{python, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Read data'
#| warning: false
#| output: false


composite_crude_oil_prices = pd.read_csv("data/composite_crude_oil_prices.csv")

citygate_gas_prices = pd.read_csv("data/citygate_gas_prices.csv")

total_electricity_prices = pd.read_csv("data/total_electricity_prices.csv")

gdp_data = pd.read_csv("data/gdp_data.csv")

cpi_data = pd.read_csv("data/cpi_data.csv")

```



```{python, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'convert to ts'
#| warning: false
#| output: false


# For Crude Oil
composite_crude_oil_prices['Date'] = pd.to_datetime(composite_crude_oil_prices['Date'])
start_year = min(composite_crude_oil_prices['Date']).year
start_month = min(composite_crude_oil_prices['Date']).month
composite_crude_oil_ts = pd.Series(composite_crude_oil_prices['Value'].values, index=pd.date_range(start=f'{start_year}-{start_month}', periods=len(composite_crude_oil_prices), freq='ME'))
# Log-transform Value
composite_crude_oil_prices['LOG_Value'] = np.log(composite_crude_oil_prices['Value'])
oil_log_ts = pd.Series(composite_crude_oil_prices['LOG_Value'].values, index=pd.date_range(start=f'{start_year}-{start_month}', periods=len(composite_crude_oil_prices), freq='ME'))

# For Natural Gas
citygate_gas_prices['Date'] = pd.to_datetime(citygate_gas_prices['Date'])
start_year_gas = min(citygate_gas_prices['Date']).year
start_month_gas = min(citygate_gas_prices['Date']).month
citygate_gas_ts = pd.Series(citygate_gas_prices['Value'].values, index=pd.date_range(start=f'{start_year_gas}-{start_month_gas}', periods=len(citygate_gas_prices), freq='ME'))
# Log-transform Value
citygate_gas_prices['LOG_Value'] = np.log(citygate_gas_prices['Value'])
gas_log_ts = pd.Series(citygate_gas_prices['LOG_Value'].values, index=pd.date_range(start=f'{start_year_gas}-{start_month_gas}', periods=len(citygate_gas_prices), freq='ME'))

# For Electricity
total_electricity_prices['Date'] = pd.to_datetime(total_electricity_prices['Date'])
start_year_elec = min(total_electricity_prices['Date']).year
start_month_elec = min(total_electricity_prices['Date']).month
total_electricity_ts = pd.Series(total_electricity_prices['Value'].values, index=pd.date_range(start=f'{start_year_elec}-{start_month_elec}', periods=len(total_electricity_prices), freq='ME'))
# Log-transform Value
total_electricity_prices['LOG_Value'] = np.log(total_electricity_prices['Value'])
electricity_log_ts = pd.Series(total_electricity_prices['LOG_Value'].values, index=pd.date_range(start=f'{start_year_elec}-{start_month_elec}', periods=len(total_electricity_prices), freq='ME'))

# For GDP (it's quarterly)
gdp_data['DATE'] = pd.to_datetime(gdp_data['DATE'])
start_year_gdp = min(gdp_data['DATE']).year
start_quarter_gdp = min(gdp_data['DATE']).quarter
# Log-transform GDP
gdp_data['LOG_GDP'] = np.log(gdp_data['GDP'])
gdp_log_ts = pd.Series(gdp_data['LOG_GDP'].values, index=pd.date_range(start=f'{start_year_gdp}-Q{start_quarter_gdp}', periods=len(gdp_data), freq='QE'))

# For CPI (it's monthly)
cpi_data['DATE'] = pd.to_datetime(cpi_data['DATE'])
start_year_cpi = min(cpi_data['DATE']).year
start_month_cpi = min(cpi_data['DATE']).month
# Log-transform CPI
cpi_data['LOG_CPI'] = np.log(cpi_data['CPIAUCSL'])
cpi_log_ts = pd.Series(cpi_data['LOG_CPI'].values, index=pd.date_range(start=f'{start_year_cpi}-{start_month_cpi}', periods=len(cpi_data), freq='ME'))


```



# 1. Introduction


Time series forecasting plays a pivotal role in various domains, including economics, finance, and business operations. Accurately predicting future trends based on historical data can provide valuable insights for decision-making and strategic planning. Traditionally, statistical methods such as Autoregressive Integrated Moving Average (ARIMA) models have been widely employed for time series analysis. However, these conventional approaches often struggle to capture intricate patterns, non-linear relationships, and long-term dependencies present in complex time series data.

In recent years, deep learning techniques have emerged as powerful alternatives, offering new avenues for time series forecasting. Neural networks, with their ability to learn from data and uncover intricate patterns, have shown remarkable potential in handling sequential data and capturing temporal dependencies. This exploration focuses on three prominent deep learning architectures: Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Gated Recurrent Units (GRUs), each designed to tackle the unique challenges of time series data.

The objective of this study is to evaluate the performance of these deep learning models in forecasting [specify the time series data or domain, e.g., energy prices, stock market trends, or sales data]. By leveraging their capability to handle complex non-linear relationships and long-term dependencies, we aim to enhance our forecasting accuracy and extend the predictive horizon compared to traditional methods.

Furthermore, this exploration will provide insights into the relative strengths and weaknesses of each neural network architecture, informing future decisions on selecting the most suitable approach for specific time series forecasting tasks. We will thoroughly analyze and compare the accuracy, predictive power, and generalization capabilities of RNNs, LSTMs, and GRUs, offering a comprehensive understanding of their applicability in the realm of time series analysis.

By embracing deep learning techniques, we strive to push the boundaries of time series forecasting, enabling more informed decision-making and strategic planning across various industries and domains. This study will contribute to the growing body of knowledge in the field, advancing our understanding of the potential and limitations of deep learning for time series analysis.



# 2. Data Visualization

To better understand the patterns in our data, we will first visualize it to examine any trends, seasonality, or anomalies. The visualization helps provide insights into how this time series behaves over time.


::: panel-tabset

## Crude Oil

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true



oil_plot = px.line(composite_crude_oil_prices, x='Date', y='Value',
                   title='Crude Oil Prices Over Time',
                   labels={'Date': 'Date', 'Value': 'Crude Oil Prices'},
                   template='plotly_white', 
                   color_discrete_sequence=['#99494d'])

oil_plot
```



## Natural Gas


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

ng_plot = px.line(citygate_gas_prices, x='Date', y='Value',
                  title='Natural Gas Price Over Time',
                  labels={'Date': 'Date', 'Value': 'Price (USD)'},
                  template='plotly_white',  
                  color_discrete_sequence=['#99494d'])

# Display the plot
ng_plot


```



## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot Electricity Prices'
#| warning: false
#| output: true

electricity_plot = px.line(total_electricity_prices, x='Date', y='Value',
                           title='Average Price of Electricity to Ultimate Customers',
                           labels={'Date': 'Date', 'Value': 'Price (Cents per Kilowatthour)'},
                           template='plotly_white',  
                           color_discrete_sequence=['#99494d'])

# Display the plot
electricity_plot.show()


```



## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot GDP'
#| warning: false
#| output: true

gdp_plot = px.line(gdp_data, x='DATE', y='GDP',
                   title='Gross Domestic Product Over Time',
                   labels={'DATE': 'Date', 'GDP': 'GDP (Billion USD)'},
                   template='plotly_white', 
                   color_discrete_sequence=['#99494d'])

# Display the plot
gdp_plot.show()



```


## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot CPI'
#| warning: false
#| output: true

cpi_plot = px.line(cpi_data, x='DATE', y='CPIAUCSL',
                   title='Consumer Price Index Over Time',
                   labels={'DATE': 'Date', 'CPIAUCSL': 'CPI'},
                   template='plotly_white',  # Using the 'plotly_white' as a minimal theme
                   color_discrete_sequence=['#99494d'])

# Display the plot
cpi_plot.show()

```




:::





# 3. Normalize & Split the Data 

To improve the accuracy of our models, we will normalize the time series data and split it into training and testing sets. Normalizing helps align the data distributions and prepares the data for training. We'll use a common split ratio of 80% training and 20% testing.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# Normalize the crude oil data
oil_normalized = (composite_crude_oil_prices['Value'] - composite_crude_oil_prices['Value'].mean()) / composite_crude_oil_prices['Value'].std()

# Split data into training and testing
split_index = int(0.8 * len(oil_normalized))
oil_train = oil_normalized[:split_index]
oil_test = oil_normalized[split_index:]

# Print the shapes
print("Shape of the Crude Oil Data:", len(oil_normalized))
print("Shape of the Crude Oil Train Data:", len(oil_train))
print("Shape of the Crude Oil Test Data:", len(oil_test))

# Prepare data for plotting
train_dates = composite_crude_oil_prices['Date'].iloc[:split_index]
test_dates = composite_crude_oil_prices['Date'].iloc[split_index:]

oil_data = pd.DataFrame({
    'Date': pd.concat([train_dates, test_dates]),
    'Value': pd.concat([oil_train, oil_test]),
    'Set': ['Training'] * len(oil_train) + ['Testing'] * len(oil_test)
})

# Plot the training and testing sets
oil_plot1 = px.area(oil_data, x='Date', y='Value', color='Set', title='Crude Oil Training and Testing Sets',
                    labels={'Value': 'Normalized Crude Oil Prices'}, template='plotly_white')
oil_plot1.update_layout(title_x=0.5)


```



## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# Normalize the natural gas data
gas_normalized = (citygate_gas_prices['Value'] - citygate_gas_prices['Value'].mean()) / citygate_gas_prices['Value'].std()

# Split data into training and testing
split_index = int(0.8 * len(gas_normalized))
gas_train = gas_normalized[:split_index]
gas_test = gas_normalized[split_index:]

# Print the shapes
print("Shape of the Natural Gas Data:", len(gas_normalized))
print("Shape of the Natural Gas Train Data:", len(gas_train))
print("Shape of the Natural Gas Test Data:", len(gas_test))

# Prepare data for plotting
train_dates_gas = citygate_gas_prices['Date'].iloc[:split_index]
test_dates_gas = citygate_gas_prices['Date'].iloc[split_index:]

gas_data = pd.DataFrame({
    'Date': pd.concat([train_dates_gas, test_dates_gas]),
    'Value': pd.concat([gas_train, gas_test]),
    'Set': ['Training'] * len(gas_train) + ['Testing'] * len(gas_test)
})

# Plot the training and testing sets
gas_plot = px.area(gas_data, x='Date', y='Value', color='Set', title='Natural Gas Training and Testing Sets',
                   labels={'Value': 'Normalized Natural Gas Prices'}, template='plotly_white')
gas_plot.update_layout(title_x=0.5)




```




## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# Normalize the electricity data
electricity_normalized = (total_electricity_prices['Value'] - total_electricity_prices['Value'].mean()) / total_electricity_prices['Value'].std()

# Split data into training and testing
split_index = int(0.8 * len(electricity_normalized))
electricity_train = electricity_normalized[:split_index]
electricity_test = electricity_normalized[split_index:]

# Print the shapes
print("Shape of the Electricity Prices Data:", len(electricity_normalized))
print("Shape of the Electricity Prices Train Data:", len(electricity_train))
print("Shape of the Electricity Prices Test Data:", len(electricity_test))

# Prepare data for plotting
train_dates_elec = total_electricity_prices['Date'].iloc[:split_index]
test_dates_elec = total_electricity_prices['Date'].iloc[split_index:]

elec_data = pd.DataFrame({
    'Date': pd.concat([train_dates_elec, test_dates_elec]),
    'Value': pd.concat([electricity_train, electricity_test]),
    'Set': ['Training'] * len(electricity_train) + ['Testing'] * len(electricity_test)
})

# Plot the training and testing sets
elec_plot = px.area(elec_data, x='Date', y='Value', color='Set', title='Electricity Training and Testing Sets',
                    labels={'Value': 'Normalized Electricity Prices'}, template='plotly_white')
elec_plot.update_layout(title_x=0.5)



```



## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true



# Normalize the GDP data
gdp_normalized = (gdp_data['GDP'] - gdp_data['GDP'].mean()) / gdp_data['GDP'].std()

# Split data into training and testing
split_index = int(0.8 * len(gdp_normalized))
gdp_train = gdp_normalized[:split_index]
gdp_test = gdp_normalized[split_index:]

# Print the shapes
print("Shape of the GDP Data:", len(gdp_normalized))
print("Shape of the GDP Train Data:", len(gdp_train))
print("Shape of the GDP Test Data:", len(gdp_test))

# Prepare data for plotting
train_dates_gdp = gdp_data['DATE'].iloc[:split_index]
test_dates_gdp = gdp_data['DATE'].iloc[split_index:]

gdp_data_plot = pd.DataFrame({
    'Date': pd.concat([train_dates_gdp, test_dates_gdp]),
    'Value': pd.concat([gdp_train, gdp_test]),
    'Set': ['Training'] * len(gdp_train) + ['Testing'] * len(gdp_test)
})

# Plot the training and testing sets
gdp_plot1 = px.area(gdp_data_plot, x='Date', y='Value', color='Set', title='GDP Training and Testing Sets',
                    labels={'Value': 'Normalized GDP'}, template='plotly_white')
gdp_plot1.update_layout(title_x=0.5)





```



## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# Normalize the CPI data
cpi_normalized = (cpi_data['CPIAUCSL'] - cpi_data['CPIAUCSL'].mean()) / cpi_data['CPIAUCSL'].std()

# Split data into training and testing
split_index = int(0.8 * len(cpi_normalized))
cpi_train = cpi_normalized[:split_index]
cpi_test = cpi_normalized[split_index:]

# Print the shapes
print("Shape of the CPI Data:", len(cpi_normalized))
print("Shape of the CPI Train Data:", len(cpi_train))
print("Shape of the CPI Test Data:", len(cpi_test))

# Prepare data for plotting
train_dates_cpi = cpi_data['DATE'].iloc[:split_index]
test_dates_cpi = cpi_data['DATE'].iloc[split_index:]

cpi_data_plot = pd.DataFrame({
    'Date': pd.concat([train_dates_cpi, test_dates_cpi]),
    'Value': pd.concat([cpi_train, cpi_test]),
    'Set': ['Training'] * len(cpi_train) + ['Testing'] * len(cpi_test)
})

# Plot the training and testing sets
cpi_plot1 = px.area(cpi_data_plot, x='Date', y='Value', color='Set', title='CPI Training and Testing Sets',
                    labels={'Value': 'Normalized CPI'}, template='plotly_white')
cpi_plot1.update_layout(title_x=0.5)




```


:::


# 4. Mini Batching


Mini-batching is a technique used in deep learning to efficiently train models on large datasets. Instead of processing the entire dataset at once, mini-batching involves creating small, sequential subsets of the data called "mini-batches." This approach offers several benefits, including memory efficiency, faster convergence, and the ability to use stochastic gradient descent (SGD) optimization. By processing a portion of the data at a time, mini-batching reduces the memory requirements compared to processing the entire dataset simultaneously, making it feasible to train models on large datasets.

To implement mini-batching, the dataset is typically shuffled and divided into smaller batches of a fixed size. The model is then trained iteratively, processing one mini-batch at a time. The gradients are computed for each mini-batch, and the model's parameters are updated accordingly. This process is repeated until the model has seen the entire dataset, which constitutes one epoch of training. The choice of mini-batch size is an important hyperparameter that determines the number of samples processed in each iteration, and it can impact the convergence speed and stability of the training process.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  # Subtract 1 to stay within bounds
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
oil_train = np.array(oil_train)
oil_test = np.array(oil_test)

# Create mini-batches
oil_train_batches_x, oil_train_batches_y = form_arrays(oil_train, lookback=lookback, delay=1, step=1)
oil_test_batches_x, oil_test_batches_y = form_arrays(oil_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {oil_train_batches_x.shape}, {oil_train_batches_y.shape}")
print(f"Test shape: {oil_test_batches_x.shape}, {oil_test_batches_y.shape}")

```


## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
gas_train = np.array(gas_train)
gas_test = np.array(gas_test)

# Create mini-batches
gas_train_batches_x, gas_train_batches_y = form_arrays(gas_train, lookback=lookback, delay=1, step=1)
gas_test_batches_x, gas_test_batches_y = form_arrays(gas_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {gas_train_batches_x.shape}, {gas_train_batches_y.shape}")
print(f"Test shape: {gas_test_batches_x.shape}, {gas_test_batches_y.shape}")

```


## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
electricity_train = np.array(electricity_train)
electricity_test = np.array(electricity_test)

# Create mini-batches
electricity_train_batches_x, electricity_train_batches_y = form_arrays(electricity_train, lookback=lookback, delay=1, step=1)
electricity_test_batches_x, electricity_test_batches_y = form_arrays(electricity_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {electricity_train_batches_x.shape}, {electricity_train_batches_y.shape}")
print(f"Test shape: {electricity_test_batches_x.shape}, {electricity_test_batches_y.shape}")


```


## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
gdp_train = np.array(gdp_train)
gdp_test = np.array(gdp_test)

# Create mini-batches
gdp_train_batches_x, gdp_train_batches_y = form_arrays(gdp_train, lookback=lookback, delay=1, step=1)
gdp_test_batches_x, gdp_test_batches_y = form_arrays(gdp_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {gdp_train_batches_x.shape}, {gdp_train_batches_y.shape}")
print(f"Test shape: {gdp_test_batches_x.shape}, {gdp_test_batches_y.shape}")

```


## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
cpi_train = np.array(cpi_train)
cpi_test = np.array(cpi_test)

# Create mini-batches
cpi_train_batches_x, cpi_train_batches_y = form_arrays(cpi_train, lookback=lookback, delay=1, step=1)
cpi_test_batches_x, cpi_test_batches_y = form_arrays(cpi_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {cpi_train_batches_x.shape}, {cpi_train_batches_y.shape}")
print(f"Test shape: {cpi_test_batches_x.shape}, {cpi_test_batches_y.shape}")


```




::: 




# 5. Recurrent Neural Networks (RNNs)

Recurrent Neural Networks (RNNs) are designed to process sequential data, making them ideal for time series forecasting. Unlike traditional neural networks, RNNs have a memory component that allows them to consider previous time steps, making them particularly adept at handling temporal dependencies. In this section, we will implement an RNN to forecast future trends in our data.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (oil_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 64  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])
     
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg.summary())

print("Model with regularization:")
print(model_reg.summary())

```


```{python, warning=FALSE, message=FALSE, echo=FALSE}

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_oil_no_reg = model_no_reg.fit(oil_train_batches_x,
                                      oil_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(oil_test_batches_x, oil_test_batches_y),
                                      callbacks=[early_stopping])

history_oil_reg = model_reg.fit(oil_train_batches_x,
                                oil_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(oil_test_batches_x, oil_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg, history_oil_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_oil_no_reg = model_no_reg.predict(oil_train_batches_x)
Yvp_oil_no_reg = model_no_reg.predict(oil_test_batches_x)
Ytp_oil_reg = model_reg.predict(oil_train_batches_x)
Yvp_oil_reg = model_reg.predict(oil_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg, train_mae_no_reg, val_mse_no_reg, val_mae_no_reg = regression_report(oil_train_batches_y, Ytp_oil_no_reg, oil_test_batches_y, Yvp_oil_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg, train_mae_reg, val_mse_reg, val_mae_reg = regression_report(oil_train_batches_y, Ytp_oil_reg, oil_test_batches_y, Yvp_oil_reg, "Regularized Model")
```



## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gas_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 32  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gas = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_gas = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gas.summary())

print("Model with regularization:")
print(model_reg_gas.summary())

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gas_no_reg = model_no_reg_gas.fit(gas_train_batches_x,
                                      gas_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gas_test_batches_x, gas_test_batches_y),
                                      callbacks=[early_stopping])

history_gas_reg = model_reg_gas.fit(gas_train_batches_x,
                                gas_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gas_test_batches_x, gas_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg, history_gas_reg)

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_gas_no_reg = model_no_reg_gas.predict(gas_train_batches_x)
Yvp_gas_no_reg = model_no_reg_gas.predict(gas_test_batches_x)
Ytp_gas_reg = model_reg_gas.predict(gas_train_batches_x)
Yvp_gas_reg = model_reg_gas.predict(gas_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Natural Gas Prices', ylabel='Predicted Natural Gas Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Natural Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Natural Gas Prices', ylabel='Predicted Natural Gas Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Natural Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gas, train_mae_no_reg_gas, val_mse_no_reg_gas, val_mae_no_reg_gas = regression_report(gas_train_batches_y, Ytp_gas_no_reg, gas_test_batches_y, Yvp_gas_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gas, train_mae_reg_gas, val_mse_reg_gas, val_mae_reg_gas = regression_report(gas_train_batches_y, Ytp_gas_reg, gas_test_batches_y, Yvp_gas_reg, "Regularized Model")
```



## Electricity


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 200 # Update based on the best hyperparameters
L2 = 0.01  # Update based on the best hyperparameters
input_shape = (electricity_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 128  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_elec = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_elec = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```



```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_elec.summary())

print("Model with regularization:")
print(model_reg_elec.summary())

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_elec.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_elec.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_elec_no_reg = model_no_reg_elec.fit(electricity_train_batches_x,
                                      electricity_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                      callbacks=[early_stopping])

history_elec_reg = model_reg_elec.fit(electricity_train_batches_x,
                                electricity_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_elec_no_reg, history_elec_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_elec_no_reg = model_no_reg_elec.predict(electricity_train_batches_x)
Yvp_elec_no_reg = model_no_reg_elec.predict(electricity_test_batches_x)
Ytp_elec_reg = model_reg_elec.predict(electricity_train_batches_x)
Yvp_elec_reg = model_reg_elec.predict(electricity_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_elec, train_mae_no_reg_elec, val_mse_no_reg_elec, val_mae_no_reg_elec = regression_report(electricity_train_batches_y, Ytp_elec_no_reg, electricity_test_batches_y, Yvp_elec_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_elec, train_mae_reg_elec, val_mse_reg_elec, val_mae_reg_elec = regression_report(electricity_train_batches_y, Ytp_elec_reg, electricity_test_batches_y, Yvp_elec_reg, "Regularized Model")
```




## GDP


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 50 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gdp_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 128  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 30  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gdp = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_gdp = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gdp.summary())

print("Model with regularization:")
print(model_reg_gdp.summary())

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_gdp.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gdp_no_reg = model_no_reg_gdp.fit(gdp_train_batches_x,
                                      gdp_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                      callbacks=[early_stopping])

history_gdp_reg = model_reg_gdp.fit(gdp_train_batches_x,
                                gdp_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg, history_gdp_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_gdp_no_reg = model_no_reg_gdp.predict(gdp_train_batches_x)
Yvp_gdp_no_reg = model_no_reg_gdp.predict(gdp_test_batches_x)
Ytp_gdp_reg = model_reg_gdp.predict(gdp_train_batches_x)
Yvp_gdp_reg = model_reg_gdp.predict(gdp_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gdp, train_mae_no_reg_gdp, val_mse_no_reg_gdp, val_mae_no_reg_gdp = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg, gdp_test_batches_y, Yvp_gdp_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gdp, train_mae_reg_gdp, val_mse_reg_gdp, val_mae_reg_gdp = regression_report(gdp_train_batches_y, Ytp_gdp_reg, gdp_test_batches_y, Yvp_gdp_reg, "Regularized Model")
```





## CPI


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 200 # Update based on the best hyperparameters
L2 = 0.01  # Update based on the best hyperparameters
input_shape = (cpi_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 64  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_cpi = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_cpi = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_cpi.summary())

print("Model with regularization:")
print(model_reg_cpi.summary())

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_cpi.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_cpi_no_reg = model_no_reg_cpi.fit(cpi_train_batches_x,
                                      cpi_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                      callbacks=[early_stopping])

history_cpi_reg = model_reg_cpi.fit(cpi_train_batches_x,
                                cpi_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg, history_cpi_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_cpi_no_reg = model_no_reg_cpi.predict(cpi_train_batches_x)
Yvp_cpi_no_reg = model_no_reg_cpi.predict(cpi_test_batches_x)
Ytp_cpi_reg = model_reg_cpi.predict(cpi_train_batches_x)
Yvp_cpi_reg = model_reg_cpi.predict(cpi_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual cpi', ylabel='Predicted CPI',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='cpi (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_cpi, train_mae_no_reg_cpi, val_mse_no_reg_cpi, val_mae_no_reg_cpi = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg, cpi_test_batches_y, Yvp_cpi_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_cpi, train_mae_reg_cpi, val_mse_reg_cpi, val_mae_reg_cpi = regression_report(cpi_train_batches_y, Ytp_cpi_reg, cpi_test_batches_y, Yvp_cpi_reg, "Regularized Model")
```

:::




# 6. LSTM

Long Short-Term Memory Networks (LSTMs) are a specialized kind of Recurrent Neural Network (RNN) that excel in capturing long-term dependencies within time series data, a task where traditional RNNs often falter due to issues like vanishing and exploding gradients. LSTMs are designed with a complex gating mechanism that controls the flow of information. These gates—forget gate, input gate, and output gate—determine what information should be retained or discarded as the sequence progresses, allowing LSTMs to maintain a longer memory and avoid the stability problems seen in standard RNNs.

Now, I will implement an LSTM model to address the challenges posed by the intricate patterns and non-linear relationships.


::: panel-tabset

## Crude Oil



```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.001
L2_no_reg = 0
input_shape = (oil_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 100 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_l = keras.Sequential()
model_no_reg_l.add(Input(shape=input_shape))
model_no_reg_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_l = keras.Sequential()
model_reg_l.add(Input(shape=input_shape))
model_reg_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_l.summary())
print("LSTM Model with regularization:")
print(model_reg_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_oil_no_reg_l = model_no_reg_l.fit(oil_train_batches_x,
                                      oil_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(oil_test_batches_x, oil_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_oil_reg_l = model_reg_l.fit(oil_train_batches_x,
                                oil_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(oil_test_batches_x, oil_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_l, history_oil_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_oil_no_reg_l = model_no_reg_l.predict(oil_train_batches_x)
Yvp_oil_no_reg_l = model_no_reg_l.predict(oil_test_batches_x)
Ytp_oil_reg_l = model_reg_l.predict(oil_train_batches_x)
Yvp_oil_reg_l = model_reg_l.predict(oil_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_l, train_mae_no_reg_l, val_mse_no_reg_l, val_mae_no_reg_l = regression_report(oil_train_batches_y, Ytp_oil_no_reg_l, oil_test_batches_y, Yvp_oil_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_l, train_mae_reg_l, val_mse_reg_l, val_mae_reg_l = regression_report(oil_train_batches_y, Ytp_oil_reg_l, oil_test_batches_y, Yvp_oil_reg_l, "Regularized Model")


```






## Natural Gas



```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gas_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gas_l = keras.Sequential()
model_no_reg_gas_l.add(Input(shape=input_shape))
model_no_reg_gas_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_gas_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_gas_l = keras.Sequential()
model_reg_gas_l.add(Input(shape=input_shape))
model_reg_gas_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_gas_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_gas_l.summary())
print("LSTM Model with regularization:")
print(model_reg_gas_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_gas_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gas_no_reg_l = model_no_reg_gas_l.fit(gas_train_batches_x,
                                      gas_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gas_test_batches_x, gas_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_gas_reg_l = model_reg_gas_l.fit(gas_train_batches_x,
                                gas_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gas_test_batches_x, gas_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg_l, history_gas_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_gas_no_reg_l = model_no_reg_gas_l.predict(gas_train_batches_x)
Yvp_gas_no_reg_l = model_no_reg_gas_l.predict(gas_test_batches_x)
Ytp_gas_reg_l = model_reg_gas_l.predict(gas_train_batches_x)
Yvp_gas_reg_l = model_reg_gas_l.predict(gas_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Gas Prices', ylabel='Predicted Gas Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Gas Prices', ylabel='Predicted Gas Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gas_l, train_mae_no_reg_gas_l, val_mse_no_reg_gas_l, val_mae_no_reg_gas_l = regression_report(gas_train_batches_y, Ytp_gas_no_reg_l, gas_test_batches_y, Yvp_gas_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gas_l, train_mae_reg_gas_l, val_mse_reg_gas_l, val_mae_reg_gas_l = regression_report(gas_train_batches_y, Ytp_gas_reg_l, gas_test_batches_y, Yvp_gas_reg_l, "Regularized Model")


```





## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.01
L2_no_reg = 0
input_shape = (electricity_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_elec_l = keras.Sequential()
model_no_reg_elec_l.add(Input(shape=input_shape))
model_no_reg_elec_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_elec_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_elec_l = keras.Sequential()
model_reg_elec_l.add(Input(shape=input_shape))
model_reg_elec_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_elec_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_elec_l.summary())
print("LSTM Model with regularization:")
print(model_reg_elec_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_elec_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_elec_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_elec_no_reg_l = model_no_reg_elec_l.fit(electricity_train_batches_x,
                                      electricity_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_elec_reg_l = model_reg_elec_l.fit(electricity_train_batches_x,
                                electricity_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_elec_no_reg_l, history_elec_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_elec_no_reg_l = model_no_reg_elec_l.predict(electricity_train_batches_x)
Yvp_elec_no_reg_l = model_no_reg_elec_l.predict(electricity_test_batches_x)
Ytp_elec_reg_l = model_reg_elec_l.predict(electricity_train_batches_x)
Yvp_elec_reg_l = model_reg_elec_l.predict(electricity_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_elec_l, train_mae_no_reg_elec_l, val_mse_no_reg_elec_l, val_mae_no_reg_elec_l = regression_report(electricity_train_batches_y, Ytp_elec_no_reg_l, electricity_test_batches_y, Yvp_elec_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_elec_l, train_mae_reg_elec_l, val_mse_reg_elec_l, val_mae_reg_elec_l = regression_report(electricity_train_batches_y, Ytp_elec_reg_l, electricity_test_batches_y, Yvp_elec_reg_l, "Regularized Model")


```



## GDP





```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gdp_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gdp_l = keras.Sequential()
model_no_reg_gdp_l.add(Input(shape=input_shape))
model_no_reg_gdp_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_gdp_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_gdp_l = keras.Sequential()
model_reg_gdp_l.add(Input(shape=input_shape))
model_reg_gdp_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_gdp_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_gdp_l.summary())
print("LSTM Model with regularization:")
print(model_reg_gdp_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_gdp_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gdp_no_reg_l = model_no_reg_gdp_l.fit(gdp_train_batches_x,
                                      gdp_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_gdp_reg_l = model_reg_gdp_l.fit(gdp_train_batches_x,
                                gdp_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg_l, history_gdp_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_gdp_no_reg_l = model_no_reg_gdp_l.predict(gdp_train_batches_x)
Yvp_gdp_no_reg_l = model_no_reg_gdp_l.predict(gdp_test_batches_x)
Ytp_gdp_reg_l = model_reg_gdp_l.predict(gdp_train_batches_x)
Yvp_gdp_reg_l = model_reg_gdp_l.predict(gdp_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gdp_l, train_mae_no_reg_gdp_l, val_mse_no_reg_gdp_l, val_mae_no_reg_gdp_l = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg_l, gdp_test_batches_y, Yvp_gdp_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gdp_l, train_mae_reg_gdp_l, val_mse_reg_gdp_l, val_mae_reg_gdp_l = regression_report(gdp_train_batches_y, Ytp_gdp_reg_l, gdp_test_batches_y, Yvp_gdp_reg_l, "Regularized Model")


```




## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.01
L2_no_reg = 0
input_shape = (cpi_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 30 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_cpi_l = keras.Sequential()
model_no_reg_cpi_l.add(Input(shape=input_shape))
model_no_reg_cpi_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_cpi_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_cpi_l = keras.Sequential()
model_reg_cpi_l.add(Input(shape=input_shape))
model_reg_cpi_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_cpi_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_cpi_l.summary())
print("LSTM Model with regularization:")
print(model_reg_cpi_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_cpi_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_cpi_no_reg_l = model_no_reg_cpi_l.fit(cpi_train_batches_x,
                                      cpi_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_cpi_reg_l = model_reg_cpi_l.fit(cpi_train_batches_x,
                                cpi_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg_l, history_cpi_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_cpi_no_reg_l = model_no_reg_cpi_l.predict(cpi_train_batches_x)
Yvp_cpi_no_reg_l = model_no_reg_cpi_l.predict(cpi_test_batches_x)
Ytp_cpi_reg_l = model_reg_cpi_l.predict(cpi_train_batches_x)
Yvp_cpi_reg_l = model_reg_cpi_l.predict(cpi_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_cpi_l, train_mae_no_reg_cpi_l, val_mse_no_reg_cpi_l, val_mae_no_reg_cpi_l = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg_l, cpi_test_batches_y, Yvp_cpi_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_cpi_l, train_mae_reg_cpi_l, val_mse_reg_cpi_l, val_mae_reg_cpi_l = regression_report(cpi_train_batches_y, Ytp_cpi_reg_l, cpi_test_batches_y, Yvp_cpi_reg_l, "Regularized Model")


```





:::



# 7. GRU

Gated Recurrent Units (GRUs) are another variant of RNNs that are similar to LSTMs but are often favored for their simpler architectural design and efficiency in training. Like LSTMs, GRUs use gating mechanisms to control and manage the flow of information within the unit; however, they combine the forget and input gates into a single “update gate” and merge the cell state and hidden state, resulting in fewer parameters and faster training times.

Now, I will explore the application of GRUs in forecasting.


::: panel-tabset

## Crude Oil



```{python, warning=FALSE, message=FALSE, echo=FALSE}
# ############HYPERPARAMETER
# # Define the number of features
# features = 1
# 
# # Model definition function
# def get_model(optimizer="adam", units=50, L2=0.01):
#     model = Sequential()
#     model.add(GRU(units, input_shape=(lookback, features), activation='relu',
#                   recurrent_regularizer=regularizers.l2(L2)))
#     model.add(Dense(1, activation='linear'))
#     model.compile(loss="mse", optimizer=optimizer)
#     return model
# 
# # Create the SciKeras wrapper and specify training-related parameters
# model = KerasRegressor(model=get_model, model__optimizer="adam", model__units=50, model__L2=0.01)
# 
# # Define the hyperparameter space to search
# param_dist = {
#     'model__optimizer': ['adam', 'rmsprop'],
#     'model__units': [30, 50, 100],
#     'model__L2': [0.01, 0.001, 0.0001],
#     'batch_size': [32, 64, 128],
#     'epochs': [50, 100, 200]
# }
# 
# X_train = oil_train_batches_x
# y_train = oil_train_batches_y
# 
# # Initialize and fit the RandomizedSearchCV
# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3)
# random_search.fit(X_train, y_train)
# 
# # Print the best score and parameters
# print("Best score: %f using %s" % (random_search.best_score_, random_search.best_params_))
```




```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (oil_train_batches_x.shape[1], 1)


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters


# BUILD MODEL
recurrent_hidden_units = 30 # Update based on the best hyperparameters


# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_g = keras.Sequential()
model_no_reg_g.add(Input(shape=input_shape))
model_no_reg_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_g.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_g = keras.Sequential()
model_reg_g.add(Input(shape=input_shape))
model_reg_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_g.add(Dense(units=1, activation='linear'))

```




```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_g.summary())
print("GRU Model with regularization:")
print(model_reg_g.summary())
```



```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_oil_no_reg_g = model_no_reg_g.fit(oil_train_batches_x,
                                      oil_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(oil_test_batches_x, oil_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_oil_reg_g = model_reg_g.fit(oil_train_batches_x,
                                oil_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(oil_test_batches_x, oil_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_g, history_oil_reg_g)


```



```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_oil_no_reg_g = model_no_reg_g.predict(oil_train_batches_x)
Yvp_oil_no_reg_g = model_no_reg_g.predict(oil_test_batches_x)
Ytp_oil_reg_g = model_reg_g.predict(oil_train_batches_x)
Yvp_oil_reg_g = model_reg_g.predict(oil_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_g, train_mae_no_reg_g, val_mse_no_reg_g, val_mae_no_reg_g = regression_report(oil_train_batches_y, Ytp_oil_no_reg_g, oil_test_batches_y, Yvp_oil_no_reg_g, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_g, train_mae_reg_g, val_mse_reg_g, val_mae_reg_g = regression_report(oil_train_batches_y, Ytp_oil_reg_g, oil_test_batches_y, Yvp_oil_reg_g, "Regularized Model")


```




## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

```




## Electricity


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

```



## GDP


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

```



## CPI


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

```



:::



# 8. Discussion

Now, we will move forward to fit an ARCH/GARCH model to the stock price data. The first step in this process is to prepare the time series by calculating the stock price returns, which are typically used in financial time series modeling because they are more likely to exhibit stationary properties—a key requirement for ARCH/GARCH models.


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

results = []

# RNN results
results.append(['Crude Oil', 'RNN', 'No Reg', train_mse_no_reg, val_mse_no_reg])
results.append(['Crude Oil', 'RNN', 'Reg', train_mse_reg, val_mse_reg])

# LSTM results
results.append(['Crude Oil', 'LSTM', 'No Reg', train_mse_no_reg_l, val_mse_no_reg_l])
results.append(['Crude Oil', 'LSTM', 'Reg', train_mse_reg_l, val_mse_reg_l])

# GRU results
results.append(['Crude Oil', 'GRU', 'No Reg', train_mse_no_reg_g, val_mse_no_reg_g])
results.append(['Crude Oil', 'GRU', 'Reg', train_mse_reg_g, val_mse_reg_g])

# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)

result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)

print(result_df)
```







## 8.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?

Based on the validation RMSE (Root Mean Squared Error) values, the performance of the three ANN models can be ranked as follows:

LSTM (Regularized): 0.731646
GRU (Regularized): 0.777009
RNN (No Regularization): 0.840821
LSTM (No Regularization): 0.843815
GRU (No Regularization): 0.848463

The LSTM model with regularization demonstrates the highest accuracy and predictive power, followed closely by the GRU model with regularization. The RNN model without regularization performs slightly better than the LSTM and GRU models without regularization.
Overall, the LSTM and GRU models outperform the simple RNN model in terms of accuracy and predictive power for crude oil price prediction.




## 8.2 What effect does including regularization have on your results?



Comparing the performance of models with and without regularization, we can observe the following:

LSTM:

With regularization: 0.731646
Without regularization: 0.843815


GRU:

With regularization: 0.777009
Without regularization: 0.848463


RNN:

With regularization: 1.171465
Without regularization: 0.840821



For both LSTM and GRU models, incorporating regularization significantly improves the validation RMSE. The regularized LSTM and GRU models achieve lower RMSE values compared to their non-regularized counterparts, indicating better accuracy and predictive power.
However, for the RNN model, regularization seems to have a negative impact on performance. The regularized RNN model has a higher validation RMSE (1.171465) compared to the non-regularized RNN model (0.840821).
These results suggest that regularization techniques are effective in improving the performance of LSTM and GRU models for crude oil price prediction. However, the simple RNN model does not benefit from regularization in this case.






## 8.3 How far into the future can the deep learning model accurately predict the future?


The ability of the deep learning models to accurately predict future crude oil prices depends on various factors, such as the quality and quantity of historical data, the complexity of the underlying patterns, and the stability of the market dynamics.
Based on the results, we can see that the models achieve relatively low validation RMSE values, indicating good predictive performance. However, it's important to note that the results shown are for a specific validation set, which likely covers a limited time period.





## 8.4. How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?

Now, we will move forward to fit an ARCH/GARCH model to the stock price data. The first step in this process is to prepare the time series by calculating the stock price returns, which are typically used in financial time series modeling because they are more likely to exhibit stationary properties—a key requirement for ARCH/GARCH models.





# 9. Write a discussion paragraph Comparing your models (use RMSE) and forecasts from these sections with your Deep Learning Models.

Now, we will move forward to fit an ARCH/GARCH model to the stock price data. The first step in this process is to prepare the time series by calculating the stock price returns, which are typically used in financial time series modeling because they are more likely to exhibit stationary properties—a key requirement for ARCH/GARCH models.


