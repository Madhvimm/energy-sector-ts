---
title: "Deep Learning for TS"
output: html

---




```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false

library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)

```





```{python, warning=FALSE, message=FALSE, echo=FALSE}


#!pip install scikit-learn 
#!pip install tensorflow
#!pip install plotly
#!pip install statsmodels 
#!pip install IPython 
#!pip install matplotlib 
#!pip install seaborn 
#!pip install jupyter 
#!pip install scikeras

import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import pandas as pd 
import plotly.express as px 
import statsmodels.api as sm 
from IPython.display import IFrame
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error 
from tensorflow import keras 
from tensorflow.keras import layers 
from tensorflow.keras import initializers 
from tensorflow.keras import regularizers 
from keras.layers import Dense, SimpleRNN, LSTM, GRU, Dropout
# Import L1 and L2 regularizers from Python
import tensorflow.keras.regularizers
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Input
from keras.optimizers import RMSprop

from keras.callbacks import EarlyStopping
from keras.layers import LSTM, Dropout
from scikeras.wrappers import KerasRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import RandomizedSearchCV

```


```{python, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Read data'
#| warning: false
#| output: false


composite_crude_oil_prices = pd.read_csv("data/composite_crude_oil_prices.csv")

citygate_gas_prices = pd.read_csv("data/citygate_gas_prices.csv")

total_electricity_prices = pd.read_csv("data/total_electricity_prices.csv")

gdp_data = pd.read_csv("data/gdp_data.csv")

cpi_data = pd.read_csv("data/cpi_data.csv")

```



```{python, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'convert to ts'
#| warning: false
#| output: false


# For Crude Oil
composite_crude_oil_prices['Date'] = pd.to_datetime(composite_crude_oil_prices['Date'])
start_year = min(composite_crude_oil_prices['Date']).year
start_month = min(composite_crude_oil_prices['Date']).month
composite_crude_oil_ts = pd.Series(composite_crude_oil_prices['Value'].values, index=pd.date_range(start=f'{start_year}-{start_month}', periods=len(composite_crude_oil_prices), freq='ME'))
# Log-transform Value
composite_crude_oil_prices['LOG_Value'] = np.log(composite_crude_oil_prices['Value'])
oil_log_ts = pd.Series(composite_crude_oil_prices['LOG_Value'].values, index=pd.date_range(start=f'{start_year}-{start_month}', periods=len(composite_crude_oil_prices), freq='ME'))

# For Natural Gas
citygate_gas_prices['Date'] = pd.to_datetime(citygate_gas_prices['Date'])
start_year_gas = min(citygate_gas_prices['Date']).year
start_month_gas = min(citygate_gas_prices['Date']).month
citygate_gas_ts = pd.Series(citygate_gas_prices['Value'].values, index=pd.date_range(start=f'{start_year_gas}-{start_month_gas}', periods=len(citygate_gas_prices), freq='ME'))
# Log-transform Value
citygate_gas_prices['LOG_Value'] = np.log(citygate_gas_prices['Value'])
gas_log_ts = pd.Series(citygate_gas_prices['LOG_Value'].values, index=pd.date_range(start=f'{start_year_gas}-{start_month_gas}', periods=len(citygate_gas_prices), freq='ME'))

# For Electricity
total_electricity_prices['Date'] = pd.to_datetime(total_electricity_prices['Date'])
start_year_elec = min(total_electricity_prices['Date']).year
start_month_elec = min(total_electricity_prices['Date']).month
total_electricity_ts = pd.Series(total_electricity_prices['Value'].values, index=pd.date_range(start=f'{start_year_elec}-{start_month_elec}', periods=len(total_electricity_prices), freq='ME'))
# Log-transform Value
total_electricity_prices['LOG_Value'] = np.log(total_electricity_prices['Value'])
electricity_log_ts = pd.Series(total_electricity_prices['LOG_Value'].values, index=pd.date_range(start=f'{start_year_elec}-{start_month_elec}', periods=len(total_electricity_prices), freq='ME'))

# For GDP (it's quarterly)
gdp_data['DATE'] = pd.to_datetime(gdp_data['DATE'])
start_year_gdp = min(gdp_data['DATE']).year
start_quarter_gdp = min(gdp_data['DATE']).quarter
# Log-transform GDP
gdp_data['LOG_GDP'] = np.log(gdp_data['GDP'])
gdp_log_ts = pd.Series(gdp_data['LOG_GDP'].values, index=pd.date_range(start=f'{start_year_gdp}-Q{start_quarter_gdp}', periods=len(gdp_data), freq='QE'))

# For CPI (it's monthly)
cpi_data['DATE'] = pd.to_datetime(cpi_data['DATE'])
start_year_cpi = min(cpi_data['DATE']).year
start_month_cpi = min(cpi_data['DATE']).month
# Log-transform CPI
cpi_data['LOG_CPI'] = np.log(cpi_data['CPIAUCSL'])
cpi_log_ts = pd.Series(cpi_data['LOG_CPI'].values, index=pd.date_range(start=f'{start_year_cpi}-{start_month_cpi}', periods=len(cpi_data), freq='ME'))


```



# 1. Introduction


Time series forecasting plays a pivotal role in various domains, including economics, finance, and business operations. Accurately predicting future trends based on historical data can provide valuable insights for decision-making and strategic planning. Traditionally, statistical methods such as Autoregressive Integrated Moving Average (ARIMA) models have been widely employed for time series analysis. However, these conventional approaches often struggle to capture intricate patterns, non-linear relationships, and long-term dependencies present in complex time series data.

In recent years, deep learning techniques have emerged as powerful alternatives, offering new avenues for time series forecasting. Neural networks, with their ability to learn from data and uncover intricate patterns, have shown remarkable potential in handling sequential data and capturing temporal dependencies. This exploration focuses on three prominent deep learning architectures: Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and Gated Recurrent Units (GRUs), each designed to tackle the unique challenges of time series data.

The objective of this study is to evaluate the performance of these deep learning models in forecasting [specify the time series data or domain, e.g., energy prices, stock market trends, or sales data]. By leveraging their capability to handle complex non-linear relationships and long-term dependencies, we aim to enhance our forecasting accuracy and extend the predictive horizon compared to traditional methods.

Furthermore, this exploration will provide insights into the relative strengths and weaknesses of each neural network architecture, informing future decisions on selecting the most suitable approach for specific time series forecasting tasks. We will thoroughly analyze and compare the accuracy, predictive power, and generalization capabilities of RNNs, LSTMs, and GRUs, offering a comprehensive understanding of their applicability in the realm of time series analysis.

By embracing deep learning techniques, we strive to push the boundaries of time series forecasting, enabling more informed decision-making and strategic planning across various industries and domains. This study will contribute to the growing body of knowledge in the field, advancing our understanding of the potential and limitations of deep learning for time series analysis.



# 2. Data Visualization

To better understand the patterns in our data, we will first visualize it to examine any trends, seasonality, or anomalies. The visualization helps provide insights into how this time series behaves over time.


::: panel-tabset

## Crude Oil

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true



oil_plot = px.line(composite_crude_oil_prices, x='Date', y='Value',
                   title='Crude Oil Prices Over Time',
                   labels={'Date': 'Date', 'Value': 'Crude Oil Prices'},
                   template='plotly_white', 
                   color_discrete_sequence=['#99494d'])

oil_plot
```



## Natural Gas


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

ng_plot = px.line(citygate_gas_prices, x='Date', y='Value',
                  title='Natural Gas Price Over Time',
                  labels={'Date': 'Date', 'Value': 'Price (USD)'},
                  template='plotly_white',  
                  color_discrete_sequence=['#99494d'])

# Display the plot
ng_plot


```



## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot Electricity Prices'
#| warning: false
#| output: true

electricity_plot = px.line(total_electricity_prices, x='Date', y='Value',
                           title='Average Price of Electricity to Ultimate Customers',
                           labels={'Date': 'Date', 'Value': 'Price (Cents per Kilowatthour)'},
                           template='plotly_white',  
                           color_discrete_sequence=['#99494d'])

# Display the plot
electricity_plot.show()


```



## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot GDP'
#| warning: false
#| output: true

gdp_plot = px.line(gdp_data, x='DATE', y='GDP',
                   title='Gross Domestic Product Over Time',
                   labels={'DATE': 'Date', 'GDP': 'GDP (Billion USD)'},
                   template='plotly_white', 
                   color_discrete_sequence=['#99494d'])

# Display the plot
gdp_plot.show()



```


## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot CPI'
#| warning: false
#| output: true

cpi_plot = px.line(cpi_data, x='DATE', y='CPIAUCSL',
                   title='Consumer Price Index Over Time',
                   labels={'DATE': 'Date', 'CPIAUCSL': 'CPI'},
                   template='plotly_white',  # Using the 'plotly_white' as a minimal theme
                   color_discrete_sequence=['#99494d'])

# Display the plot
cpi_plot.show()

```




:::





# 3. Normalize & Split the Data 

To improve the accuracy of our models, we will normalize the time series data and split it into training and testing sets. Normalizing helps align the data distributions and prepares the data for training. We'll use a common split ratio of 80% training and 20% testing.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# Normalize the crude oil data
oil_normalized = (composite_crude_oil_prices['Value'] - composite_crude_oil_prices['Value'].mean()) / composite_crude_oil_prices['Value'].std()

# Split data into training and testing
split_index = int(0.8 * len(oil_normalized))
oil_train = oil_normalized[:split_index]
oil_test = oil_normalized[split_index:]

# Print the shapes
print("Shape of the Crude Oil Data:", len(oil_normalized))
print("Shape of the Crude Oil Train Data:", len(oil_train))
print("Shape of the Crude Oil Test Data:", len(oil_test))

# Prepare data for plotting
train_dates = composite_crude_oil_prices['Date'].iloc[:split_index]
test_dates = composite_crude_oil_prices['Date'].iloc[split_index:]

oil_data = pd.DataFrame({
    'Date': pd.concat([train_dates, test_dates]),
    'Value': pd.concat([oil_train, oil_test]),
    'Set': ['Training'] * len(oil_train) + ['Testing'] * len(oil_test)
})

# Plot the training and testing sets
oil_plot1 = px.area(oil_data, x='Date', y='Value', color='Set', title='Crude Oil Training and Testing Sets',
                    labels={'Value': 'Normalized Crude Oil Prices'}, template='plotly_white')
oil_plot1.update_layout(title_x=0.5)


```



## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# Normalize the natural gas data
gas_normalized = (citygate_gas_prices['Value'] - citygate_gas_prices['Value'].mean()) / citygate_gas_prices['Value'].std()

# Split data into training and testing
split_index = int(0.8 * len(gas_normalized))
gas_train = gas_normalized[:split_index]
gas_test = gas_normalized[split_index:]

# Print the shapes
print("Shape of the Natural Gas Data:", len(gas_normalized))
print("Shape of the Natural Gas Train Data:", len(gas_train))
print("Shape of the Natural Gas Test Data:", len(gas_test))

# Prepare data for plotting
train_dates_gas = citygate_gas_prices['Date'].iloc[:split_index]
test_dates_gas = citygate_gas_prices['Date'].iloc[split_index:]

gas_data = pd.DataFrame({
    'Date': pd.concat([train_dates_gas, test_dates_gas]),
    'Value': pd.concat([gas_train, gas_test]),
    'Set': ['Training'] * len(gas_train) + ['Testing'] * len(gas_test)
})

# Plot the training and testing sets
gas_plot = px.area(gas_data, x='Date', y='Value', color='Set', title='Natural Gas Training and Testing Sets',
                   labels={'Value': 'Normalized Natural Gas Prices'}, template='plotly_white')
gas_plot.update_layout(title_x=0.5)




```




## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# Normalize the electricity data
electricity_normalized = (total_electricity_prices['Value'] - total_electricity_prices['Value'].mean()) / total_electricity_prices['Value'].std()

# Split data into training and testing
split_index = int(0.8 * len(electricity_normalized))
electricity_train = electricity_normalized[:split_index]
electricity_test = electricity_normalized[split_index:]

# Print the shapes
print("Shape of the Electricity Prices Data:", len(electricity_normalized))
print("Shape of the Electricity Prices Train Data:", len(electricity_train))
print("Shape of the Electricity Prices Test Data:", len(electricity_test))

# Prepare data for plotting
train_dates_elec = total_electricity_prices['Date'].iloc[:split_index]
test_dates_elec = total_electricity_prices['Date'].iloc[split_index:]

elec_data = pd.DataFrame({
    'Date': pd.concat([train_dates_elec, test_dates_elec]),
    'Value': pd.concat([electricity_train, electricity_test]),
    'Set': ['Training'] * len(electricity_train) + ['Testing'] * len(electricity_test)
})

# Plot the training and testing sets
elec_plot = px.area(elec_data, x='Date', y='Value', color='Set', title='Electricity Training and Testing Sets',
                    labels={'Value': 'Normalized Electricity Prices'}, template='plotly_white')
elec_plot.update_layout(title_x=0.5)



```



## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true



# Normalize the GDP data
gdp_normalized = (gdp_data['GDP'] - gdp_data['GDP'].mean()) / gdp_data['GDP'].std()

# Split data into training and testing
split_index = int(0.8 * len(gdp_normalized))
gdp_train = gdp_normalized[:split_index]
gdp_test = gdp_normalized[split_index:]

# Print the shapes
print("Shape of the GDP Data:", len(gdp_normalized))
print("Shape of the GDP Train Data:", len(gdp_train))
print("Shape of the GDP Test Data:", len(gdp_test))

# Prepare data for plotting
train_dates_gdp = gdp_data['DATE'].iloc[:split_index]
test_dates_gdp = gdp_data['DATE'].iloc[split_index:]

gdp_data_plot = pd.DataFrame({
    'Date': pd.concat([train_dates_gdp, test_dates_gdp]),
    'Value': pd.concat([gdp_train, gdp_test]),
    'Set': ['Training'] * len(gdp_train) + ['Testing'] * len(gdp_test)
})

# Plot the training and testing sets
gdp_plot1 = px.area(gdp_data_plot, x='Date', y='Value', color='Set', title='GDP Training and Testing Sets',
                    labels={'Value': 'Normalized GDP'}, template='plotly_white')
gdp_plot1.update_layout(title_x=0.5)





```



## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# Normalize the CPI data
cpi_normalized = (cpi_data['CPIAUCSL'] - cpi_data['CPIAUCSL'].mean()) / cpi_data['CPIAUCSL'].std()

# Split data into training and testing
split_index = int(0.8 * len(cpi_normalized))
cpi_train = cpi_normalized[:split_index]
cpi_test = cpi_normalized[split_index:]

# Print the shapes
print("Shape of the CPI Data:", len(cpi_normalized))
print("Shape of the CPI Train Data:", len(cpi_train))
print("Shape of the CPI Test Data:", len(cpi_test))

# Prepare data for plotting
train_dates_cpi = cpi_data['DATE'].iloc[:split_index]
test_dates_cpi = cpi_data['DATE'].iloc[split_index:]

cpi_data_plot = pd.DataFrame({
    'Date': pd.concat([train_dates_cpi, test_dates_cpi]),
    'Value': pd.concat([cpi_train, cpi_test]),
    'Set': ['Training'] * len(cpi_train) + ['Testing'] * len(cpi_test)
})

# Plot the training and testing sets
cpi_plot1 = px.area(cpi_data_plot, x='Date', y='Value', color='Set', title='CPI Training and Testing Sets',
                    labels={'Value': 'Normalized CPI'}, template='plotly_white')
cpi_plot1.update_layout(title_x=0.5)




```


:::


# 4. Mini Batching


Mini-batching is a technique used in deep learning to efficiently train models on large datasets. Instead of processing the entire dataset at once, mini-batching involves creating small, sequential subsets of the data called "mini-batches." This approach offers several benefits, including memory efficiency, faster convergence, and the ability to use stochastic gradient descent (SGD) optimization. By processing a portion of the data at a time, mini-batching reduces the memory requirements compared to processing the entire dataset simultaneously, making it feasible to train models on large datasets.

To implement mini-batching, the dataset is typically shuffled and divided into smaller batches of a fixed size. The model is then trained iteratively, processing one mini-batch at a time. The gradients are computed for each mini-batch, and the model's parameters are updated accordingly. This process is repeated until the model has seen the entire dataset, which constitutes one epoch of training. The choice of mini-batch size is an important hyperparameter that determines the number of samples processed in each iteration, and it can impact the convergence speed and stability of the training process.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  # Subtract 1 to stay within bounds
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
oil_train = np.array(oil_train)
oil_test = np.array(oil_test)

# Create mini-batches
oil_train_batches_x, oil_train_batches_y = form_arrays(oil_train, lookback=lookback, delay=1, step=1)
oil_test_batches_x, oil_test_batches_y = form_arrays(oil_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {oil_train_batches_x.shape}, {oil_train_batches_y.shape}")
print(f"Test shape: {oil_test_batches_x.shape}, {oil_test_batches_y.shape}")

```


## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
gas_train = np.array(gas_train)
gas_test = np.array(gas_test)

# Create mini-batches
gas_train_batches_x, gas_train_batches_y = form_arrays(gas_train, lookback=lookback, delay=1, step=1)
gas_test_batches_x, gas_test_batches_y = form_arrays(gas_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {gas_train_batches_x.shape}, {gas_train_batches_y.shape}")
print(f"Test shape: {gas_test_batches_x.shape}, {gas_test_batches_y.shape}")

```


## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
electricity_train = np.array(electricity_train)
electricity_test = np.array(electricity_test)

# Create mini-batches
electricity_train_batches_x, electricity_train_batches_y = form_arrays(electricity_train, lookback=lookback, delay=1, step=1)
electricity_test_batches_x, electricity_test_batches_y = form_arrays(electricity_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {electricity_train_batches_x.shape}, {electricity_train_batches_y.shape}")
print(f"Test shape: {electricity_test_batches_x.shape}, {electricity_test_batches_y.shape}")


```


## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
gdp_train = np.array(gdp_train)
gdp_test = np.array(gdp_test)

# Create mini-batches
gdp_train_batches_x, gdp_train_batches_y = form_arrays(gdp_train, lookback=lookback, delay=1, step=1)
gdp_test_batches_x, gdp_test_batches_y = form_arrays(gdp_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {gdp_train_batches_x.shape}, {gdp_train_batches_y.shape}")
print(f"Test shape: {gdp_test_batches_x.shape}, {gdp_test_batches_y.shape}")

```


## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
    count = 0
    x_out = []
    y_out = []
    i_start = 0

    # Adjust the stopping condition for the loop to account for delay
    while i_start + lookback + delay - 1 < len(x):
        i_stop = i_start + lookback
        i_pred = i_stop + delay - 1  
        indices_to_keep = list(range(i_start, i_stop, step))
        xtmp = x[indices_to_keep]
        ytmp = x[i_pred]
        x_out.append(xtmp)
        y_out.append(ytmp)

        if verbose and count < 2:
            plt.plot(np.arange(len(x)), x, 'b-', label='Value')
            plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
            plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.legend()
            plt.show()

        # Update start point
        i_start += step
        count += 1

    return np.array(x_out), np.array(y_out)

lookback = 5

# Convert to numpy arrays
cpi_train = np.array(cpi_train)
cpi_test = np.array(cpi_test)

# Create mini-batches
cpi_train_batches_x, cpi_train_batches_y = form_arrays(cpi_train, lookback=lookback, delay=1, step=1)
cpi_test_batches_x, cpi_test_batches_y = form_arrays(cpi_test, lookback=lookback, delay=1, step=1)

# Print shapes

print(f"Train shape: {cpi_train_batches_x.shape}, {cpi_train_batches_y.shape}")
print(f"Test shape: {cpi_test_batches_x.shape}, {cpi_test_batches_y.shape}")


```




::: 




# 5. Recurrent Neural Networks (RNNs)

Recurrent Neural Networks (RNNs) are designed to process sequential data, making them ideal for time series forecasting. Unlike traditional neural networks, RNNs have a memory component that allows them to consider previous time steps, making them particularly adept at handling temporal dependencies. In this section, we will implement an RNN to forecast future trends in our data.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (oil_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 64  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])
     
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg.summary())

print("Model with regularization:")
print(model_reg.summary())

```


```{python, warning=FALSE, message=FALSE, echo=FALSE}

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_oil_no_reg = model_no_reg.fit(oil_train_batches_x,
                                      oil_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(oil_test_batches_x, oil_test_batches_y),
                                      callbacks=[early_stopping])

history_oil_reg = model_reg.fit(oil_train_batches_x,
                                oil_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(oil_test_batches_x, oil_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg, history_oil_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_oil_no_reg = model_no_reg.predict(oil_train_batches_x)
Yvp_oil_no_reg = model_no_reg.predict(oil_test_batches_x)
Ytp_oil_reg = model_reg.predict(oil_train_batches_x)
Yvp_oil_reg = model_reg.predict(oil_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg, train_mae_no_reg, val_mse_no_reg, val_mae_no_reg = regression_report(oil_train_batches_y, Ytp_oil_no_reg, oil_test_batches_y, Yvp_oil_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg, train_mae_reg, val_mse_reg, val_mae_reg = regression_report(oil_train_batches_y, Ytp_oil_reg, oil_test_batches_y, Yvp_oil_reg, "Regularized Model")
```



## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gas_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 32  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gas = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_gas = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gas.summary())

print("Model with regularization:")
print(model_reg_gas.summary())

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gas_no_reg = model_no_reg_gas.fit(gas_train_batches_x,
                                      gas_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gas_test_batches_x, gas_test_batches_y),
                                      callbacks=[early_stopping])

history_gas_reg = model_reg_gas.fit(gas_train_batches_x,
                                gas_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gas_test_batches_x, gas_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg, history_gas_reg)

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_gas_no_reg = model_no_reg_gas.predict(gas_train_batches_x)
Yvp_gas_no_reg = model_no_reg_gas.predict(gas_test_batches_x)
Ytp_gas_reg = model_reg_gas.predict(gas_train_batches_x)
Yvp_gas_reg = model_reg_gas.predict(gas_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Natural Gas Prices', ylabel='Predicted Natural Gas Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Natural Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Natural Gas Prices', ylabel='Predicted Natural Gas Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Natural Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gas, train_mae_no_reg_gas, val_mse_no_reg_gas, val_mae_no_reg_gas = regression_report(gas_train_batches_y, Ytp_gas_no_reg, gas_test_batches_y, Yvp_gas_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gas, train_mae_reg_gas, val_mse_reg_gas, val_mae_reg_gas = regression_report(gas_train_batches_y, Ytp_gas_reg, gas_test_batches_y, Yvp_gas_reg, "Regularized Model")
```



## Electricity


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 200 # Update based on the best hyperparameters
L2 = 0.01  # Update based on the best hyperparameters
input_shape = (electricity_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 128  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_elec = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_elec = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```



```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_elec.summary())

print("Model with regularization:")
print(model_reg_elec.summary())

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_elec.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_elec.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_elec_no_reg = model_no_reg_elec.fit(electricity_train_batches_x,
                                      electricity_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                      callbacks=[early_stopping])

history_elec_reg = model_reg_elec.fit(electricity_train_batches_x,
                                electricity_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_elec_no_reg, history_elec_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_elec_no_reg = model_no_reg_elec.predict(electricity_train_batches_x)
Yvp_elec_no_reg = model_no_reg_elec.predict(electricity_test_batches_x)
Ytp_elec_reg = model_reg_elec.predict(electricity_train_batches_x)
Yvp_elec_reg = model_reg_elec.predict(electricity_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_elec, train_mae_no_reg_elec, val_mse_no_reg_elec, val_mae_no_reg_elec = regression_report(electricity_train_batches_y, Ytp_elec_no_reg, electricity_test_batches_y, Yvp_elec_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_elec, train_mae_reg_elec, val_mse_reg_elec, val_mae_reg_elec = regression_report(electricity_train_batches_y, Ytp_elec_reg, electricity_test_batches_y, Yvp_elec_reg, "Regularized Model")
```




## GDP


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 50 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gdp_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 128  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gdp = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_gdp = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gdp.summary())

print("Model with regularization:")
print(model_reg_gdp.summary())

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_gdp.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gdp_no_reg = model_no_reg_gdp.fit(gdp_train_batches_x,
                                      gdp_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                      callbacks=[early_stopping])

history_gdp_reg = model_reg_gdp.fit(gdp_train_batches_x,
                                gdp_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg, history_gdp_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_gdp_no_reg = model_no_reg_gdp.predict(gdp_train_batches_x)
Yvp_gdp_no_reg = model_no_reg_gdp.predict(gdp_test_batches_x)
Ytp_gdp_reg = model_reg_gdp.predict(gdp_train_batches_x)
Yvp_gdp_reg = model_reg_gdp.predict(gdp_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gdp, train_mae_no_reg_gdp, val_mse_no_reg_gdp, val_mae_no_reg_gdp = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg, gdp_test_batches_y, Yvp_gdp_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gdp, train_mae_reg_gdp, val_mse_reg_gdp, val_mae_reg_gdp = regression_report(gdp_train_batches_y, Ytp_gdp_reg, gdp_test_batches_y, Yvp_gdp_reg, "Regularized Model")
```





## CPI


```{python, warning=FALSE, message=FALSE, echo=FALSE}

optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 200 # Update based on the best hyperparameters
L2 = 0.01  # Update based on the best hyperparameters
input_shape = (cpi_train_batches_x.shape[1], 1)  


# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 64  # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50  # Update based on the best hyperparameters


# CREATE MODEL
model_no_reg = keras.Sequential()

# ADD RECURRENT LAYER

# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_cpi = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
    Dense(units=1, activation='linear')
])

#CREATE MODEL WITH REGULARIZATION
model_reg_cpi = Sequential([
    Input(shape=input_shape),  # Add Input layer to define input shape
    SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
              recurrent_regularizer=regularizers.l2(L2)),
    Dense(units=1, activation='linear')
])

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}

# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_cpi.summary())

print("Model with regularization:")
print(model_reg_cpi.summary())

```

```{python, warning=FALSE, message=FALSE, echo=TRUE}

# COMPILING THE MODELS
model_no_reg_cpi.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_cpi_no_reg = model_no_reg_cpi.fit(cpi_train_batches_x,
                                      cpi_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                      callbacks=[early_stopping])

history_cpi_reg = model_reg_cpi.fit(cpi_train_batches_x,
                                cpi_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg, history_cpi_reg)

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
# PREDICTIONS
Ytp_cpi_no_reg = model_no_reg_cpi.predict(cpi_train_batches_x)
Yvp_cpi_no_reg = model_no_reg_cpi.predict(cpi_test_batches_x)
Ytp_cpi_reg = model_reg_cpi.predict(cpi_train_batches_x)
Yvp_cpi_reg = model_reg_cpi.predict(cpi_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual cpi', ylabel='Predicted CPI',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='cpi (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_cpi, train_mae_no_reg_cpi, val_mse_no_reg_cpi, val_mae_no_reg_cpi = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg, cpi_test_batches_y, Yvp_cpi_no_reg, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_cpi, train_mae_reg_cpi, val_mse_reg_cpi, val_mae_reg_cpi = regression_report(cpi_train_batches_y, Ytp_cpi_reg, cpi_test_batches_y, Yvp_cpi_reg, "Regularized Model")
```

:::




# 6. LSTM

Long Short-Term Memory Networks (LSTMs) are a specialized kind of Recurrent Neural Network (RNN) that excel in capturing long-term dependencies within time series data, a task where traditional RNNs often falter due to issues like vanishing and exploding gradients. LSTMs are designed with a complex gating mechanism that controls the flow of information. These gatesforget gate, input gate, and output gatedetermine what information should be retained or discarded as the sequence progresses, allowing LSTMs to maintain a longer memory and avoid the stability problems seen in standard RNNs.

Now, I will implement an LSTM model to address the challenges posed by the intricate patterns and non-linear relationships.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (oil_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_l = keras.Sequential()
model_no_reg_l.add(Input(shape=input_shape))
model_no_reg_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_l = keras.Sequential()
model_reg_l.add(Input(shape=input_shape))
model_reg_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_l.summary())
print("LSTM Model with regularization:")
print(model_reg_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_oil_no_reg_l = model_no_reg_l.fit(oil_train_batches_x,
                                      oil_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(oil_test_batches_x, oil_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_oil_reg_l = model_reg_l.fit(oil_train_batches_x,
                                oil_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(oil_test_batches_x, oil_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_l, history_oil_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_oil_no_reg_l = model_no_reg_l.predict(oil_train_batches_x)
Yvp_oil_no_reg_l = model_no_reg_l.predict(oil_test_batches_x)
Ytp_oil_reg_l = model_reg_l.predict(oil_train_batches_x)
Yvp_oil_reg_l = model_reg_l.predict(oil_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_l, train_mae_no_reg_l, val_mse_no_reg_l, val_mae_no_reg_l = regression_report(oil_train_batches_y, Ytp_oil_no_reg_l, oil_test_batches_y, Yvp_oil_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_l, train_mae_reg_l, val_mse_reg_l, val_mae_reg_l = regression_report(oil_train_batches_y, Ytp_oil_reg_l, oil_test_batches_y, Yvp_oil_reg_l, "Regularized Model")


```






## Natural Gas


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gas_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gas_l = keras.Sequential()
model_no_reg_gas_l.add(Input(shape=input_shape))
model_no_reg_gas_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_gas_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_gas_l = keras.Sequential()
model_reg_gas_l.add(Input(shape=input_shape))
model_reg_gas_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_gas_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_gas_l.summary())
print("LSTM Model with regularization:")
print(model_reg_gas_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_gas_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gas_no_reg_l = model_no_reg_gas_l.fit(gas_train_batches_x,
                                      gas_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gas_test_batches_x, gas_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_gas_reg_l = model_reg_gas_l.fit(gas_train_batches_x,
                                gas_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gas_test_batches_x, gas_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg_l, history_gas_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_gas_no_reg_l = model_no_reg_gas_l.predict(gas_train_batches_x)
Yvp_gas_no_reg_l = model_no_reg_gas_l.predict(gas_test_batches_x)
Ytp_gas_reg_l = model_reg_gas_l.predict(gas_train_batches_x)
Yvp_gas_reg_l = model_reg_gas_l.predict(gas_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Gas Prices', ylabel='Predicted Gas Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Gas Prices', ylabel='Predicted Gas Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gas_l, train_mae_no_reg_gas_l, val_mse_no_reg_gas_l, val_mae_no_reg_gas_l = regression_report(gas_train_batches_y, Ytp_gas_no_reg_l, gas_test_batches_y, Yvp_gas_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gas_l, train_mae_reg_gas_l, val_mse_reg_gas_l, val_mae_reg_gas_l = regression_report(gas_train_batches_y, Ytp_gas_reg_l, gas_test_batches_y, Yvp_gas_reg_l, "Regularized Model")


```





## Electricity



```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.01
L2_no_reg = 0
input_shape = (electricity_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 100 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_elec_l = keras.Sequential()
model_no_reg_elec_l.add(Input(shape=input_shape))
model_no_reg_elec_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_elec_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_elec_l = keras.Sequential()
model_reg_elec_l.add(Input(shape=input_shape))
model_reg_elec_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_elec_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_elec_l.summary())
print("LSTM Model with regularization:")
print(model_reg_elec_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_elec_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_elec_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_elec_no_reg_l = model_no_reg_elec_l.fit(electricity_train_batches_x,
                                      electricity_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_elec_reg_l = model_reg_elec_l.fit(electricity_train_batches_x,
                                electricity_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_elec_no_reg_l, history_elec_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_elec_no_reg_l = model_no_reg_elec_l.predict(electricity_train_batches_x)
Yvp_elec_no_reg_l = model_no_reg_elec_l.predict(electricity_test_batches_x)
Ytp_elec_reg_l = model_reg_elec_l.predict(electricity_train_batches_x)
Yvp_elec_reg_l = model_reg_elec_l.predict(electricity_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_elec_l, train_mae_no_reg_elec_l, val_mse_no_reg_elec_l, val_mae_no_reg_elec_l = regression_report(electricity_train_batches_y, Ytp_elec_no_reg_l, electricity_test_batches_y, Yvp_elec_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_elec_l, train_mae_reg_elec_l, val_mse_reg_elec_l, val_mae_reg_elec_l = regression_report(electricity_train_batches_y, Ytp_elec_reg_l, electricity_test_batches_y, Yvp_elec_reg_l, "Regularized Model")


```



## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 50
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gdp_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gdp_l = keras.Sequential()
model_no_reg_gdp_l.add(Input(shape=input_shape))
model_no_reg_gdp_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_gdp_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_gdp_l = keras.Sequential()
model_reg_gdp_l.add(Input(shape=input_shape))
model_reg_gdp_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_gdp_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_gdp_l.summary())
print("LSTM Model with regularization:")
print(model_reg_gdp_l.summary())
```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_gdp_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gdp_no_reg_l = model_no_reg_gdp_l.fit(gdp_train_batches_x,
                                      gdp_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_gdp_reg_l = model_reg_gdp_l.fit(gdp_train_batches_x,
                                gdp_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg_l, history_gdp_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_gdp_no_reg_l = model_no_reg_gdp_l.predict(gdp_train_batches_x)
Yvp_gdp_no_reg_l = model_no_reg_gdp_l.predict(gdp_test_batches_x)
Ytp_gdp_reg_l = model_reg_gdp_l.predict(gdp_train_batches_x)
Yvp_gdp_reg_l = model_reg_gdp_l.predict(gdp_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gdp_l, train_mae_no_reg_gdp_l, val_mse_no_reg_gdp_l, val_mae_no_reg_gdp_l = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg_l, gdp_test_batches_y, Yvp_gdp_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gdp_l, train_mae_reg_gdp_l, val_mse_reg_gdp_l, val_mae_reg_gdp_l = regression_report(gdp_train_batches_y, Ytp_gdp_reg_l, gdp_test_batches_y, Yvp_gdp_reg_l, "Regularized Model")


```




## CPI


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.01
L2_no_reg = 0
input_shape = (cpi_train_batches_x.shape[1], 1) 


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters

# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters

# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_cpi_l = keras.Sequential()
model_no_reg_cpi_l.add(Input(shape=input_shape))
model_no_reg_cpi_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_cpi_l.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_cpi_l = keras.Sequential()
model_reg_cpi_l.add(Input(shape=input_shape))
model_reg_cpi_l.add(LSTM(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_cpi_l.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_cpi_l.summary())
print("LSTM Model with regularization:")
print(model_reg_cpi_l.summary())
```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_cpi_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_cpi_no_reg_l = model_no_reg_cpi_l.fit(cpi_train_batches_x,
                                      cpi_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_cpi_reg_l = model_reg_cpi_l.fit(cpi_train_batches_x,
                                cpi_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg_l, history_cpi_reg_l)


```

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_cpi_no_reg_l = model_no_reg_cpi_l.predict(cpi_train_batches_x)
Yvp_cpi_no_reg_l = model_no_reg_cpi_l.predict(cpi_test_batches_x)
Ytp_cpi_reg_l = model_reg_cpi_l.predict(cpi_train_batches_x)
Yvp_cpi_reg_l = model_reg_cpi_l.predict(cpi_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_cpi_l, train_mae_no_reg_cpi_l, val_mse_no_reg_cpi_l, val_mae_no_reg_cpi_l = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg_l, cpi_test_batches_y, Yvp_cpi_no_reg_l, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_cpi_l, train_mae_reg_cpi_l, val_mse_reg_cpi_l, val_mae_reg_cpi_l = regression_report(cpi_train_batches_y, Ytp_cpi_reg_l, cpi_test_batches_y, Yvp_cpi_reg_l, "Regularized Model")


```





:::



# 7. GRU

Gated Recurrent Units (GRUs) are another variant of RNNs that are similar to LSTMs but are often favored for their simpler architectural design and efficiency in training. Like LSTMs, GRUs use gating mechanisms to control and manage the flow of information within the unit; however, they combine the forget and input gates into a single update gate and merge the cell state and hidden state, resulting in fewer parameters and faster training times.

Now, I will explore the application of GRUs in forecasting.


::: panel-tabset

## Crude Oil


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (oil_train_batches_x.shape[1], 1)


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters


# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters


# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_g = keras.Sequential()
model_no_reg_g.add(Input(shape=input_shape))
model_no_reg_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_g.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_g = keras.Sequential()
model_reg_g.add(Input(shape=input_shape))
model_reg_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_g.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_g.summary())
print("GRU Model with regularization:")
print(model_reg_g.summary())
```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_oil_no_reg_g = model_no_reg_g.fit(oil_train_batches_x,
                                      oil_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(oil_test_batches_x, oil_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_oil_reg_g = model_reg_g.fit(oil_train_batches_x,
                                oil_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(oil_test_batches_x, oil_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_g, history_oil_reg_g)


```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_oil_no_reg_g = model_no_reg_g.predict(oil_train_batches_x)
Yvp_oil_no_reg_g = model_no_reg_g.predict(oil_test_batches_x)
Ytp_oil_reg_g = model_reg_g.predict(oil_train_batches_x)
Yvp_oil_reg_g = model_reg_g.predict(oil_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_g, train_mae_no_reg_g, val_mse_no_reg_g, val_mae_no_reg_g = regression_report(oil_train_batches_y, Ytp_oil_no_reg_g, oil_test_batches_y, Yvp_oil_no_reg_g, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_g, train_mae_reg_g, val_mse_reg_g, val_mae_reg_g = regression_report(oil_train_batches_y, Ytp_oil_reg_g, oil_test_batches_y, Yvp_oil_reg_g, "Regularized Model")


```




## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 100
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gas_train_batches_x.shape[1], 1)


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters


# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters


# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gas_g = keras.Sequential()
model_no_reg_gas_g.add(Input(shape=input_shape))
model_no_reg_gas_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_gas_g.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_gas_g = keras.Sequential()
model_reg_gas_g.add(Input(shape=input_shape))
model_reg_gas_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_gas_g.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_gas_g.summary())
print("GRU Model with regularization:")
print(model_reg_gas_g.summary())
```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_gas_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gas_no_reg_g = model_no_reg_gas_g.fit(gas_train_batches_x,
                                      gas_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gas_test_batches_x, gas_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_gas_reg_g = model_reg_gas_g.fit(gas_train_batches_x,
                                gas_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gas_test_batches_x, gas_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg_g, history_gas_reg_g)


```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_gas_no_reg_g = model_no_reg_gas_g.predict(gas_train_batches_x)
Yvp_gas_no_reg_g = model_no_reg_gas_g.predict(gas_test_batches_x)
Ytp_gas_reg_g = model_reg_gas_g.predict(gas_train_batches_x)
Yvp_gas_reg_g = model_reg_gas_g.predict(gas_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Gas Prices', ylabel='Predicted Gas Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Gas Prices', ylabel='Predicted Gas Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Gas Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gas_g, train_mae_no_reg_gas_g, val_mse_no_reg_gas_g, val_mae_no_reg_gas_g = regression_report(gas_train_batches_y, Ytp_gas_no_reg_g, gas_test_batches_y, Yvp_gas_no_reg_g, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gas_g, train_mae_reg_gas_g, val_mse_reg_gas_g, val_mae_reg_gas_g = regression_report(gas_train_batches_y, Ytp_gas_reg_g, gas_test_batches_y, Yvp_gas_reg_g, "Regularized Model")


```



## Electricity


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.01
L2_no_reg = 0
input_shape = (electricity_train_batches_x.shape[1], 1)


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters


# BUILD MODEL
recurrent_hidden_units = 100 # Update based on the best hyperparameters


# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_electricity_g = keras.Sequential()
model_no_reg_electricity_g.add(Input(shape=input_shape))
model_no_reg_electricity_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_electricity_g.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_electricity_g = keras.Sequential()
model_reg_electricity_g.add(Input(shape=input_shape))
model_reg_electricity_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_electricity_g.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_electricity_g.summary())
print("GRU Model with regularization:")
print(model_reg_electricity_g.summary())
```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_electricity_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_electricity_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_electricity_no_reg_g = model_no_reg_electricity_g.fit(electricity_train_batches_x,
                                      electricity_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_electricity_reg_g = model_reg_electricity_g.fit(electricity_train_batches_x,
                                electricity_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(electricity_test_batches_x, electricity_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_electricity_no_reg_g, history_electricity_reg_g)


```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_electricity_no_reg_g = model_no_reg_electricity_g.predict(electricity_train_batches_x)
Yvp_electricity_no_reg_g = model_no_reg_electricity_g.predict(electricity_test_batches_x)
Ytp_electricity_reg_g = model_reg_electricity_g.predict(electricity_train_batches_x)
Yvp_electricity_reg_g = model_reg_electricity_g.predict(electricity_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_electricity_g, train_mae_no_reg_electricity_g, val_mse_no_reg_electricity_g, val_mae_no_reg_electricity_g = regression_report(electricity_train_batches_y, Ytp_electricity_no_reg_g, electricity_test_batches_y, Yvp_electricity_no_reg_g, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_electricity_g, train_mae_reg_electricity_g, val_mse_reg_electricity_g, val_mae_reg_electricity_g = regression_report(electricity_train_batches_y, Ytp_electricity_reg_g, electricity_test_batches_y, Yvp_electricity_reg_g, "Regularized Model")


```


## GDP


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 50
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gdp_train_batches_x.shape[1], 1)


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 128 # Update based on the best hyperparameters


# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters


# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gdp_g = keras.Sequential()
model_no_reg_gdp_g.add(Input(shape=input_shape))
model_no_reg_gdp_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_gdp_g.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_gdp_g = keras.Sequential()
model_reg_gdp_g.add(Input(shape=input_shape))
model_reg_gdp_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_gdp_g.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_gdp_g.summary())
print("GRU Model with regularization:")
print(model_reg_gdp_g.summary())
```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_gdp_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_gdp_no_reg_g = model_no_reg_gdp_g.fit(gdp_train_batches_x,
                                      gdp_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_gdp_reg_g = model_reg_gdp_g.fit(gdp_train_batches_x,
                                gdp_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(gdp_test_batches_x, gdp_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg_g, history_gdp_reg_g)


```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_gdp_no_reg_g = model_no_reg_gdp_g.predict(gdp_train_batches_x)
Yvp_gdp_no_reg_g = model_no_reg_gdp_g.predict(gdp_test_batches_x)
Ytp_gdp_reg_g = model_reg_gdp_g.predict(gdp_train_batches_x)
Yvp_gdp_reg_g = model_reg_gdp_g.predict(gdp_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_gdp_g, train_mae_no_reg_gdp_g, val_mse_no_reg_gdp_g, val_mae_no_reg_gdp_g = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg_g, gdp_test_batches_y, Yvp_gdp_no_reg_g, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_gdp_g, train_mae_reg_gdp_g, val_mse_reg_gdp_g, val_mae_reg_gdp_g = regression_report(gdp_train_batches_y, Ytp_gdp_reg_g, gdp_test_batches_y, Yvp_gdp_reg_g, "Regularized Model")


```


## CPI




```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.01
L2_no_reg = 0
input_shape = (cpi_train_batches_x.shape[1], 1)


# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters


# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters


# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_cpi_g = keras.Sequential()
model_no_reg_cpi_g.add(Input(shape=input_shape))
model_no_reg_cpi_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_no_reg),
    activation='relu'
))
model_no_reg_cpi_g.add(Dense(units=1, activation='linear'))

# CREATE MODEL WITH REGULARIZATION
model_reg_cpi_g = keras.Sequential()
model_reg_cpi_g.add(Input(shape=input_shape))
model_reg_cpi_g.add(GRU(
    units=recurrent_hidden_units,
    return_sequences=False,
    recurrent_regularizer=regularizers.L2(L2_reg),
    activation='relu'
))
model_reg_cpi_g.add(Dense(units=1, activation='linear'))

```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true


# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_cpi_g.summary())
print("GRU Model with regularization:")
print(model_reg_cpi_g.summary())
```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# # print("initial parameters:", model.get_weights())

# COMPILING THE MODELS
model_no_reg_cpi_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# TRAINING THE MODELS
history_cpi_no_reg_g = model_no_reg_cpi_g.fit(cpi_train_batches_x,
                                      cpi_train_batches_y,
                                      epochs=number_of_epochs,
                                      batch_size=batch_size,
                                      verbose=False,
                                      validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                      callbacks=[early_stopping])
                                      
history_cpi_reg_g = model_reg_cpi_g.fit(cpi_train_batches_x,
                                cpi_train_batches_y,
                                epochs=number_of_epochs,
                                batch_size=batch_size,
                                verbose=False,
                                validation_data=(cpi_test_batches_x, cpi_test_batches_y),
                                callbacks=[early_stopping])

def history_plot_separate(history_no_reg, history_reg):
    # Get the history dictionaries
    history_dict_no_reg = history_no_reg.history
    history_dict_reg = history_reg.history
    
    # Get the loss values and epochs for both models
    loss_values_no_reg = history_dict_no_reg['loss']
    val_loss_values_no_reg = history_dict_no_reg['val_loss']
    loss_values_reg = history_dict_reg['loss']
    val_loss_values_reg = history_dict_reg['val_loss']
    
    # Determine the number of epochs the training actually ran for
    epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
    epochs_reg = range(1, len(loss_values_reg) + 1)
    
    # Plot for non-regularized model
    plt.figure()
    plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
    plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
    plt.title('Training and Validation Loss for Non-Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plot for regularized model
    plt.figure()
    plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
    plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
    plt.title('Training and Validation Loss for Regularized Model')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg_g, history_cpi_reg_g)


```


```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

# PREDICTIONS
Ytp_cpi_no_reg_g = model_no_reg_cpi_g.predict(cpi_train_batches_x)
Yvp_cpi_no_reg_g = model_no_reg_cpi_g.predict(cpi_test_batches_x)
Ytp_cpi_reg_g = model_reg_cpi_g.predict(cpi_train_batches_x)
Yvp_cpi_reg_g = model_reg_cpi_g.predict(cpi_test_batches_x)

def regression_report(yt, ytp, yv, yvp, model_name):
    print(f"--------- Regression Report ({model_name}) ---------")
    print("TRAINING:")
    train_mse = np.mean((yt - ytp) ** 2)
    train_mae = np.mean(np.abs(yt - ytp))
    print("MSE", train_mse)
    print("MAE", train_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yt, ytp, 'ro')
    ax.plot(yt, yt, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Training Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    frac_plot = 1.0
    upper = int(frac_plot * yt.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yt[0:upper], 'b-')
    ax.plot(ytp[0:upper], 'r-', alpha=0.5)
    ax.plot(ytp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Training: Time-Series Prediction ({model_name})')
    plt.show()

    print("VALIDATION:")
    val_mse = np.mean((yv - yvp) ** 2)
    val_mae = np.mean(np.abs(yv - yvp))
    print("MSE", val_mse)
    print("MAE", val_mae)

    # PARITY PLOT
    fig, ax = plt.subplots()
    ax.plot(yv, yvp, 'ro')
    ax.plot(yv, yv, 'b-')
    ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
           title=f'Validation Data Parity Plot ({model_name})')
    plt.show()

    # PLOT PART OF THE PREDICTED TIME-SERIES
    upper = int(frac_plot * yv.shape[0])
    fig, ax = plt.subplots()
    ax.plot(yv[0:upper], 'b-')
    ax.plot(yvp[0:upper], 'r-', alpha=0.5)
    ax.plot(yvp[0:upper], 'ro', alpha=0.25)
    ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
           title=f'Validation: Time-Series Prediction ({model_name})')
    plt.show()

    return train_mse, train_mae, val_mse, val_mae

# Regression report for the non-regularized model
train_mse_no_reg_cpi_g, train_mae_no_reg_cpi_g, val_mse_no_reg_cpi_g, val_mae_no_reg_cpi_g = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg_g, cpi_test_batches_y, Yvp_cpi_no_reg_g, "Non-Regularized Model")

# Regression report for the regularized model
train_mse_reg_cpi_g, train_mae_reg_cpi_g, val_mse_reg_cpi_g, val_mae_reg_cpi_g = regression_report(cpi_train_batches_y, Ytp_cpi_reg_g, cpi_test_batches_y, Yvp_cpi_reg_g, "Regularized Model")


```


:::



# 8. Discussion

We have explored the application of three prominent neural network architectures: RNNs, LSTMs and GRUs. By comparing their accuracy, predictive power, and the impact of regularization, we gained valuable insights into their capabilities and limitations. The results obtained from these models provide valuable insights into their performance and suitability for various time series forecasting tasks.

Comparing our deep learning models with traditional single-variable time series models, such as ARMA and ARIMA, using metrics like root mean square error (RMSE), allows us to assess the improvements offered by deep learning approaches. Lets compare and discuss the results.


::: panel-tabset

## Crude Oil



```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

results = []

# RNN results
results.append(['Crude Oil', 'RNN', 'No Reg', train_mse_no_reg, val_mse_no_reg])
results.append(['Crude Oil', 'RNN', 'Reg', train_mse_reg, val_mse_reg])

# LSTM results
results.append(['Crude Oil', 'LSTM', 'No Reg', train_mse_no_reg_l, val_mse_no_reg_l])
results.append(['Crude Oil', 'LSTM', 'Reg', train_mse_reg_l, val_mse_reg_l])

# GRU results
results.append(['Crude Oil', 'GRU', 'No Reg', train_mse_no_reg_g, val_mse_no_reg_g])
results.append(['Crude Oil', 'GRU', 'Reg', train_mse_reg_g, val_mse_reg_g])

# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)

result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)

print(result_df)
```

## Natural Gas

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

results = []

# RNN results
results.append(['Natural Gas', 'RNN', 'No Reg', train_mse_no_reg_gas, val_mse_no_reg_gas])
results.append(['Natural Gas', 'RNN', 'Reg', train_mse_reg_gas, val_mse_reg_gas])

# LSTM results
results.append(['Natural Gas', 'LSTM', 'No Reg', train_mse_no_reg_gas_l, val_mse_no_reg_gas_l])
results.append(['Natural Gas', 'LSTM', 'Reg', train_mse_reg_gas_l, val_mse_reg_gas_l])

# GRU results
results.append(['Natural Gas', 'GRU', 'No Reg', train_mse_no_reg_gas_g, val_mse_no_reg_gas_g])
results.append(['Natural Gas', 'GRU', 'Reg', train_mse_reg_gas_g, val_mse_reg_gas_g])

# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)

result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)

print(result_df)
```


## Electricity

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

results = []

# RNN results
results.append(['Electricity', 'RNN', 'No Reg', train_mse_no_reg_elec, val_mse_no_reg_elec])
results.append(['Electricity', 'RNN', 'Reg', train_mse_reg_elec, val_mse_reg_elec])

# LSTM results
results.append(['Electricity', 'LSTM', 'No Reg', train_mse_no_reg_elec_l, val_mse_no_reg_elec_l])
results.append(['Electricity', 'LSTM', 'Reg', train_mse_reg_elec_l, val_mse_reg_elec_l])

# GRU results
results.append(['Electricity', 'GRU', 'No Reg', train_mse_no_reg_electricity_g, val_mse_no_reg_electricity_g])
results.append(['Electricity', 'GRU', 'Reg', train_mse_reg_electricity_g, val_mse_reg_electricity_g])

# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)

result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)

print(result_df)
```


## GDP

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

results = []

# RNN results
results.append(['GDP', 'RNN', 'No Reg', train_mse_no_reg_gdp, val_mse_no_reg_gdp])
results.append(['GDP', 'RNN', 'Reg', train_mse_reg_gdp, val_mse_reg_gdp])

# LSTM results
results.append(['GDP', 'LSTM', 'No Reg', train_mse_no_reg_gdp_l, val_mse_no_reg_gdp_l])
results.append(['GDP', 'LSTM', 'Reg', train_mse_reg_gdp_l, val_mse_reg_gdp_l])

# GRU results
results.append(['GDP', 'GRU', 'No Reg', train_mse_no_reg_gdp_g, val_mse_no_reg_gdp_g])
results.append(['GDP', 'GRU', 'Reg', train_mse_reg_gdp_g, val_mse_reg_gdp_g])

# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)

result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)

print(result_df)
```



## CPI

```{python, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true

results = []

# RNN results
results.append(['CPI', 'RNN', 'No Reg', train_mse_no_reg_cpi, val_mse_no_reg_cpi])
results.append(['CPI', 'RNN', 'Reg', train_mse_reg_cpi, val_mse_reg_cpi])

# LSTM results
results.append(['CPI', 'LSTM', 'No Reg', train_mse_no_reg_cpi_l, val_mse_no_reg_cpi_l])
results.append(['CPI', 'LSTM', 'Reg', train_mse_reg_cpi_l, val_mse_reg_cpi_l])

# GRU results
results.append(['CPI', 'GRU', 'No Reg', train_mse_no_reg_cpi_g, val_mse_no_reg_cpi_g])
results.append(['CPI', 'GRU', 'Reg', train_mse_reg_cpi_g, val_mse_reg_cpi_g])

# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)

result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)

print(result_df)
```





:::


## 8.1 How do the results from the 3 different ANN models compare with each other in terms of accuracy and predictive power?


::: panel-tabset

## Crude Oil

Based on the validation RMSE (Root Mean Squared Error) values, which measure the models' predictive accuracy, the performance of the three ANN models for crude oil price forecasting can be ranked as follows:

GRU (Regularized): 0.764183
RNN (Regularized): 0.794744
LSTM (Regularized): 0.798898
LSTM (No Regularization): 0.832182
RNN (No Regularization): 0.841763
GRU (No Regularization): 0.856426

The GRU model with regularization demonstrates the highest accuracy and predictive power, closely followed by the RNN model with regularization. This suggests that regularization techniques are effective in improving the performance of RNN and GRU models for crude oil price forecasting.



## Natural Gas

Based on the validation RMSE (Root Mean Squared Error) values, the performance of the three ANN models can be ranked as follows:

LSTM (Regularized): 0.898807
GRU (Regularized): 0.906645
GRU (No Regularization): 1.103939
RNN (Regularized): 1.136816
LSTM (No Regularization): 1.152883
RNN (No Regularization): 1.155457


The LSTM model with regularization demonstrates the highest accuracy and predictive power, closely followed by the GRU model with regularization. This suggests that regularization techniques are effective in improving the performance of LSTM and GRU models for Natural Gas price forecasting.


## Electricity

Based on the validation RMSE (Root Mean Squared Error) values, the performance of the three ANN models can be ranked as follows:

GRU (No Regularization): 0.654712
RNN (No Regularization): 0.693142
LSTM (No Regularization): 0.696627
RNN (Regularized): 0.985358
GRU (Regularized): 1.266256
LSTM (Regularized): 1.340776


The GRU model with no regularization demonstrates the highest accuracy and predictive power, closely followed by the RNN model with no regularization. This suggests that non-regularization techniques are effective in improving the performance of GRU, RNN and LSTM models for Electricity price forecasting.


## GDP

Based on the validation RMSE (Root Mean Squared Error) values, the performance of the three ANN models can be ranked as follows:

RNN (No Regularization): 0.57151
GRU (No Regularization): 0.589705
LSTM (No Regularization): 0.601461
GRU (Regularized): 1.502871
LSTM (Regularized): 1.640242
RNN (Regularized): 1.904983


The RNN model with no regularization demonstrates the highest accuracy and predictive power, closely followed by the GRU model with no regularization. This suggests that non-regularization techniques are effective in improving the performance of GRU, RNN and LSTM models for GDP forecasting.


## CPI

Based on the validation RMSE (Root Mean Squared Error) values, the performance of the three ANN models can be ranked as follows:

GRU (No Regularization): 0.580746
RNN (No Regularization): 0.633293
LSTM (No Regularization): 0.706091
RNN (Regularized): 1.051578
LSTM (Regularized): 1.398939
GRU (Regularized): 1.523046


The GRU model with no-regularization demonstrates the highest accuracy and predictive power, closely followed by the RNN model with no regularization. 



:::



## 8.2 What effect does including regularization have on your results?


**Crude Oil:**

In the case of crude oil price forecasting, regularization has a positive impact on the performance of the models. The regularized GRU and RNN models outperform their non-regularized counterparts, indicating that regularization techniques help in reducing overfitting and improving the models' ability to generalize well to unseen data.

**Natural Gas:**

For natural gas price forecasting, regularization has a significant positive impact on the performance of the LSTM and GRU models. The regularized LSTM and GRU models achieve lower validation RMSE values compared to their non-regularized counterparts, suggesting that regularization helps in capturing the underlying patterns more effectively. 

**Electricity:**

In contrast to the crude oil and natural gas datasets, regularization has a negative impact on the performance of the models for electricity price forecasting. The non-regularized GRU, RNN, and LSTM models outperform regularized. This suggests that regularization techniques may not be suitable for capturing the complex patterns present in electricity price data.

**GDP:**

Similar to the electricity dataset, regularization has a negative effect on the performance of the models for GDP forecasting. The non-regularized RNN, GRU, and LSTM models achieve significantly lower validation RMSE values compared to regularized. This indicates that the models are able to capture the underlying patterns in the GDP data more effectively without regularization.

**CPI:**

Similar to the Electricity Prices and GDP, regularization has a negative effect on the performance of the models for CPI forecasting. The non-regularized RNN, GRU, and LSTM models achieve significantly lower validation RMSE values compared to regularized. This indicates that the models are able to capture the underlying patterns in the CPI data more effectively without regularization.



## 8.3 How far into the future can the deep learning model accurately predict the future?


The ability of the deep learning models to accurately predict future crude oil prices depends on various factors, such as the quality and quantity of historical data, the complexity of the underlying patterns, and the stability of the market dynamics. Based on the results, we can see that the models achieve relatively low validation RMSE values, indicating good predictive performance. 




## 8.4. How does your deep learning modeling compare to the traditional single-variable time-series ARMA/ARIMA models from HW-3?


::: panel-tabset

## Crude Oil

**From the ARIMA model** results for crude oil, we have:

Training set RMSE: 0.07735308

**From Deep Learning method:**

GRU (Regularized): 0.764183
RNN (Regularized): 0.794744
LSTM (Regularized): 0.798898
LSTM (No Regularization): 0.832182
RNN (No Regularization): 0.841763
GRU (No Regularization): 0.856426


Comparing these results, we can observe that the ARIMA model achieves a significantly lower RMSE value (0.07735308) compared to the deep learning models. This suggests that for crude oil price forecasting, the traditional single-variable time-series ARIMA model outperforms the deep learning models.




## Natural Gas

**From the ARIMA model** results for Natural Gas, we have:

Training set RMSE: 0.1178166

**From Deep Learning method:**

LSTM (Regularized): 0.898807
GRU (Regularized): 0.906645
GRU (No Regularization): 1.103939
RNN (Regularized): 1.136816
LSTM (No Regularization): 1.152883
RNN (No Regularization): 1.155457


Comparing these results, we can observe that the ARIMA model achieves a significantly lower RMSE value (0.1178166) compared to the deep learning models. This suggests that for Natural Gas price forecasting, the traditional single-variable time-series ARIMA model outperforms the deep learning models.


## Electricity

**From the ARIMA model** results for Electricity prives, we have:

Training set RMSE: 0.01863433

**From Deep Learning method:**

GRU (Regularized): 0.764183
RNN (Regularized): 0.794744
LSTM (Regularized): 0.798898
LSTM (No Regularization): 0.832182
RNN (No Regularization): 0.841763
GRU (No Regularization): 0.856426


Comparing these results, we can observe that the ARIMA model achieves a significantly lower RMSE value (0.01863433) compared to the deep learning models. This suggests that for electricity price forecasting, the traditional single-variable time-series ARIMA model outperforms the deep learning models.

## GDP

**From the ARIMA model** results for GDP, we have:

Training set RMSE: 0.01250364

**From Deep Learning method:**

RNN (No Regularization): 0.57151
GRU (No Regularization): 0.589705
LSTM (No Regularization): 0.601461
GRU (Regularized): 1.502871
LSTM (Regularized): 1.640242
RNN (Regularized): 1.904983


Comparing these results, we can observe that the ARIMA model achieves a significantly lower RMSE value (0.01250364) compared to the deep learning models. This suggests that for GDP forecasting, the traditional single-variable time-series ARIMA model outperforms the deep learning models.


## CPI

**From the ARIMA model** results for CPI, we have:

Training set RMSE: 0.002346106

**From Deep Learning method:**

GRU (No Regularization): 0.580746
RNN (No Regularization): 0.633293
LSTM (No Regularization): 0.706091
RNN (Regularized): 1.051578
LSTM (Regularized): 1.398939
GRU (Regularized): 1.523046


Comparing these results, we can observe that the ARIMA model achieves a significantly lower RMSE value (0.002346106) compared to the deep learning models. This suggests that for CPI forecasting, the traditional single-variable time-series ARIMA model outperforms the deep learning models.





:::



In summary, across all the datasets (Crude Oil, Natural Gas, Electricity, GDP, and CPI), the traditional single-variable time-series ARIMA models consistently outperform the deep learning models in terms of RMSE. The ARIMA models achieve significantly lower RMSE values compared to the deep learning models, indicating their superior predictive accuracy for these specific datasets.




# 9. Write a discussion paragraph Comparing your models (use RMSE) and forecasts from these sections with your Deep Learning Models.


When comparing the traditional ARIMA models with the deep learning models (RNNs, LSTMs, and GRUs) for forecasting various economic variables such as crude oil prices, natural gas prices, electricity prices, GDP, and CPI, a clear pattern emerges. Across all these datasets, the ARIMA models consistently outperform the deep learning models as seen by their lower RMSE values.

For crude oil price forecasting, the ARIMA model achieves an RMSE of 0.07735308, while the best-performing deep learning model (GRU with regularization) has an RMSE of 0.764183. Similarly, for natural gas prices, the ARIMA model's RMSE is 0.1178166, considerably lower than the LSTM with regularization, which has an RMSE of 0.898807.

These results suggest that, the traditional ARIMA models are more effective in capturing the underlying patterns and making accurate predictions compared to the more complex deep learning approaches. While deep learning models have shown great success in various domains, it appears that the simpler ARIMA models are better suited for these specific time-series forecasting problems. 




<a href="financial-ts-models.qmd" class="previous-page-link" style="float: left;">&larr; Previous Page: Financial TS</a>



<a href="conclusions.qmd" class="next-page-link" style="float: right;">Next Page: Conclusion &rarr;</a>



