batch_size = 64 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 100 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_l = keras.Sequential()
model_no_reg_l.add(Input(shape=input_shape))
model_no_reg_l.add(LSTM(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_l.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_l = keras.Sequential()
model_reg_l.add(Input(shape=input_shape))
model_reg_l.add(LSTM(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_l.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("LSTM Model without regularization:")
print(model_no_reg_l.summary())
print("LSTM Model with regularization:")
print(model_reg_l.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_gas_no_reg_l = model_no_reg_l.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
history_oil_reg_l = model_reg_l.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_l, history_oil_reg_l)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_l.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_oil_no_reg_l = model_no_reg_l.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
history_oil_reg_l = model_reg_l.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_l, history_oil_reg_l)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_oil_no_reg_l = model_no_reg_l.predict(oil_train_batches_x)
Yvp_oil_no_reg_l = model_no_reg_l.predict(oil_test_batches_x)
Ytp_oil_reg_l = model_reg_l.predict(oil_train_batches_x)
Yvp_oil_reg_l = model_reg_l.predict(oil_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_l, train_mae_no_reg_l, val_mse_no_reg_l, val_mae_no_reg_l = regression_report(oil_train_batches_y, Ytp_oil_no_reg_l, oil_test_batches_y, Yvp_oil_no_reg_l, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_l, train_mae_reg_l, val_mse_reg_l, val_mae_reg_l = regression_report(oil_train_batches_y, Ytp_oil_reg_l, oil_test_batches_y, Yvp_oil_reg_l, "Regularized Model")
############HYPERPARAMETER
# Define the number of features
features = 1
# Model definition function
def get_model(optimizer="adam", units=50, L2=0.01):
model = Sequential()
model.add(GRU(units, input_shape=(lookback, features), activation='relu',
recurrent_regularizer=regularizers.l2(L2)))
model.add(Dense(1, activation='linear'))
model.compile(loss="mse", optimizer=optimizer)
return model
# Create the SciKeras wrapper and specify training-related parameters
model = KerasRegressor(model=get_model, model__optimizer="adam", model__units=50, model__L2=0.01)
# Define the hyperparameter space to search
param_dist = {
'model__optimizer': ['adam', 'rmsprop'],
'model__units': [30, 50, 100],
'model__L2': [0.01, 0.001, 0.0001],
'batch_size': [32, 64, 128],
'epochs': [50, 100, 200]
}
X_train = oil_train_batches_x
y_train = oil_train_batches_y
# Initialize and fit the RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3)
random_search.fit(X_train, y_train)
# Print the best score and parameters
print("Best score: %f using %s" % (random_search.best_score_, random_search.best_params_))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (oil_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 30 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_g = keras.Sequential()
model_no_reg_g.add(Input(shape=input_shape))
model_no_reg_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_g.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_g = keras.Sequential()
model_reg_g.add(Input(shape=input_shape))
model_reg_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_g.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_g.summary())
print("GRU Model with regularization:")
print(model_reg_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (oil_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 64 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 30 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_g = keras.Sequential()
model_no_reg_g.add(Input(shape=input_shape))
model_no_reg_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_g.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_g = keras.Sequential()
model_reg_g.add(Input(shape=input_shape))
model_reg_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_g.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_g.summary())
print("GRU Model with regularization:")
print(model_reg_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_oil_no_reg_g = model_no_reg_g.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
history_oil_reg_g = model_reg_g.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg_g, history_oil_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_oil_no_reg_g = model_no_reg_g.predict(oil_train_batches_x)
Yvp_oil_no_reg_g = model_no_reg_g.predict(oil_test_batches_x)
Ytp_oil_reg_g = model_reg_g.predict(oil_train_batches_x)
Yvp_oil_reg_g = model_reg_g.predict(oil_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_g, train_mae_no_reg_g, val_mse_no_reg_g, val_mae_no_reg_g = regression_report(oil_train_batches_y, Ytp_oil_no_reg_g, oil_test_batches_y, Yvp_oil_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_g, train_mae_reg_g, val_mse_reg_g, val_mae_reg_g = regression_report(oil_train_batches_y, Ytp_oil_reg_g, oil_test_batches_y, Yvp_oil_reg_g, "Regularized Model")
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
results = []
# RNN results
results.append(['Crude Oil', 'RNN', 'No Reg', train_mse_no_reg, val_mse_no_reg])
results.append(['Crude Oil', 'RNN', 'Reg', train_mse_reg, val_mse_reg])
# LSTM results
results.append(['Crude Oil', 'LSTM', 'No Reg', train_mse_no_reg_l, val_mse_no_reg_l])
results.append(['Crude Oil', 'LSTM', 'Reg', train_mse_reg_l, val_mse_reg_l])
# GRU results
results.append(['Crude Oil', 'GRU', 'No Reg', train_mse_no_reg_g, val_mse_no_reg_g])
results.append(['Crude Oil', 'GRU', 'Reg', train_mse_reg_g, val_mse_reg_g])
# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)
result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)
print(result_df)
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false
library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)
reticulate::repl_python()
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
reticulate::repl_python()
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
reticulate::repl_python()
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
reticulate::repl_python()
