#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_electricity_g.summary())
print("GRU Model with regularization:")
print(model_reg_electricity_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_electricity_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_electricity_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_electricity_no_reg_g = model_no_reg_electricity_g.fit(electricity_train_batches_x,
electricity_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(electricity_test_batches_x, electricity_test_batches_y),
callbacks=[early_stopping])
history_electricity_reg_g = model_reg_electricity_g.fit(electricity_train_batches_x,
electricity_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(electricity_test_batches_x, electricity_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_electricity_no_reg_g, history_electricity_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_electricity_no_reg_g = model_no_reg_electricity_g.predict(electricity_train_batches_x)
Yvp_electricity_no_reg_g = model_no_reg_electricity_g.predict(electricity_test_batches_x)
Ytp_electricity_reg_g = model_reg_electricity_g.predict(electricity_train_batches_x)
Yvp_electricity_reg_g = model_reg_electricity_g.predict(electricity_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_electricity_g, train_mae_no_reg_electricity_g, val_mse_no_reg_electricity_g, val_mae_no_reg_electricity_g = regression_report(electricity_train_batches_y, Ytp_electricity_no_reg_g, electricity_test_batches_y, Yvp_electricity_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_electricity_g, train_mae_reg_electricity_g, val_mse_reg_electricity_g, val_mae_reg_electricity_g = regression_report(electricity_train_batches_y, Ytp_electricity_reg_g, electricity_test_batches_y, Yvp_electricity_reg_g, "Regularized Model")
############HYPERPARAMETER
# Define the number of features
features = 1
# Model definition function
def get_model(optimizer="adam", units=50, L2=0.01):
model = Sequential()
model.add(GRU(units, input_shape=(lookback, features), activation='relu',
recurrent_regularizer=regularizers.l2(L2)))
model.add(Dense(1, activation='linear'))
model.compile(loss="mse", optimizer=optimizer)
return model
# Create the SciKeras wrapper and specify training-related parameters
model = KerasRegressor(model=get_model, model__optimizer="adam", model__units=50, model__L2=0.01)
# Define the hyperparameter space to search
param_dist = {
'model__optimizer': ['adam', 'rmsprop'],
'model__units': [30, 50, 100],
'model__L2': [0.01, 0.001, 0.0001],
'batch_size': [32, 64, 128],
'epochs': [50, 100, 200]
}
X_train = gdp_train_batches_x
y_train = gdp_train_batches_y
# Initialize and fit the RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3)
random_search.fit(X_train, y_train)
# Print the best score and parameters
print("Best score: %f using %s" % (random_search.best_score_, random_search.best_params_))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gdp_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 30 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gdp_g = keras.Sequential()
model_no_reg_gdp_g.add(Input(shape=input_shape))
model_no_reg_gdp_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_gdp_g.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_gdp_g = keras.Sequential()
model_reg_gdp_g.add(Input(shape=input_shape))
model_reg_gdp_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_gdp_g.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_gdp_g.summary())
print("GRU Model with regularization:")
print(model_reg_gdp_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_gdp_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_gdp_no_reg_g = model_no_reg_gdp_g.fit(gdp_train_batches_x,
gdp_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gdp_test_batches_x, gdp_test_batches_y),
callbacks=[early_stopping])
history_gdp_reg_g = model_reg_gdp_g.fit(gdp_train_batches_x,
gdp_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gdp_test_batches_x, gdp_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg_g, history_gdp_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_gdp_no_reg_g = model_no_reg_gdp_g.predict(gdp_train_batches_x)
Yvp_gdp_no_reg_g = model_no_reg_gdp_g.predict(gdp_test_batches_x)
Ytp_gdp_reg_g = model_reg_gdp_g.predict(gdp_train_batches_x)
Yvp_gdp_reg_g = model_reg_gdp_g.predict(gdp_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_gdp_g, train_mae_no_reg_gdp_g, val_mse_no_reg_gdp_g, val_mae_no_reg_gdp_g = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg_g, gdp_test_batches_y, Yvp_gdp_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_gdp_g, train_mae_reg_gdp_g, val_mse_reg_gdp_g, val_mae_reg_gdp_g = regression_report(gdp_train_batches_y, Ytp_gdp_reg_g, gdp_test_batches_y, Yvp_gdp_reg_g, "Regularized Model")
############HYPERPARAMETER
# Define the number of features
features = 1
# Model definition function
def get_model(optimizer="adam", units=50, L2=0.01):
model = Sequential()
model.add(GRU(units, input_shape=(lookback, features), activation='relu',
recurrent_regularizer=regularizers.l2(L2)))
model.add(Dense(1, activation='linear'))
model.compile(loss="mse", optimizer=optimizer)
return model
# Create the SciKeras wrapper and specify training-related parameters
model = KerasRegressor(model=get_model, model__optimizer="adam", model__units=50, model__L2=0.01)
# Define the hyperparameter space to search
param_dist = {
'model__optimizer': ['adam', 'rmsprop'],
'model__units': [30, 50, 100],
'model__L2': [0.01, 0.001, 0.0001],
'batch_size': [32, 64, 128],
'epochs': [50, 100, 200]
}
X_train = cpi_train_batches_x
y_train = cpi_train_batches_y
# Initialize and fit the RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3)
random_search.fit(X_train, y_train)
# Print the best score and parameters
print("Best score: %f using %s" % (random_search.best_score_, random_search.best_params_))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (cpi_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_cpi_g = keras.Sequential()
model_no_reg_cpi_g.add(Input(shape=input_shape))
model_no_reg_cpi_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_cpi_g.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_cpi_g = keras.Sequential()
model_reg_cpi_g.add(Input(shape=input_shape))
model_reg_cpi_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_cpi_g.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_cpi_g.summary())
print("GRU Model with regularization:")
print(model_reg_cpi_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_cpi_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_cpi_no_reg_g = model_no_reg_cpi_g.fit(cpi_train_batches_x,
cpi_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(cpi_test_batches_x, cpi_test_batches_y),
callbacks=[early_stopping])
history_cpi_reg_g = model_reg_cpi_g.fit(cpi_train_batches_x,
cpi_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(cpi_test_batches_x, cpi_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg_g, history_cpi_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_cpi_no_reg_g = model_no_reg_cpi_g.predict(cpi_train_batches_x)
Yvp_cpi_no_reg_g = model_no_reg_cpi_g.predict(cpi_test_batches_x)
Ytp_cpi_reg_g = model_reg_cpi_g.predict(cpi_train_batches_x)
Yvp_cpi_reg_g = model_reg_cpi_g.predict(cpi_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_cpi_g, train_mae_no_reg_cpi_g, val_mse_no_reg_cpi_g, val_mae_no_reg_cpi_g = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg_g, cpi_test_batches_y, Yvp_cpi_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_cpi_g, train_mae_reg_cpi_g, val_mse_reg_cpi_g, val_mae_reg_cpi_g = regression_report(cpi_train_batches_y, Ytp_cpi_reg_g, cpi_test_batches_y, Yvp_cpi_reg_g, "Regularized Model")
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false
library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false
library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)
reticulate::repl_python()
