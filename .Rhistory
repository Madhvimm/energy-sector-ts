#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
count = 0
x_out = []
y_out = []
i_start = 0
# Adjust the stopping condition for the loop to account for delay
while i_start + lookback + delay - 1 < len(x):
i_stop = i_start + lookback
i_pred = i_stop + delay - 1
indices_to_keep = list(range(i_start, i_stop, step))
xtmp = x[indices_to_keep]
ytmp = x[i_pred]
x_out.append(xtmp)
y_out.append(ytmp)
if verbose and count < 2:
plt.plot(np.arange(len(x)), x, 'b-', label='Value')
plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.show()
# Update start point
i_start += step
count += 1
return np.array(x_out), np.array(y_out)
lookback = 5
# Convert to numpy arrays
electricity_train = np.array(electricity_train)
electricity_test = np.array(electricity_test)
# Create mini-batches
electricity_train_batches_x, electricity_train_batches_y = form_arrays(electricity_train, lookback=lookback, delay=1, step=1)
electricity_test_batches_x, electricity_test_batches_y = form_arrays(electricity_test, lookback=lookback, delay=1, step=1)
# Print shapes
print(f"Train shape: {electricity_train_batches_x.shape}, {electricity_train_batches_y.shape}")
print(f"Test shape: {electricity_test_batches_x.shape}, {electricity_test_batches_y.shape}")
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
count = 0
x_out = []
y_out = []
i_start = 0
# Adjust the stopping condition for the loop to account for delay
while i_start + lookback + delay - 1 < len(x):
i_stop = i_start + lookback
i_pred = i_stop + delay - 1
indices_to_keep = list(range(i_start, i_stop, step))
xtmp = x[indices_to_keep]
ytmp = x[i_pred]
x_out.append(xtmp)
y_out.append(ytmp)
if verbose and count < 2:
plt.plot(np.arange(len(x)), x, 'b-', label='Value')
plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.show()
# Update start point
i_start += step
count += 1
return np.array(x_out), np.array(y_out)
lookback = 5
# Convert to numpy arrays
gdp_train = np.array(gdp_train)
gdp_test = np.array(gdp_test)
# Create mini-batches
gdp_train_batches_x, gdp_train_batches_y = form_arrays(gdp_train, lookback=lookback, delay=1, step=1)
gdp_test_batches_x, gdp_test_batches_y = form_arrays(gdp_test, lookback=lookback, delay=1, step=1)
# Print shapes
print(f"Train shape: {gdp_train_batches_x.shape}, {gdp_train_batches_y.shape}")
print(f"Test shape: {gdp_test_batches_x.shape}, {gdp_test_batches_y.shape}")
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
def form_arrays(x, lookback=3, delay=1, step=1, verbose=False):
count = 0
x_out = []
y_out = []
i_start = 0
# Adjust the stopping condition for the loop to account for delay
while i_start + lookback + delay - 1 < len(x):
i_stop = i_start + lookback
i_pred = i_stop + delay - 1
indices_to_keep = list(range(i_start, i_stop, step))
xtmp = x[indices_to_keep]
ytmp = x[i_pred]
x_out.append(xtmp)
y_out.append(ytmp)
if verbose and count < 2:
plt.plot(np.arange(len(x)), x, 'b-', label='Value')
plt.scatter(indices_to_keep, xtmp, color='green', label='Features', marker='o')
plt.scatter(i_pred, ytmp, color='red', label='Target', marker='o')
plt.xlabel('Index')
plt.ylabel('Value')
plt.legend()
plt.show()
# Update start point
i_start += step
count += 1
return np.array(x_out), np.array(y_out)
lookback = 5
# Convert to numpy arrays
cpi_train = np.array(cpi_train)
cpi_test = np.array(cpi_test)
# Create mini-batches
cpi_train_batches_x, cpi_train_batches_y = form_arrays(cpi_train, lookback=lookback, delay=1, step=1)
cpi_test_batches_x, cpi_test_batches_y = form_arrays(cpi_test, lookback=lookback, delay=1, step=1)
# Print shapes
print(f"Train shape: {cpi_train_batches_x.shape}, {cpi_train_batches_y.shape}")
print(f"Test shape: {cpi_test_batches_x.shape}, {cpi_test_batches_y.shape}")
from scikeras.wrappers import KerasRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from sklearn.model_selection import RandomizedSearchCV
import numpy as np
# Define the number of features
features = 1
# Model definition function
def get_model(optimizer="adam", units=50, L2=0.01):
model = Sequential()
model.add(SimpleRNN(units, input_shape=(lookback, features), activation='relu',
recurrent_regularizer=regularizers.l2(L2)))
model.add(Dense(1, activation='linear'))
model.compile(loss="mse", optimizer=optimizer)
return model
# Create the SciKeras wrapper and specify training-related parameters
model = KerasRegressor(model=get_model, model__optimizer="adam", model__units=50, model__L2=0.01)
# Define the hyperparameter space to search
param_dist = {
'model__optimizer': ['adam', 'rmsprop'],
'model__units': [30, 50, 100],
'model__L2': [0.01, 0.001, 0.0001],
'batch_size': [32, 64, 128],
'epochs': [50, 100, 200]
}
X_train = gas_train_batches_x
y_train = gas_train_batches_y
# Initialize and fit the RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3)
random_search.fit(X_train, y_train)
# Print the best score and parameters
print("Best score: %f using %s" % (random_search.best_score_, random_search.best_params_))
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gas_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 32  # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters
# CREATE MODEL
model_no_reg = keras.Sequential()
# ADD RECURRENT LAYER
# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gas = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
Dense(units=1, activation='linear')
])
#CREATE MODEL WITH REGULARIZATION
model_reg_gas = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
recurrent_regularizer=regularizers.l2(L2)),
Dense(units=1, activation='linear')
])
model_no_reg_gas.add(Dense(units=1, activation='linear'))
model_reg_gas.add(Dense(units=1, activation='linear'))
# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gas.summary())
print("Model with regularization:")
print(model_reg_gas.summary())
optimizer = "rmsprop"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (oil_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 64  # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters
# CREATE MODEL
model_no_reg = keras.Sequential()
# ADD RECURRENT LAYER
# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
Dense(units=1, activation='linear')
])
#CREATE MODEL WITH REGULARIZATION
model_reg = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
recurrent_regularizer=regularizers.l2(L2)),
Dense(units=1, activation='linear')
])
# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg.summary())
print("Model with regularization:")
print(model_reg.summary())
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_oil_no_reg = model_no_reg.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
history_oil_reg = model_reg.fit(oil_train_batches_x,
oil_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(oil_test_batches_x, oil_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_oil_no_reg, history_oil_reg)
# PREDICTIONS
Ytp_oil_no_reg = model_no_reg.predict(oil_train_batches_x)
Yvp_oil_no_reg = model_no_reg.predict(oil_test_batches_x)
Ytp_oil_reg = model_reg.predict(oil_train_batches_x)
Yvp_oil_reg = model_reg.predict(oil_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual Crude Oil Prices', ylabel='Predicted Crude Oil Prices',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Crude Oil Prices (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg, train_mae_no_reg, val_mse_no_reg, val_mae_no_reg = regression_report(oil_train_batches_y, Ytp_oil_no_reg, oil_test_batches_y, Yvp_oil_no_reg, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg, train_mae_reg, val_mse_reg, val_mae_reg = regression_report(oil_train_batches_y, Ytp_oil_reg, oil_test_batches_y, Yvp_oil_reg, "Regularized Model")
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gas_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 32  # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters
# CREATE MODEL
model_no_reg = keras.Sequential()
# ADD RECURRENT LAYER
# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gas = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
Dense(units=1, activation='linear')
])
#CREATE MODEL WITH REGULARIZATION
model_reg_gas = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
recurrent_regularizer=regularizers.l2(L2)),
Dense(units=1, activation='linear')
])
# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gas.summary())
print("Model with regularization:")
print(model_reg_gas.summary())
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_gas_no_reg = model_no_reg.fit(gas_train_batches_x,
gas_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gas_test_batches_x, gas_test_batches_y),
callbacks=[early_stopping])
history_gas_reg = model_reg.fit(gas_train_batches_x,
gas_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gas_test_batches_x, gas_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg, history_gas_reg)
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
number_of_epochs = 100 # Update based on the best hyperparameters
L2 = 0.0001  # Update based on the best hyperparameters
input_shape = (gas_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
#batch_size=1                       # stocastic training
#batch_size=int(len(Xt1_oil)/2.)    # mini-batch training
batch_size = 32  # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 100  # Update based on the best hyperparameters
# CREATE MODEL
model_no_reg = keras.Sequential()
# ADD RECURRENT LAYER
# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU
#model.add(LSTM(
#model.add(GRU(
model_no_reg_gas = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu'),
Dense(units=1, activation='linear')
])
#CREATE MODEL WITH REGULARIZATION
model_reg_gas = Sequential([
Input(shape=input_shape),  # Add Input layer to define input shape
SimpleRNN(units=recurrent_hidden_units, return_sequences=False, activation='relu',
recurrent_regularizer=regularizers.l2(L2)),
Dense(units=1, activation='linear')
])
# MODEL SUMMARY
print("Model without regularization:")
print(model_no_reg_gas.summary())
print("Model with regularization:")
print(model_reg_gas.summary())
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gas.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_gas_no_reg = model_no_reg.fit(gas_train_batches_x,
gas_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gas_test_batches_x, gas_test_batches_y),
callbacks=[early_stopping])
history_gas_reg = model_reg.fit(gas_train_batches_x,
gas_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gas_test_batches_x, gas_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_gas_no_reg, history_gas_reg)
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false
library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)
reticulate::repl_python()
