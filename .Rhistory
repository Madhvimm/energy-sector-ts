val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_electricity_no_reg_g, history_electricity_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_electricity_no_reg_g = model_no_reg_electricity_g.predict(electricity_train_batches_x)
Yvp_electricity_no_reg_g = model_no_reg_electricity_g.predict(electricity_test_batches_x)
Ytp_electricity_reg_g = model_reg_electricity_g.predict(electricity_train_batches_x)
Yvp_electricity_reg_g = model_reg_electricity_g.predict(electricity_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual Electricity Prices', ylabel='Predicted Electricity Prices',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='Electricity Prices (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_electricity_g, train_mae_no_reg_electricity_g, val_mse_no_reg_electricity_g, val_mae_no_reg_electricity_g = regression_report(electricity_train_batches_y, Ytp_electricity_no_reg_g, electricity_test_batches_y, Yvp_electricity_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_electricity_g, train_mae_reg_electricity_g, val_mse_reg_electricity_g, val_mae_reg_electricity_g = regression_report(electricity_train_batches_y, Ytp_electricity_reg_g, electricity_test_batches_y, Yvp_electricity_reg_g, "Regularized Model")
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (gdp_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 30 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_gdp_g = keras.Sequential()
model_no_reg_gdp_g.add(Input(shape=input_shape))
model_no_reg_gdp_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_gdp_g.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_gdp_g = keras.Sequential()
model_reg_gdp_g.add(Input(shape=input_shape))
model_reg_gdp_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_gdp_g.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_gdp_g.summary())
print("GRU Model with regularization:")
print(model_reg_gdp_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_gdp_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_gdp_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_gdp_no_reg_g = model_no_reg_gdp_g.fit(gdp_train_batches_x,
gdp_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gdp_test_batches_x, gdp_test_batches_y),
callbacks=[early_stopping])
history_gdp_reg_g = model_reg_gdp_g.fit(gdp_train_batches_x,
gdp_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(gdp_test_batches_x, gdp_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_gdp_no_reg_g, history_gdp_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_gdp_no_reg_g = model_no_reg_gdp_g.predict(gdp_train_batches_x)
Yvp_gdp_no_reg_g = model_no_reg_gdp_g.predict(gdp_test_batches_x)
Ytp_gdp_reg_g = model_reg_gdp_g.predict(gdp_train_batches_x)
Yvp_gdp_reg_g = model_reg_gdp_g.predict(gdp_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual GDP', ylabel='Predicted GDP',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='GDP (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_gdp_g, train_mae_no_reg_gdp_g, val_mse_no_reg_gdp_g, val_mae_no_reg_gdp_g = regression_report(gdp_train_batches_y, Ytp_gdp_no_reg_g, gdp_test_batches_y, Yvp_gdp_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_gdp_g, train_mae_reg_gdp_g, val_mse_reg_gdp_g, val_mae_reg_gdp_g = regression_report(gdp_train_batches_y, Ytp_gdp_reg_g, gdp_test_batches_y, Yvp_gdp_reg_g, "Regularized Model")
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
optimizer = "adam"
loss_function = "MeanSquaredError"
learning_rate = 0.001
numbers_epochs = 200
L2_reg = 0.0001
L2_no_reg = 0
input_shape = (cpi_train_batches_x.shape[1], 1)
# ------ Choose the batch size ------
# batch_size = 1 # stochastic training
# batch_size = int(len(trainX) / 2.) # mini-batch training
batch_size = 32 # Update based on the best hyperparameters
# BUILD MODEL
recurrent_hidden_units = 50 # Update based on the best hyperparameters
# CREATE MODEL WITHOUT REGULARIZATION
model_no_reg_cpi_g = keras.Sequential()
model_no_reg_cpi_g.add(Input(shape=input_shape))
model_no_reg_cpi_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_no_reg),
activation='relu'
))
model_no_reg_cpi_g.add(Dense(units=1, activation='linear'))
# CREATE MODEL WITH REGULARIZATION
model_reg_cpi_g = keras.Sequential()
model_reg_cpi_g.add(Input(shape=input_shape))
model_reg_cpi_g.add(GRU(
units=recurrent_hidden_units,
return_sequences=False,
recurrent_regularizer=regularizers.L2(L2_reg),
activation='relu'
))
model_reg_cpi_g.add(Dense(units=1, activation='linear'))
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# MODEL SUMMARY
print("GRU Model without regularization:")
print(model_no_reg_cpi_g.summary())
print("GRU Model with regularization:")
print(model_reg_cpi_g.summary())
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# # print("initial parameters:", model.get_weights())
# COMPILING THE MODELS
model_no_reg_cpi_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
model_reg_cpi_g.compile(optimizer=RMSprop(learning_rate=learning_rate), loss=loss_function)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# TRAINING THE MODELS
history_cpi_no_reg_g = model_no_reg_cpi_g.fit(cpi_train_batches_x,
cpi_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(cpi_test_batches_x, cpi_test_batches_y),
callbacks=[early_stopping])
history_cpi_reg_g = model_reg_cpi_g.fit(cpi_train_batches_x,
cpi_train_batches_y,
epochs=number_of_epochs,
batch_size=batch_size,
verbose=False,
validation_data=(cpi_test_batches_x, cpi_test_batches_y),
callbacks=[early_stopping])
def history_plot_separate(history_no_reg, history_reg):
# Get the history dictionaries
history_dict_no_reg = history_no_reg.history
history_dict_reg = history_reg.history
# Get the loss values and epochs for both models
loss_values_no_reg = history_dict_no_reg['loss']
val_loss_values_no_reg = history_dict_no_reg['val_loss']
loss_values_reg = history_dict_reg['loss']
val_loss_values_reg = history_dict_reg['val_loss']
# Determine the number of epochs the training actually ran for
epochs_no_reg = range(1, len(loss_values_no_reg) + 1)
epochs_reg = range(1, len(loss_values_reg) + 1)
# Plot for non-regularized model
plt.figure()
plt.plot(epochs_no_reg, loss_values_no_reg, 'bo-', label='Training Loss (No Reg)')
plt.plot(epochs_no_reg, val_loss_values_no_reg, 'b-', label='Validation Loss (No Reg)')
plt.title('Training and Validation Loss for Non-Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Plot for regularized model
plt.figure()
plt.plot(epochs_reg, loss_values_reg, 'ro-', label='Training Loss (Reg)')
plt.plot(epochs_reg, val_loss_values_reg, 'r-', label='Validation Loss (Reg)')
plt.title('Training and Validation Loss for Regularized Model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Call the function to plot the history separately
history_plot_separate(history_cpi_no_reg_g, history_cpi_reg_g)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
# PREDICTIONS
Ytp_cpi_no_reg_g = model_no_reg_cpi_g.predict(cpi_train_batches_x)
Yvp_cpi_no_reg_g = model_no_reg_cpi_g.predict(cpi_test_batches_x)
Ytp_cpi_reg_g = model_reg_cpi_g.predict(cpi_train_batches_x)
Yvp_cpi_reg_g = model_reg_cpi_g.predict(cpi_test_batches_x)
def regression_report(yt, ytp, yv, yvp, model_name):
print(f"--------- Regression Report ({model_name}) ---------")
print("TRAINING:")
train_mse = np.mean((yt - ytp) ** 2)
train_mae = np.mean(np.abs(yt - ytp))
print("MSE", train_mse)
print("MAE", train_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yt, ytp, 'ro')
ax.plot(yt, yt, 'b-')
ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
title=f'Training Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
frac_plot = 1.0
upper = int(frac_plot * yt.shape[0])
fig, ax = plt.subplots()
ax.plot(yt[0:upper], 'b-')
ax.plot(ytp[0:upper], 'r-', alpha=0.5)
ax.plot(ytp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
title=f'Training: Time-Series Prediction ({model_name})')
plt.show()
print("VALIDATION:")
val_mse = np.mean((yv - yvp) ** 2)
val_mae = np.mean(np.abs(yv - yvp))
print("MSE", val_mse)
print("MAE", val_mae)
# PARITY PLOT
fig, ax = plt.subplots()
ax.plot(yv, yvp, 'ro')
ax.plot(yv, yv, 'b-')
ax.set(xlabel='Actual CPI', ylabel='Predicted CPI',
title=f'Validation Data Parity Plot ({model_name})')
plt.show()
# PLOT PART OF THE PREDICTED TIME-SERIES
upper = int(frac_plot * yv.shape[0])
fig, ax = plt.subplots()
ax.plot(yv[0:upper], 'b-')
ax.plot(yvp[0:upper], 'r-', alpha=0.5)
ax.plot(yvp[0:upper], 'ro', alpha=0.25)
ax.set(xlabel='Time', ylabel='CPI (Blue=Actual & Red=Prediction)',
title=f'Validation: Time-Series Prediction ({model_name})')
plt.show()
return train_mse, train_mae, val_mse, val_mae
# Regression report for the non-regularized model
train_mse_no_reg_cpi_g, train_mae_no_reg_cpi_g, val_mse_no_reg_cpi_g, val_mae_no_reg_cpi_g = regression_report(cpi_train_batches_y, Ytp_cpi_no_reg_g, cpi_test_batches_y, Yvp_cpi_no_reg_g, "Non-Regularized Model")
# Regression report for the regularized model
train_mse_reg_cpi_g, train_mae_reg_cpi_g, val_mse_reg_cpi_g, val_mae_reg_cpi_g = regression_report(cpi_train_batches_y, Ytp_cpi_reg_g, cpi_test_batches_y, Yvp_cpi_reg_g, "Regularized Model")
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
results = []
# RNN results
results.append(['Crude Oil', 'RNN', 'No Reg', train_mse_no_reg, val_mse_no_reg])
results.append(['Crude Oil', 'RNN', 'Reg', train_mse_reg, val_mse_reg])
# LSTM results
results.append(['Crude Oil', 'LSTM', 'No Reg', train_mse_no_reg_l, val_mse_no_reg_l])
results.append(['Crude Oil', 'LSTM', 'Reg', train_mse_reg_l, val_mse_reg_l])
# GRU results
results.append(['Crude Oil', 'GRU', 'No Reg', train_mse_no_reg_g, val_mse_no_reg_g])
results.append(['Crude Oil', 'GRU', 'Reg', train_mse_reg_g, val_mse_reg_g])
# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)
result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)
print(result_df)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
results = []
# RNN results
results.append(['Natural Gas', 'RNN', 'No Reg', train_mse_no_reg_gas, val_mse_no_reg_gas])
results.append(['Natural Gas', 'RNN', 'Reg', train_mse_reg_gas, val_mse_reg_gas])
# LSTM results
results.append(['Natural Gas', 'LSTM', 'No Reg', train_mse_no_reg_gas_l, val_mse_no_reg_gas_l])
results.append(['Natural Gas', 'LSTM', 'Reg', train_mse_reg_gas_l, val_mse_reg_gas_l])
# GRU results
results.append(['Natural Gas', 'GRU', 'No Reg', train_mse_no_reg_gas_g, val_mse_no_reg_gas_g])
results.append(['Natural Gas', 'GRU', 'Reg', train_mse_reg_gas_g, val_mse_reg_gas_g])
# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)
result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)
print(result_df)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
results = []
# RNN results
results.append(['Electricity', 'RNN', 'No Reg', train_mse_no_reg_elec, val_mse_no_reg_elec])
results.append(['Electricity', 'RNN', 'Reg', train_mse_reg_elec, val_mse_reg_elec])
# LSTM results
results.append(['Electricity', 'LSTM', 'No Reg', train_mse_no_reg_elec_l, val_mse_no_reg_elec_l])
results.append(['Electricity', 'LSTM', 'Reg', train_mse_reg_elec_l, val_mse_reg_elec_l])
# GRU results
results.append(['Electricity', 'GRU', 'No Reg', train_mse_no_reg_electricity_g, val_mse_no_reg_electricity_g])
results.append(['Electricity', 'GRU', 'Reg', train_mse_reg_electricity_g, val_mse_reg_electricity_g])
# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)
result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)
print(result_df)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
results = []
# RNN results
results.append(['GDP', 'RNN', 'No Reg', train_mse_no_reg_gdp, val_mse_no_reg_gdp])
results.append(['GDP', 'RNN', 'Reg', train_mse_reg_gdp, val_mse_reg_gdp])
# LSTM results
results.append(['GDP', 'LSTM', 'No Reg', train_mse_no_reg_gdp_l, val_mse_no_reg_gdp_l])
results.append(['GDP', 'LSTM', 'Reg', train_mse_reg_gdp_l, val_mse_reg_gdp_l])
# GRU results
results.append(['GDP', 'GRU', 'No Reg', train_mse_no_reg_gdp_g, val_mse_no_reg_gdp_g])
results.append(['GDP', 'GRU', 'Reg', train_mse_reg_gdp_g, val_mse_reg_gdp_g])
# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)
result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)
print(result_df)
#| code-fold: true
#| code-summary: 'Code'
#| warning: false
#| output: true
results = []
# RNN results
results.append(['CPI', 'RNN', 'No Reg', train_mse_no_reg_cpi, val_mse_no_reg_cpi])
results.append(['CPI', 'RNN', 'Reg', train_mse_reg_cpi, val_mse_reg_cpi])
# LSTM results
results.append(['CPI', 'LSTM', 'No Reg', train_mse_no_reg_cpi_l, val_mse_no_reg_cpi_l])
results.append(['CPI', 'LSTM', 'Reg', train_mse_reg_cpi_l, val_mse_reg_cpi_l])
# GRU results
results.append(['CPI', 'GRU', 'No Reg', train_mse_no_reg_cpi_g, val_mse_no_reg_cpi_g])
results.append(['CPI', 'GRU', 'Reg', train_mse_reg_cpi_g, val_mse_reg_cpi_g])
# Combine results into table
result_df = pd.DataFrame(results, columns=['Data Type', 'Model Type', 'Regularization', 'Train MSE', 'Validation MSE'])
result_df['Train RMSE'] = result_df['Train MSE'].apply(lambda x: x ** 0.5)
result_df['Validation RMSE'] = result_df['Validation MSE'].apply(lambda x: x ** 0.5)
result_df = result_df[['Data Type', 'Model Type', 'Regularization', 'Train RMSE', 'Validation RMSE']].sort_values(by=['Data Type', 'Validation RMSE'], ascending=True)
print(result_df)
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false
library(reticulate)
use_python("/Users/madhvimalhotra/myenv/bin/python", required = TRUE)
reticulate::repl_python()
