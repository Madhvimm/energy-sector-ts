---
title: "Univariate TS Models (ARIMA/SARIMA)"
output: distill::distill_article
---

```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Importing Libraries'
#| warning: false
#| output: false
library(ggplot2)
library(readr)
library(tidyverse)
library(lubridate)
library(forecast)
library(tseries)
library(plotly)
library(gridExtra)
library(zoo)
library(astsa)
library(ggplot2)
library(zoo)
library(plotly)

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Read data'
#| warning: false
#| output: false

composite_crude_oil_prices <- read.csv("data/composite_crude_oil_prices.csv")
citygate_gas_prices <- read.csv("data/citygate_gas_prices.csv")
total_electricity_prices <- read.csv("data/total_electricity_prices.csv")
gdp_data <- read.csv("data/gdp_data.csv")
cpi_data <- read.csv("data/cpi_data.csv")

```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'convert to ts'
#| warning: false
#| output: false


# For Crude Oil
composite_crude_oil_prices$Date <- as.Date(composite_crude_oil_prices$Date)
start_year <- as.numeric(format(min(composite_crude_oil_prices$Date), "%Y"))
start_month <- as.numeric(format(min(composite_crude_oil_prices$Date), "%m"))
composite_crude_oil_ts <- ts(composite_crude_oil_prices$Value, start=c(start_year, start_month), frequency=12)
# Log-transform Value
composite_crude_oil_prices$LOG_Value <- log(composite_crude_oil_prices$Value)
oil_log_ts <- ts(composite_crude_oil_prices$LOG_Value, start=c(start_year, start_month), frequency=12)

# For Natural Gas
citygate_gas_prices$Date <- as.Date(citygate_gas_prices$Date)
start_year_gas <- as.numeric(format(min(citygate_gas_prices$Date), "%Y"))
start_month_gas <- as.numeric(format(min(citygate_gas_prices$Date), "%m"))
citygate_gas_ts <- ts(citygate_gas_prices$Value, start=c(start_year_gas, start_month_gas), frequency=12)
# Log-transform Value
citygate_gas_prices$LOG_Value <- log(citygate_gas_prices$Value)
gas_log_ts <- ts(citygate_gas_prices$LOG_Value, start=c(start_year_gas, start_month_gas), frequency=12)

# For Electricity
total_electricity_prices$Date <- as.Date(total_electricity_prices$Date, format = "%Y-%m-%d")
start_year_elec <- as.numeric(format(min(total_electricity_prices$Date), "%Y"))
start_month_elec <- as.numeric(format(min(total_electricity_prices$Date), "%m"))
total_electricity_ts <- ts(total_electricity_prices$Value, start = c(start_year_elec, start_month_elec), frequency = 12)
# Log-transform Value
total_electricity_prices$LOG_Value <- log(total_electricity_prices$Value)
electricity_log_ts <- ts(total_electricity_prices$LOG_Value, start=c(start_year_elec, start_month_elec), frequency=12)


# For GDP (it's quarterly)
gdp_data$DATE <- as.Date(gdp_data$DATE, format = "%Y-%m-%d")
start_year_gdp <- as.numeric(format(min(gdp_data$DATE), "%Y"))
start_quarter_gdp <- quarter(min(gdp_data$DATE))
# Log-transform GDP
gdp_data$LOG_GDP <- log(gdp_data$GDP)
gdp_log_ts <- ts(gdp_data$LOG_GDP, start=c(start_year_gdp, start_quarter_gdp), frequency=4)


# For CPI (it's monthly)
cpi_data$DATE <- as.Date(cpi_data$DATE, format = "%Y-%m-%d")
start_year_cpi <- as.numeric(format(min(cpi_data$DATE), "%Y"))
start_month_cpi <- as.numeric(format(min(cpi_data$DATE), "%m"))
# Log-transform CPI
cpi_data$LOG_CPI <- log(cpi_data$CPIAUCSL)
cpi_log_ts <- ts(cpi_data$LOG_CPI, start = c(start_year_cpi, start_month_cpi), frequency = 12)

```

# ACF & PACF Plots

Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots are crucial tools in time series analysis, helping to identify the type of model that best describes a series. The ACF plot shows the correlation of the series with its own lags, providing insights into the overall correlation structure and potential seasonality. On the other hand, the PACF plot reveals the direct effect of past values on the current value, helping to pinpoint the order of autoregressive models.

By examining the ACF and PACF plots, we can discern patterns that suggest the presence of autoregressive (AR) or moving average (MA) components in our time series models. Significant spikes in the ACF plot indicate potential AR terms, while significant spikes in the PACF plot suggest MA terms. These plots also assist in determining the stationarity of the series, a crucial aspect in time series modeling, where non-stationary data often require differencing to achieve stationarity.

This section delves into the ACF and PACF plots for our datasets, examining their autocorrelation structures to get insights that will guide our model selection and inform our forecasting methodology.


::: panel-tabset



## Crude Oil


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ACF-PACF Plot'
#| warning: false
#| output: true

#ACF Plot for Crude Oil
crude_oil_acf <- ggAcf(oil_log_ts) + 
  ggtitle("ACF Plot for Crude Oil (Log)") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#PACF Plot for Crude Oil
crude_oil_pacf <- ggPacf(oil_log_ts) + 
  ggtitle("PACF Plot for Crude Oil (Log)") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange the plots
grid.arrange(crude_oil_acf, crude_oil_pacf, nrow = 2)



```

The **ACF plot** for crude oil prices demonstrates prolonged significant autocorrelation, suggesting a non-stationary series. The gradual decline in correlation as lags increase indicates a potential long-term dependency or trend in the data. 

The **PACF plot** shows significant spike at lag 1 and 2, followed by non-significant values

The ACF and PACF plots suggest considering an ARIMA model with 'p' to be 1 and 2. The slow decay in the ACF implies that differencing (d > 0) may be necessary to achieve stationarity.


## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ACF-PACF Plot'
#| warning: false
#| output: true

# ACF and PACF for Natural Gas
natural_gas_acf <- ggAcf(gas_log_ts) + 
  ggtitle("ACF Plot for Natural Gas Price (Log)") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

natural_gas_pacf <- ggPacf(gas_log_ts) + 
  ggtitle("PACF Plot for Natural Gas Price (Log)") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

grid.arrange(natural_gas_acf, natural_gas_pacf, nrow = 2)


```

The **ACF plot** for natural gas prices shows a very slow decay, suggesting non-stationarity and a need for differencing. 

The **PACF plot** shows a significant spike at lag 1, followed by a drop-off.

The ACF and PACF plots suggest considering an ARIMA model with 'p' to be 1, 2 and 3. The slow decay in the ACF implies that differencing (d > 0) may be necessary to achieve stationarity.




## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ACF-PACF Plot'
#| warning: false
#| output: true

# ACF and PACF for Electricity
electricity_acf <- ggAcf(electricity_log_ts) + 
  ggtitle("ACF Plot for Average Price of Electricity to Ultimate Customers (Log)") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

electricity_pacf <- ggPacf(electricity_log_ts) + 
  ggtitle("PACF Plot for Average Price of Electricity to Ultimate Customers (Log)") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

grid.arrange(electricity_acf, electricity_pacf, nrow = 2)


```

The **ACF plot** for electricity shows a strong positive autocorrelation across all the lags indicating a potential MA term and a need for differencing due to non-stationarity.

The **PACF plot** shows a significant correlations at lag 1. The choice of 'p' could be 1 or 2 based on the first significant spikes.

The consistent autocorrelation in the ACF plot suggests a potential need for a higher-order MA term or differencing, leading to an ARIMA(p,d,q) model consideration.



## GDP


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ACF-PACF Plot'
#| warning: false
#| output: true

# ACF and PACF for GDP (Log)
gdp_acf <- ggAcf(gdp_log_ts) + 
  ggtitle("ACF Plot for GDP") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

gdp_pacf <- ggPacf(gdp_log_ts) + 
  ggtitle("PACF Plot for GDP") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) 

grid.arrange(gdp_acf, gdp_pacf, nrow = 2)


```


The **ACF plot** for GDP shows a persistent strong autocorrelation across all lags indicates non-stationarity, suggesting that differencing may be necessary.

The **PACF plot** have a sharp cutoff after lag 1 indicating an AR(1) process, suggesting that previous values have a significant impact on current GDP.

Given the strong autocorrelation and the PACF cutoff, an ARIMA(1,1,0) model may be a good starting point for modeling GDP, but  differencing (d > 0) may be necessary.




## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ACF-PACF Plot'
#| warning: false
#| output: true

# ACF and PACF for CPI (Log)
cpi_acf <- ggAcf(cpi_log_ts) + 
  ggtitle("ACF Plot for CPI") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

cpi_pacf <- ggPacf(cpi_log_ts) + 
  ggtitle("PACF Plot for CPI") + 
  theme_bw() +
  geom_segment(lineend = "butt", color = "#99494d") +
  geom_hline(yintercept = 0, color = "#99494d")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) 

grid.arrange(cpi_acf, cpi_pacf, nrow = 2)



```



The **ACF plot** for CPI shows a sustained high autocorrelation across lags suggests a non-stationary time series, indicative of CPI's long memory.

The **PACF plot** have a sharp spike and a cutoff after lag 1 indicating an AR(1) process, suggesting that previous values have a significant impact on current GDP.

The sustained autocorrelation in the ACF plot implies that differencing might be needed. An initial ARIMA(1,1,0) model could be considered.



## 

:::


# Detrend VS First - Difference


Detrending and differencing are foundational techniques used to render time series data stationary. **Detrending** removes the underlying trend from the data, usually by subtracting an estimated trend component from the original series. **Differencing**, in contrast, focuses on the changes between consecutive observations by transforming the series into a sequence of differences. While detrending is aimed at addressing trends, differencing is capable of eliminating both trend and seasonal components, potentially achieving stationarity in the process.

**Applying Detrending and Differencing to Our Datasets:**

Now, let's apply these critical techniques of detrending and differencing to our datasets, observing how they enhance the stationarity of the data:

::: panel-tabset


## Crude Oil


```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: false

# Convert residuals to a ts object
fit_crude_oil_log <- lm(LOG_Value ~ Date, data = composite_crude_oil_prices, na.action = NULL)

resid_crude_oil_ts_log <- ts(resid(fit_crude_oil_log), start=c(start_year, start_month), frequency=12)

```

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot the residuals
plot1_crude_oil <- autoplot(resid_crude_oil_ts_log, series="Detrended Crude Oil", colour = "#499995") + 
  theme_bw() +
  xlab("Time") +
  ylab("Residuals") +
  ggtitle("Detrended Crude Oil (Log)") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Differencing plot
plot2_crude_oil <- autoplot(diff(oil_log_ts), series = "First Difference Crude Oil", colour = "#99494d") + 
  theme_bw() +
  xlab("Time") +
  ylab("Differences") +
  ggtitle("First Difference Crude Oil (Log)") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange plots
grid.arrange(plot1_crude_oil, plot2_crude_oil, nrow = 2)
```

**Detrended:**

Post-detrending residuals may exhibit patterns, suggesting that linear detrending doesn’t fully capture all dynamics within the series.

**First Difference:**

The series shows fluctuations that average out around zero, suggesting enhanced stationarity. Still, a deeper autocorrelation analysis is necessary to confirm full stationarity.



## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: false

# Detrending using the original data frame
fit_natural_gas <- lm(LOG_Value ~ Date, data = citygate_gas_prices, na.action = NULL)

# Convert residuals to a ts object for plotting
resid_natural_gas_ts_log <- ts(resid(fit_natural_gas), start=c(start_year_gas, start_month_gas), frequency=12)
```


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot the residuals
plot1_natural_gas <- autoplot(resid_natural_gas_ts_log, series="Detrended Natural Gas (Log)", colour = "#499995") + 
  theme_bw() +
  xlab("Year") +
  ylab("Residuals") +
  ggtitle("Detrended Natural Gas (Log)") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Differencing plot
plot2_natural_gas <- autoplot(diff(gas_log_ts), series = "First Difference Natural Gas (Log)", colour = "#99494d") + 
  theme_bw() +
  xlab("Year") +
  ylab("Differences") +
  ggtitle("First Difference Natural Gas (Log)") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange plots
grid.arrange(plot1_natural_gas, plot2_natural_gas, nrow = 2)
```


**Detrended:**

Even after detrending, the series displays volatility, implying that mere removal of a linear trend doesn't encapsulate all the complexities in the data.

**First Difference:**

The series maintains a consistent mean, though volatility persists, with noticeable spikes potentially attributable to external shocks like the 2020 pandemic.




## Electricity


```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: false

# Detrending using the original data frame
fit_electricity <- lm(LOG_Value ~ Date, data = total_electricity_prices, na.action = NULL)

# Convert residuals to a ts object for plotting
resid_electricity_ts_log <- ts(resid(fit_electricity), start=c(start_year_elec, start_month_elec), frequency=12)
```


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot the residuals
plot1_electricity <- autoplot(resid_electricity_ts_log, series="Detrended Electricity Log", colour = "#499995") + 
  theme_bw() +
  xlab("Year") +
  ylab("Residuals") +
  ggtitle("Detrended Electricity (Log)") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Differencing plot
plot2_electricity <- autoplot(diff(electricity_log_ts), series = "First Difference Electricity Log", colour = "#99494d") + 
  theme_bw() +
  xlab("Year") +
  ylab("Differences") +
  ggtitle("First Difference Electricity (Log)") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange plots
grid.arrange(plot1_electricity, plot2_electricity, nrow = 2)


```


**Detrended:**

Periodic fluctuations in the detrended plot hint at seasonality within the electricity price data.

**First Difference:**

The first differenced series oscillates around a central mean, which is indicative of stationarity in the mean of the series. However, the consistent pattern of spikes indicates a strong seasonal component.




## GDP


```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: false



# Detrending using the original data frame
fit_log_gdp <- lm(LOG_GDP ~ DATE, data = gdp_data, na.action = NULL)

# Convert residuals to a ts object for plotting
resid_log_gdp_ts <- ts(resid(fit_log_gdp), start=c(start_year_gdp, start_quarter_gdp), frequency=4)

```


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot the residuals
plot1_log_gdp <- autoplot(resid_log_gdp_ts, series="Detrended Log GDP", colour = "#499995") + 
  theme_bw() +
  xlab("Year") +
  ylab("Residuals") +
  ggtitle("Detrended Log GDP") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Differencing plot
plot2_log_gdp <- autoplot(diff(gdp_log_ts), series = "First Difference Log GDP", colour = "#99494d") + 
  theme_bw() +
  xlab("Year") +
  ylab("Differences") +
  ggtitle("First Difference Log GDP") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


#Arrange plots
grid.arrange(plot1_log_gdp, plot2_log_gdp, nrow = 2)


```


**Detrended:**

The detrended GDP plot showcases that the residuals seem to have a non-linear component, as evidenced by the gradual decline and subsequent increase over time. The residuals decrease and then slowly begin to rise after the 1980s, accelerating significantly in recent years. GDP growth rate is not constant and a simple linear model may not be sufficient to capture the complexities.


**First Difference:**

The differenced series predominantly hovers around the zero line, which indicates that this transformation effectively removes the trend from the data, leading to a stationary series in terms of the mean. The substantial spike observed towards the end is likely due to the recent economic downturn due to the COVID-19 pandemic. 



## CPI


```{r, warning=FALSE, message=FALSE, echo=FALSE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: false


# Detrending using the original data frame
fit_log_cpi <- lm(LOG_CPI ~ DATE, data = cpi_data, na.action = NULL)


# Convert residuals to a ts object for plotting
resid_log_cpi_ts <- ts(resid(fit_log_cpi), start=c(start_year_cpi, start_month_cpi), frequency=12)

```



```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot the residuals
plot1_log_cpi <- autoplot(resid_log_cpi_ts, series="Detrended Log CPI", colour = "#499995") + 
  theme_bw() +
  xlab("") +
  ylab("Residuals") +
  ggtitle("Detrended Log CPI") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


#Differencing plot
plot2_diff_log_cpi <- autoplot(diff(cpi_log_ts), series = "First Difference Log CPI", colour = "#99494d") + 
  theme_bw() +
  xlab("Year") +
  ylab("Differences") +
  ggtitle("First Difference Log CPI") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


#Arrange plots
grid.arrange(plot1_log_cpi, plot2_diff_log_cpi, nrow = 2)

```




**Detrended:**

A decline followed by stabilization and increase in the residuals indicates that inflationary trends are not linear over Year.


**First Difference:**

The first difference plot for CPI demonstrates a series that fluctuates around a central mean value.




##


:::


# Original Vs First Difference


This section focuses on a direct comparison between our original time series data and their first differenced forms. By examining the differences, we can highlight the effectiveness of the differencing technique in achieving stationarity—a key prerequisite for many time series modeling approaches.



::: panel-tabset



## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot for the original
plot1 <- ggplot(composite_crude_oil_prices, aes(x = Date, y = LOG_Value)) + 
  geom_line(colour = "#207068") + 
  theme_bw() +
  xlab("Year") + 
  ylab("Crude Oil Prices (USD)") +
  ggtitle("Original Crude Oil Prices (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Calculate the first differences
composite_crude_oil_prices$Diff_Value <- c(NA, diff(composite_crude_oil_prices$LOG_Value))
composite_crude_oil_prices_filtered <- composite_crude_oil_prices %>% 
  filter(!is.na(Diff_Value))


#Plot for the first-differenced
plot2 <- ggplot(composite_crude_oil_prices_filtered, aes(x = Date, y = Diff_Value)) + 
  geom_line(colour = "#702028") + 
  theme_bw() +
  xlab("Year") + 
  ylab("First Differences") +
  ggtitle("First Differenced Crude Oil Data (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange
grid.arrange(plot1, plot2, nrow = 2)


```

## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot for the original
plot1_ng <- ggplot(citygate_gas_prices, aes(x = Date, y = LOG_Value)) + 
  geom_line(colour = "#207068") + 
  theme_bw() +
  xlab("Year") + 
  ylab("Natural Gas Prices") +
  ggtitle("Original Natural Gas (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Calculate the first differences
citygate_gas_prices$Diff_Value <- c(NA, diff(citygate_gas_prices$LOG_Value))
citygate_gas_prices_filtered <- citygate_gas_prices %>% 
  filter(!is.na(Diff_Value))

#Plot for the first-differenced
plot2_ng <- ggplot(citygate_gas_prices_filtered, aes(x = Date, y = Diff_Value)) + 
  geom_line(colour = "#702028") + 
  theme_bw() +
  xlab("Year") + 
  ylab("First Differences") +
  ggtitle("First Differenced Natural Gas (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

# Arrange
grid.arrange(plot1_ng, plot2_ng, nrow = 2)
```



## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot for the original
plot1_elec <- ggplot(total_electricity_prices, aes(x = Date, y = LOG_Value)) + 
  geom_line(colour = "#207068") + 
  theme_bw() +
  xlab("Year") + 
  ylab("Electricity Prices (Log)") +
  ggtitle("Original Electricity Data (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Calculate the first differences
total_electricity_prices$Diff_Value <- c(NA, diff(total_electricity_prices$LOG_Value))
total_electricity_prices_filtered <- total_electricity_prices %>% 
  filter(!is.na(Diff_Value))


#Plot for the first-differenced
plot2_elec <- ggplot(total_electricity_prices_filtered, aes(x = Date, y = Diff_Value)) + 
  geom_line(colour = "#702028") + 
  theme_bw() +
  xlab("Year") + 
  ylab("First Differences") +
  ggtitle("First Differenced Electricity Data (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange
grid.arrange(plot1_elec, plot2_elec, nrow = 2)


```


## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot for the original
plot1_gdp <- ggplot(gdp_data, aes(x = DATE, y = LOG_GDP)) + 
  geom_line(colour = "#207068") + 
  theme_bw() +
  xlab("Year") + 
  ylab("Log of GDP") +
  ggtitle("Original GDP Data (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Calculate the first differences
gdp_data$Diff_GDP <- c(NA, diff(gdp_data$LOG_GDP))
gdp_data_filtered <- gdp_data %>% 
  filter(!is.na(Diff_GDP))

#Plot for the first-differenced
plot2_gdp <- ggplot(gdp_data_filtered, aes(x = DATE, y = Diff_GDP)) + 
  geom_line(colour = "#702028") + 
  theme_bw() +
  xlab("Year") + 
  ylab("First Differences") +
  ggtitle("First Differenced GDP (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange
grid.arrange(plot1_gdp, plot2_gdp, nrow = 2)


```


## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

#Plot for the original
plot1_cpi <- ggplot(cpi_data, aes(x = DATE, y = LOG_CPI)) + 
  geom_line(colour = "#207068") + 
  theme_bw() +
  xlab("Year") + 
  ylab("CPI") +
  ggtitle("Original CPI Data (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Calculate the first differences
cpi_data$Diff_CPI <- c(NA, diff(cpi_data$LOG_CPI))
cpi_data_filtered <- cpi_data %>% 
  filter(!is.na(Diff_CPI))

#Plot for the first-differenced
plot2_cpi <- ggplot(cpi_data_filtered, aes(x = DATE, y = Diff_CPI)) + 
  geom_line(colour = "#702028") + 
  theme_bw() +
  xlab("Year") + 
  ylab("First Differences") +
  ggtitle("First Differenced CPI Data (Log)")+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

#Arrange
grid.arrange(plot1_cpi, plot2_cpi, nrow = 2)

```


## 

:::

# Adjusted Dickey-Fuller Test

After applying various transformations to address the non-stationarity in our time series data, our next step is to conduct the Adjusted Dickey-Fuller Test on the differenced data. This test is crucial for confirming that the adjustments we've made have effectively rendered the series stationary. Stationarity is a key assumption for many time series forecasting methods, as it implies that the statistical properties of the series are consistent over time. By applying the Adjusted Dickey-Fuller Test to our transformed data, we aim to validate that the mean, variance, and autocorrelation structure of the series do not change over time.

::: panel-tabset

## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ADF Test'
#| warning: false
#| output: true

#First difference
diff_crude_oil_ts <- diff(oil_log_ts, differences = 1)

#Applying ADF test to the differenced time series
adf_test_diff_crude_oil <- suppressWarnings(
  adf.test(diff_crude_oil_ts, alternative = "stationary"))

#Display the test results
print(adf_test_diff_crude_oil)

```

In the adjusted test we can clearly see the p-value is significantly less than 0.05, we reject the null hypothesis and conclude that the differenced series is stationary. The fact that the differenced series is stationary (but the original was not) suggests that the crude oil prices exhibit a trend or a form of non-stationarity that can be removed by differencing. 



## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ADF Test'
#| warning: false
#| output: true

#First difference
diff_citygate_gas_ts <- diff(gas_log_ts, differences = 1)
diff_citygate_gas_ts <- na.omit(diff_citygate_gas_ts)

#Applying ADF test to the differenced time series
adf_test_diff_natural_gas <- suppressWarnings(
  adf.test(diff_citygate_gas_ts, alternative = "stationary"))

#Display the test results
adf_test_diff_natural_gas

```

In the adjusted test we can clearly see the p-value is significantly less than 0.05, leading us to reject the null hypothesis, concluding that the differenced natural gas price series is stationary. This indicates that the series, once differenced, does not have a unit root, and its mean and variance are constant over time.



## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ADF Test'
#| warning: false
#| output: true

#First difference
diff_total_electricity_ts <- diff(electricity_log_ts, differences = 1)
diff_total_electricity_ts <- na.omit(diff_total_electricity_ts)

#Applying ADF test to the differenced time series
adf_test_diff_electricity <- suppressWarnings(
  adf.test(diff_total_electricity_ts, alternative = "stationary"))

#Display the test results
print(adf_test_diff_electricity)


```

With the p-value now below 0.05, we can reject the null hypothesis, concluding that the differenced electricity price series is stationary. This means that after differencing, the series doesn't exhibit a unit root, and its properties like mean and variance are consistent over time.



## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ADF Test'
#| warning: false
#| output: true

#First difference
diff_gdp_ts <- diff(gdp_log_ts, differences = 1)
diff_gdp_ts <- na.omit(diff_gdp_ts)

#Applying ADF test to the differenced time series
adf_test_diff_gdp <- suppressWarnings(
  adf.test(diff_gdp_ts, alternative = "stationary"))

# Display the test results
print(adf_test_diff_gdp)

```

The p-value below 0.05 allows us to reject the null hypothesis, concluding that the differenced GDP series is stationary. This suggests that the original GDP series had a trend or other non-stationary components that were effectively removed by differencing.


## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'ADF Test'
#| warning: false
#| output: true

#First difference
diff_cpi_ts <- diff(cpi_log_ts, differences = 1)
diff_cpi_ts <- na.omit(diff_cpi_ts)

#Applying ADF test to the differenced time series
adf_test_diff_cpi <- suppressWarnings(
  adf.test(diff_cpi_ts, alternative = "stationary"))

# Display the test results
print(adf_test_diff_cpi)

```

Since the p-value is now below 0.05, we can reject the null hypothesis, concluding that the differenced CPI series is stationary. 


## 

:::

After implementing the Adjusted Dickey-Fuller Test on our differenced data sets, we observed a significant reduction in the p-values for all the data sets. This result strongly suggests that the differencing process has effectively induced stationarity in these series, as indicated by the absence of a unit root. However, to ensure the robustness of our findings and to precisely model and forecast these series, a subsequent examination of the ACF and PACF plots for the differenced data is imperative. These plots will provide further insights into the autocorrelation structure of the data, guiding us in the selection of appropriate ARIMA model parameters.




# First vs Second Differencing

In time series analysis, differencing is a technique used to stabilize the mean of a series and make it stationary. When trends and seasonality are present in a time series, they can affect the predictive models. Differencing helps to mitigate these influences by focusing on the changes in the data rather than the actual values.

Differencing operates under the principle of transformation. It is designed to remove specific types of patterns:

-   First Differencing: This method subtracts the current observation from the previous one. It is a powerful tool to eliminate trends and some types of seasonality in the data, providing a clearer view of the underlying cyclical components and irregularities.

-   Second Differencing: When first differencing is not enough to achieve stationarity, or when the time series exhibits a more complex pattern such as a trend within a trend, second differencing can be employed. This involves applying the differencing operation twice, which can further simplify the predictive structure by reducing more complex serial correlations.

::: panel-tabset

## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

# First Differencing
first_diff_crude_oil <- diff(oil_log_ts)
ggtsdisplay(first_diff_crude_oil, main = "First Differencing of Crude Oil Prices (Log)")

# Second Differencing
second_diff_crude_oil <- diff(first_diff_crude_oil)
ggtsdisplay(second_diff_crude_oil, main = "Second Differencing of Crude Oil Prices (Log)")
```

Based on these graphs, second differencing does not seem necessary. The first differencing appears to have sufficiently stabilized the mean of the time series, as there is no visible trend or seasonality, and the autocorrelation in the data seems to be addressed.


Examining the ACF and PACF plots for the first differencing of crude oil prices, we can suggest a range of values for p and q:

p =  1, 2, (to account for the spike at lag 1 and 2)

q = 1, 2, 3, 4 (as the first three lags are above the significance level)




## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

# First Differencing
first_diff_natural_gas <- diff(gas_log_ts)
ggtsdisplay(first_diff_natural_gas, main = "First Differencing of Natural Gas Prices (Log)")

# Second Differencing
second_diff_natural_gas <- diff(first_diff_natural_gas)
ggtsdisplay(second_diff_natural_gas, main = "Second Differencing of Natural Gas Prices (Log)")

```

We can clearly observe second differencing doesn’t appear to provide additional benefits. In fact, doing so may result in a loss of information and potentially over-differencing the data. The first difference might be sufficient for the natural gas prices time series, as the plots don't indicate non-stationarity. 


Examining the ACF and PACF plots for the first differencing of natural gas prices, we can suggest a range of values for p and q:

p = 1,2
q = 1,2 (considering the first significant lag)


## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

# First Differencing
first_diff_electricity <- diff(electricity_log_ts)
ggtsdisplay(first_diff_electricity, main = "First Differencing of Electricity Prices (Log)")

# Second Differencing
second_diff_electricity <- diff(first_diff_electricity)
ggtsdisplay(second_diff_electricity, main = "Second Differencing of Electricity Prices (Log)")

```


It seems that first differencing might have been adequate to stabilize the mean of the series and to reduce the autocorrelation, the second differencing does not appear to provide additional necessary correction, as evidenced by the ACF and PACF plots which do not show significant autocorrelation after the first differencing.


Examining the ACF and PACF plots for the first differencing of electricity prices, we can suggest a range of values for p and q:

p = 1, 2, 3 (considering the significant spike at the first lag)
q = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10


## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true

# First Differencing
first_diff_gdp <- diff(gdp_log_ts)
ggtsdisplay(first_diff_gdp, main = "First Differencing of GDP")

# Second Differencing
second_diff_gdp <- diff(first_diff_gdp)
ggtsdisplay(second_diff_gdp, main = "Second Differencing of GDP")

```

The first differencing of GDP may be sufficient for stationarity, as the time series graph shows no trend and the ACF plot exhibits no significant autocorrelations.  The second differencing results indicates over-differencing, as suggested by the significant negative correlations in the ACF and PACF plots at the initial lags.

Examining the ACF and PACF plots for the first differencing of GDP, we can suggest a range of values for p and q:

p = 1, 2, 3 
q = 1, 2, 3 (considering the first few significant lags)


## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Plot'
#| warning: false
#| output: true


# First Differencing
first_diff_cpi <- diff(cpi_log_ts)
ggtsdisplay(first_diff_cpi, main = "First Differencing of CPI")

# Second Differencing
second_diff_cpi <- diff(first_diff_cpi)
ggtsdisplay(second_diff_cpi, main = "Second Differencing of CPI")

```

The second differencing of CPI doesn't show a noticeable trend, the first differencing might be sufficient. The second differencing does not seem to add further information; it rather makes the ACF and PACF plots more of a white noise process, which could suggest over-differencing.

Examining the ACF and PACF plots for the first differencing of CPI, we can suggest a range of values for p and q:

p = 1 to 12 (significant spike at lag 1)
q = 1, 2, 3, 4, 5, 6, 7, 8, 9 (to capture the initial significant lags)



## 


In our analysis, we applied first and second differencing techniques to examine their effects on stationarity. First differencing is often sufficient to stabilize the mean of a series and make it stationary. Second differencing, although sometimes necessary for complex trends, wasn't found to provide substantial benefits in our datasets. Instead, it seems to indicate a potential over-differencing, which can obscure meaningful insights from the data.

For our datasets, first differencing effectively addressed visible trends and autocorrelations, the second differencing didn't reveal additional insights and, in some cases, suggested that the data might be overly differenced, indicated by increased noise in the ACF and PACF plots. Therefore, in our case, sticking with first differencing seems to be the prudent approach to preparing our time series data for further analysis and modeling.





:::


# AIC & BIC

In time series analysis, choosing the right model is paramount for accurate forecasting. Two of the most critical metrics for model selection are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Both criteria are grounded in information theory and provide a means to balance model fit with model complexity.

-   AIC is a tool for model selection that quantifies the trade-offs between model complexity (the number of parameters in the model) and the goodness of fit. AIC rewards models that achieve a high goodness of fit but penalizes those that become overly complex. A lower AIC value often indicates a preferable model.

-   BIC extends the logic of AIC by incorporating sample size into the penalty for complexity. This adjustment makes BIC more stringent with complex models when dealing with larger datasets. As with AIC, a lower BIC suggests a better model.



::: panel-tabset

## Crude Oil

p =  1, 2 (to account for the spike at lag 1 and 2)

q = 1, 2, 3, 4 

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'AIC-BIC'
#| warning: false
#| output: true


d <- 1 # Degree of differencing
temp <- data.frame()
ls <- matrix(rep(NA, 6 * 8), nrow = 8)  # 8 combinations

i <- 1
for (p in 1:2) {
  for (q in 1:4) {
    if(p-1+d+q-1<=8) {
      model <- Arima(oil_log_ts, order = c(p, d, q), include.drift = TRUE)
      ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
      i <- i + 1
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

knitr::kable(temp)


#Displaying the models with min values
temp[which.min(temp$AIC),]
temp[which.min(temp$BIC),]
temp[which.min(temp$AICc),]

```

When selecting the best ARIMA model for forecasting crude oil prices, it's common for the AIC and BIC to suggest different models. In such cases, we often prefer the model with the lower AIC. Based on the results, we've can select two models for further consideration:

- Best Model by AIC (and AICc) and BIC is : ARIMA(2,1,1)
- Second Best would be (2,1,2)



## Natural Gas

p = 1,2,3
q = 1,2 (considering the first significant lag)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'AIC-BIC'
#| warning: false
#| output: true

d <- 1 # Degree of differencing
temp <- data.frame()
ls <- matrix(rep(NA, 6 * 9), nrow = 9)  #  9 combinations: p (3 options) * q (3 options)

i <- 1
for (p in 1:3) {
  for (q in 1:3) {
    if(p-1+d+q-1<=8) {
      model <- Arima(gas_log_ts, order = c(p, d, q), include.drift = TRUE)
      ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
      i <- i + 1
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#Displaying the table
knitr::kable(temp)


#Displaying the models with min values
temp[which.min(temp$AIC),]
temp[which.min(temp$BIC),]
temp[which.min(temp$AICc),]
```

When selecting the best ARIMA model for forecasting natural gas prices, it's common for the AIC and BIC to suggest different models. In such cases, we often prefer the model with the lower AIC. Based on the results, we've can select two models for further consideration:

- Best Model by AIC (and AICc): ARIMA(3,1,1)
- Second Best Model by BIC: ARIMA(1,1,1)




## Electricity

p = 1, 2, 3 (considering the significant spike at the first lag)


q = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'AIC-BIC'
#| warning: false
#| output: true

d <- 1 # Degree of differencing
temp <- data.frame()
ls <- matrix(rep(NA, 6 * 21), nrow = 21)  # p (3 options) * q (10 options) = 30 combinations - threshold

i <- 1
for (p in 1:3) {
  for (q in 1:10) {
    if(p-1+d+q-1<=8) {
      model <- Arima(electricity_log_ts, order = c(p, d, q), include.drift = TRUE)
      ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
      i <- i + 1
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#Displaying the table
knitr::kable(temp)


#Displaying the models with min values
temp[which.min(temp$AIC),]
temp[which.min(temp$BIC),]
temp[which.min(temp$AICc),]
```



When selecting the best ARIMA model for forecasting electricity prices, it's common for the AIC and BIC to suggest different models. In such cases, we often prefer the model with the lower AIC. Based on the results, we've can select two models for further consideration:

- Best Model by AIC, BIC, and AICc (ARIMA(2,1,3))
- Second best model: ARIMA(3,1,6)




## GDP

p = 1, 2, 3 
q = 1, 2, 3 (considering the first few significant lags)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'AIC-BIC'
#| warning: false
#| output: true

d <- 1 # Degree of differencing
temp <- data.frame()
ls <- matrix(rep(NA, 6 * 9), nrow = 9)  # p (3 options) * q (3 options) = 9 combinations

i <- 1
for (p in 1:3) {
  for (q in 1:3) {
    if(p-1+d+q-1<=8) {
      model <- Arima(gdp_log_ts, order = c(p, d, q), include.drift = TRUE)
      ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
      i <- i + 1
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#Displaying the table
knitr::kable(temp)

#Displaying the models with min values
temp[which.min(temp$AIC),]
temp[which.min(temp$BIC),]
temp[which.min(temp$AICc),]

```


When selecting the best ARIMA model for forecasting GDP, it's common for the AIC and BIC to suggest different models. In such cases, we often prefer the model with the lower AIC. Based on the results, we've can select two models for further consideration:

- Best Model by AIC, BIC and AICc: ARIMA(1,1,1)
- Second Best Model: ARIMA(3,1,2)



## CPI


p = 1 to 12 (significant spike at lag 1)
q = 1, 2, 3, 4, 5, 6, 7, 8, 9 (to capture the initial significant lags)


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'AIC-BIC'
#| warning: false
#| output: true


d <- 1 # Degree of differencing
temp <- data.frame()
ls <- matrix(rep(NA, 6 * 36), nrow = 36)  

i <- 1
for (p in 1:12) {
  for (q in 1:9) {
    if (p-1+d+q-1<=8) {
      model <- Arima(cpi_log_ts, order = c(p, d, q), include.drift = TRUE)
      ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
      i <- i + 1
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

#Displaying the table
knitr::kable(temp)

#Displaying the models with min values
temp[which.min(temp$AIC),]
temp[which.min(temp$BIC),]
temp[which.min(temp$AICc),]

```


When selecting the best ARIMA model for forecasting CPI, it's common for the AIC and BIC to suggest different models. In such cases, we often prefer the model with the lower AIC. Based on the results, we've can select two models for further consideration:

- Best Model by AIC and AICc: ARIMA(3,1,2)
- Second Best Model: ARIMA(3,1,3)


## 

In our analysis, we used AIC and BIC to pick the best models for forecasting our data. Even though AIC and BIC sometimes suggest different models, we often chose the one with the lower AIC because it usually gives us a good balance and does a good job predicting future trends. This careful selection helps ensure our forecasts are as accurate as possible, providing valuable insights for each dataset.


:::

# Fitting ARIMA

After identifying the best models for our time series data using criteria like AIC and BIC, our next step is to fit these models to the data. Fitting the best ARIMA model allows us to refine our forecasts by honing in on the underlying patterns. While our primary focus is on the top-performing model, we also consider the second-best model. This approach gives us a fallback and an opportunity to compare, ensuring that our conclusions are robust and our forecasts are as reliable as possible.

For a general ARIMA(p, d, q) model, the equation can be represented as:

$$
x_t = \sum_{i=1}^{p} \phi_i x_{t-i} + \sum_{j=1}^{q} \theta_j w_{t-j} + w_t
$$

Here, 

- \( x_t \) represents the time series after differencing, 

- The coefficients for the autoregressive (AR) part are denoted as phi_i, where 'phi' represents the coefficients and 'i' is an index that ranges from 1 to p


- The coefficients for the moving average (MA) part are denoted as theta_j, where 'theta' represents the coefficients and 'j' is an index that ranges from 1 to q


- \( w_t \) is the error term at time \( t \).




## Fitting Best Model


::: panel-tabset

### Crude Oil

- Best Model by AIC (and AICc) and BIC is : ARIMA(2,1,1)
- Second Best would be (2,1,2)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_crudeoil <- Arima(diff(oil_log_ts), order=c(2, 1, 1),include.drift = TRUE) 
summary(fit_crudeoil)
```

**Coefficients:** The model includes two autoregressive terms (ar1, ar2) and one moving average terms, with all coefficients having significant values (based on their small standard errors (s.e.))
**Drift:** The drift coefficient is effectively zero, suggesting no consistent linear trend in the differenced series.


For the Best Model (ARIMA(2,1,1)):
Given the coefficients:

- AR1: 0.5693 
- AR2: -0.2302
- MA1: -1


The equation for the ARIMA(2,1,1) model, ignoring the drift since it's 0, would be:

$$
y_t = 0.5693y_{t-1} - 0.2302y_{t-2} + w_t - 1.0w_{t-1}
$$


### Natural Gas


- Best Model by AIC (and AICc): ARIMA(3,1,1)
- Second Best Model by BIC: ARIMA(1,1,1)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_NaturalGas <- Arima(diff(gas_log_ts), order=c(3, 1, 1),include.drift = TRUE) 
summary(fit_NaturalGas)

```


**Coefficients:** The model includes three autoregressive terms (ar1, ar2, ar3) and one moving average terms (ma1), with all coefficients having significant values.
**Drift:** The drift coefficient is effectively zero, suggesting no consistent linear trend in the differenced series.


For the Best Model (ARIMA(3,1,1)):

Given the coefficients:

- AR1: -0.2356
- AR2: -0.1575
- AR3: -0.0109
- MA1: -1.0000

The equation for the ARIMA(3,1,1) model, ignoring the drift since it's 0, would be:

$$
y_t = -0.2356y_{t-1} - 0.1575y_{t-2} - 0.0109y_{t-3} + w_t - 1.0w_{t-1}
$$




### Electricity

- Best Model by AIC, BIC, and AICc (ARIMA(2,1,3))
- Second best model: ARIMA(3,1,6)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_electricity <- Arima(diff(electricity_log_ts), order=c(2, 1, 3),include.drift = TRUE) 
summary(fit_electricity)

```

**Coefficients:** The model includes four autoregressive terms (ar1, ar2) and three moving average terms with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(2,1,3)):

Given the coefficients:

- AR1: 0.9728
- AR2: -0.9217
- MA1: -1.6812
- MA2: 1.6054
- MA3: -0.9242

The equation for the ARIMA(2,1,3) model:

$$
\begin{aligned}
y_t = &\ 0.9728 y_{t-1} - 0.9217 y_{t-2} \\
      &\ + w_t - 1.6812 w_{t-1} + 1.6054 w_{t-2} - 0.9242 w_{t-3}
\end{aligned}
$$



### GDP

- Best Model by AIC, BIC and AICc: ARIMA(1,1,1)
- Second Best Model: ARIMA(3,1,2)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_gdp_a <- Arima(diff(gdp_log_ts), order=c(1, 1, 1),include.drift = TRUE) 
summary(fit_gdp_a)

```

**Coefficients:** The model includes one autoregressive terms (ar1) and one moving average terms (ma1), with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(1,1,1)):

Given the coefficients:

- AR1: -0.0656
- MA1: -1.000


The equation for the ARIMA(1,1,1) model:

$$
y_t = -0.0656y_{t-1} + w_t - 1.00w_{t-1} 
$$



### CPI

- Best Model by AIC and AICc: ARIMA(3,1,2)
- Second Best Model: ARIMA(3,1,3)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_cpi_a <- Arima(diff(cpi_log_ts), order=c(3, 1, 2),include.drift = TRUE) 
summary(fit_cpi_a)

```

**Coefficients:** The model includes three autoregressive terms (ar1, ar2, ar3) and two moving average terms (ma1, ma2), with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(3,1,2)):

Given the coefficients:

- AR1: 0.8004 
- AR2: -0.3574
- AR3: 0.1405
- MA1: -1.2473
- MA2: 0.2672

The equation for the ARIMA(3,1,2) model:

$$
y_t = 0.8004y_{t-1} - 0.3574y_{t-2} + 0.1405y_{t-3} + w_t - 1.2473w_{t-1} + 0.2672w_{t-2}
$$



:::


## Fitting Second Best Model


::: panel-tabset

### Crude Oil

- Best Model by AIC (and AICc) and BIC is : ARIMA(2,1,1)
- Second Best would be (2,1,2)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_crudeoil2 <- Arima(diff(oil_log_ts), order=c(2, 1, 2),include.drift = TRUE) 
summary(fit_crudeoil2)
```


**Coefficients:** This model is simpler, with two AR and two MA parameters
**Drift:** Similar to the best model, the drift is zero.


For the Best Model (ARIMA(2,1,2)):
Given the coefficients:

- AR1: 0.9519
- AR2: -0.4028
- MA1: -1.4107 
- MA2: 0.4107 

The equation for the ARIMA(2,1,2) model, ignoring the drift since it's 0, would be:

$$
y_t = 0.9519y_{t-1} - 0.4028y_{t-2} + w_t - 1.4107w_{t-1} + 0.4107w_{t-2}
$$



#### Better Model:

- ARIMA(2,1,2) has a lower AIC (-907.19) compared to ARIMA(2,1,1) (-906.41)

- ARIMA(2,1,2) also shows a lower AICc (-906.98) than ARIMA(2,1,1) (-906.26)

- ARIMA(2,1,1) actually has a lower BIC compared to ARIMA(2,1,2)

- ARIMA(2,1,2) exhibits a higher log likelihood (459.6) compared to ARIMA(2,1,1) (458.21), indicating a slightly better fit

- ARIMA(2,1,2), both RMSE and MAE are slighlty lower than for ARIMA(2,1,1), indicating better predictive performance

- ARIMA(2,1,2) model includes an additional moving average term which seems to capture more complexity in the data dynamics


Given these considerations, the **ARIMA(2,1,2)** model is better for the series

### Natural Gas

- Best Model by AIC (and AICc): ARIMA(3,1,1)
- Second Best Model by BIC: ARIMA(1,1,1)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_NaturalGas2 <- Arima(diff(gas_log_ts), order=c(1, 1, 1),include.drift = TRUE) 
summary(fit_NaturalGas2)

```

**Coefficients:** The model includes one autoregressive terms (ar1) and one moving average terms (ma1), with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(1,1,1)):

Given the coefficients:

- AR1: -0.2018
- MA1: -1.0


The equation for the ARIMA(1,1,1) model:

$$
x_t = - 0.2018x_{t-1} + w_t - 1.0w_{t-1}
$$

#### Better Model:


- ARIMA(3,1,1) has a lower AIC (-563.17) compared to ARIMA(1,1,1) (-557.33)

- ARIMA(3,1,1) also shows a lower AICc (-562.96) than ARIMA(1,1,1) (-557.23)

- ARIMA(1,1,1) actually has a lower BIC compared to ARIMA(3,1,1)

- ARIMA(3,1,1) exhibits a higher log likelihood (287.59) compared to ARIMA(1,1,1) (282.67), indicating a slightly better fit

- For ARIMA(3,1,1), both RMSE and MAE are slightly lower than for ARIMA(1,1,1), indicating better predictive performance

- ARIMA(3,1,1) model includes additional autoregressive terms which seem to capture more complexity in the data dynamics


Given these considerations, the **ARIMA(3,1,1)** model is deemed better for the series.




### Electricity

- Best Model by AIC, BIC, and AICc (ARIMA(2,1,3))
- Second best model: ARIMA(3,1,6)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_electricity2 <- Arima(diff(electricity_log_ts), order=c(3, 1, 6),include.drift = TRUE) 
summary(fit_electricity2)

```

**Coefficients:** The model includes three autoregressive terms and six moving average terms (ma1, ma2..., ma6), with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(3,1,6)):

Given the coefficients:

- AR1: 0.0090
- AR2: 0.3478
- AR3: -0.5904
- MA1: -0.5779
- MA2: -0.7449
- MA3: 0.5961
- MA4: -0.6126
- MA5: -0.0975
- MA6: 0.4660

The equation for the ARIMA(3,1,6) model:

$$
\begin{aligned}
x_t = &\ 0.0090 x_{t-1} + 0.3478 x_{t-2} - 0.5904 x_{t-3} + w_t \\
      &\ - 0.5779 w_{t-1} - 0.7449 w_{t-2} + 0.5961 w_{t-3} \\
      &\ - 0.6126 w_{t-4} - 0.0975 w_{t-5} + 0.4660 w_{t-6}
\end{aligned}
$$

#### Better Model:

- ARIMA(3,1,6) has a lower AIC (-2062.81) compared to ARIMA(2,1,3) (-2052.82 )

- ARIMA(3,1,6) also shows a lower AICc (-2062.14) than ARIMA(2,1,3) (-2052.54)

- ARIMA(2,1,3) has a lower BIC (-2024.79) compared to ARIMA(3,1,6) (-2018.77)

- ARIMA(3,1,6) exhibits a higher log likelihood (1042.41) compared to ARIMA(2,1,3) (1033.41), indicating a slightly better fit

- For ARIMA(3,1,6), both RMSE and MAE are slightly lower than for ARIMA(2,1,3), indicating better predictive performance

- ARIMA(3,1,6) includes additional autoregressive terms which seem to capture more complexity in the data dynamics


Considering everything, even though the ARIMA(3,1,6) model seems to have a slight edge as shown by its lower AIC and slightly more accurate predictions, I would opt for **ARIMA(2,1,3)** model. ARIMA(2,1,3) model is less complex, which can make it more reliable when we use it to predict future prices.


### GDP

- Best Model by AIC, BIC and AICc: ARIMA(1,1,1)
- Second Best Model: ARIMA(3,1,2)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_gdp_a2 <- Arima(diff(gdp_log_ts), order=c(3, 1, 2),include.drift = TRUE) 
summary(fit_gdp_a2)

```


**Coefficients:** The model includes three autoregressive terms (ar1, ar2, ar3) and two moving average terms (ma1, ma2), with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(3,1,2)):

Given the coefficients:

- AR1: 0.3440
- AR2: 0.1100
- AR3: 0.0416
- MA1: -1.4101
- MA2: 0.4101


The equation for the ARIMA(3,1,2) model:

$$
x_t = 0.3440x_{t-1} + 0.1100x_{t-2} + 0.0416x_{t-3} + w_t - 1.4101w_{t-1} + 0.4101w_{t-2} 
$$

#### Better Model:

- ARIMA(1,1,1) has a lower AIC (-780) compared to ARIMA(3,1,2) (-775.75)

- ARIMA(1,1,1) also shows a lower AICc (-779.69) than ARIMA(3,1,2) (-774.86)

- ARIMA(1,1,1) has a lower BIC (-768.41) compared to ARIMA(3,1,2) (-755.46)

- ARIMA(3,1,2) exhibits a higher log likelihood (394.87) compared to ARIMA(1,1,1) (394), indicating a slightly better fit

- Both RMSE and MAE are slightly lower for ARIMA(3,1,2), but the differences are minimal, suggesting comparable predictive performance

- ARIMA(3,1,2) includes additional autoregressive terms which seem to capture more complexity in the data dynamics



Despite the ARIMA(3,1,2) having a slightly higher log likelihood and potentially capturing more complexity, the **ARIMA(1,1,1)** model is better for the series based on lower AIC, AICc, and BIC 




### CPI

- Best Model by AIC and AICc: ARIMA(3,1,2)
- Second Best Model: ARIMA(3,1,3)

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Fitting ARIMA'
#| warning: false
#| output: true

#Fitting ARIMA
fit_cpi_a2 <- Arima(diff(cpi_log_ts), order=c(3, 1, 3),include.drift = TRUE) 
summary(fit_cpi_a2)

```

**Coefficients:** The model includes three autoregressive terms (ar1, ar2, ar3) and three moving average terms (ma1, ma2, ma3), with all coefficients having significant values.
**Drift:** The drift coefficient is extremely small, suggesting a negligible linear trend in the differenced series.


For the Best Model (ARIMA(3,1,3)):

Given the coefficients:

- AR1: 0.0195 
- AR2: -0.6767
- AR3: 0.3641 
- MA1: -0.4767
- MA2: 0.2965 
- MA3: -0.7575


The equation for the ARIMA(3,1,3) model:

$$
\begin{aligned}
x_t = &\ 0.0195 x_{t-1} - 0.6767 x_{t-2} + 0.3641 x_{t-3} \\
      &\ + w_t - 0.4767 w_{t-1} + 0.2965 w_{t-2} - 0.7575 w_{t-3}
\end{aligned}
$$




#### Better Model:

- ARIMA(3,1,3) has a lower AIC (-3763.69) compared to ARIMA(3,1,2) (-3757.99)

- ARIMA(3,1,3) also shows a lower AICc (-3763.33) than ARIMA(3,1,2) (-3757.71)

- ARIMA(3,1,3) has a lower BIC (-3731.6) compared to ARIMA(3,1,2) (-3729.91)

- ARIMA(3,1,3) exhibits a higher log likelihood (1889.85) compared to ARIMA(3,1,2) (1885.99), indicating a slightly better fit

- For ARIMA(3,1,3), both RMSE and MAE are slightly lower than for ARIMA(3,1,2), indicating better predictive performance

- ARIMA(3,1,3) includes an additional moving average term which seems to capture more complexity in the data dynamics, providing a slight advantage in modeling more complex patterns



Given these considerations, the **ARIMA(3,1,3)** model is better for the series




## 

:::



# Model Diagnostics



::: panel-tabset

## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Diagnostics'
#| warning: false
#| output: true

set.seed(222)
## Fit the Best Model
diag_crudeoil <- capture.output(sarima(oil_log_ts, 2,1,2))
#print(diag_crudeoil)
cat(diag_crudeoil[25:55], diag_crudeoil[length(diag_crudeoil)], sep = "\n")

```


**Standardized Residuals**: The plot shows some fluctuation, but there don't appear to be any clear patterns or trends.

**ACF of Residuals**: The ACF plot for the residuals displays no significant spikes outside the confidence bounds (blue dashed lines), which implies that the residuals are white noise.

**Q-Q Plot of Std Residuals**: which assesses the normality of the residuals, shows an acceptable alignment

**Ljung-Box test**: The p-values in the plot are well above the 0.05 threshold, indicating that we fail to reject the null hypothesis of no autocorrelation among residuals at different lags. 

**Model Coefficients**: The coefficients are significant except for the second moving average coefficient (ma2), which has a p-value higher than 0.05. This indicates that while most coefficients contribute meaningfully to the model, the ma2 term may not be adding value.

Considering the log likelihood and the information criteria (AIC, AICc, BIC), the model seems to be well-fitted. A higher log likelihood and lower information criteria values generally indicate a better model fit.

Based on these diagnostics, the **ARIMA(2,1,2)** model seems to be adequately fitting the crude oil time series data. 


## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Diagnostics'
#| warning: false
#| output: true

set.seed(222)
## Fit the Best Model
diag_ng <- capture.output(sarima(oil_log_ts, 3,1,1))
#print(diag_ng)
cat(diag_ng[76:91], diag_ng[length(diag_ng)], sep = "\n")
```


**Standardized Residuals**: The plot of standardized residuals doesn't exhibit any obvious patterns, trends, or seasonality, which suggests that the model has captured the underlying process well. The residuals fluctuate around the zero line within a consistent range, indicating that the variance of the residuals is stable over time.

**ACF of Residuals**: The ACF plot for the residuals displays no significant spikes outside the confidence bounds (blue dashed lines), which implies that the residuals are white noise.

**Q-Q Plot of Std Residuals**: which assesses the normality of the residuals, shows an acceptable alignment

**Ljung-Box test**:The p-values in the plot are well above the 0.05 threshold, indicating that we fail to reject the null hypothesis of no autocorrelation among residuals at different lags. 

**Model Coefficients**: The coefficients are significant all appear to be statistically significant, as indicated by their p-values being well below the 0.05 threshold. This indicates that all the coefficients contribute meaningfully to the model.

Considering the log likelihood and the information criteria (AIC, AICc, BIC), the model seems to be well-fitted. A higher log likelihood and lower information criteria values generally indicate a better model fit.

Based on these diagnostics, the ARIMA(3,1,1) model seems to be adequately fitting the data.



## Electricity


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Diagnostics'
#| warning: false
#| output: true

set.seed(222)
## Fit the Best Model
diag_electricity <- capture.output(sarima(electricity_log_ts, 2,1,3))
#print(diag_electricity)
cat(diag_electricity[64:79], diag_electricity[length(diag_electricity)], sep = "\n")
```


**Standardized Residuals**: The plot of standardized residuals doesn't exhibit any obvious patterns, trends, or seasonality, which suggests that the model has captured the underlying process well. The residuals fluctuate around the zero line within a consistent range, indicating that the variance of the residuals is stable over time.

**ACF of Residuals**: The autocorrelation function (ACF) plot for the residuals does exhibits spikes outside the blue dashed confidence intervals, suggesting that there is autocorrelation.

**Q-Q Plot of Std Residuals**: which assesses the normality of the residuals, shows an acceptable alignment

**Ljung-Box test**: The p-values are very low (close to 0). This would suggest there is evidence of autocorrelation in the residuals.

**Model Coefficients**: The coefficients of the ARIMA model are statistically significant, with p-values below the 0.05 threshold, indicating they contribute meaningfully to the model.


Considering the log likelihood and the information criteria (AIC, AICc, BIC), the model seems to be well-fitted. A higher log likelihood and lower information criteria values generally indicate a better model fit.

Based on these diagnostics, we need further analysis for our model ARIMA(2,1,3)





## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Diagnostics'
#| warning: false
#| output: true

set.seed(222)
diag_gdp <- capture.output(sarima(gdp_log_ts, 1,1,1))
#print(diag_gdp)
cat(diag_gdp[30:41], diag_gdp[length(diag_gdp)], sep = "\n")
```

**Standardized Residuals**: The plot of standardized residuals doesn't exhibit any obvious patterns, trends, or seasonality, which suggests that the model has captured the underlying process well. The residuals fluctuate around the zero line within a consistent range, indicating that the variance of the residuals is stable over time.

**ACF of Residuals**: The ACF plot for the residuals displays no significant spikes outside the confidence bounds (blue dashed lines), which implies that the residuals are white noise.

**Q-Q Plot of Std Residuals**: which assesses the normality of the residuals, shows an acceptable alignment

**Ljung-Box test**:The p-values in the plot are well above the 0.05 threshold, indicating that we fail to reject the null hypothesis of no autocorrelation among residuals at different lags. 

**Model Coefficients**: The coefficients are significant all appear to be statistically significant, as indicated by their p-values being well below the 0.05 threshold. This indicates that all the coefficients contribute meaningfully to the model.

Considering the log likelihood and the information criteria (AIC, AICc, BIC), the model seems to be well-fitted. A higher log likelihood and lower information criteria values generally indicate a better model fit.

Based on these diagnostics, the ARIMA(1,1,1) model seems to be adequately fitting the data.



## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Diagnostics'
#| warning: false
#| output: true

set.seed(222)
diag_cpi <- capture.output(sarima(cpi_log_ts, 3,1,3))
#print(diag_cpi)
cat(diag_cpi[83:99], diag_cpi[length(diag_cpi)], sep = "\n")
```


**Standardized Residuals**: The plot of standardized residuals doesn't exhibit any obvious patterns, trends, or seasonality, which suggests that the model has captured the underlying process well. The residuals fluctuate around the zero line within a consistent range, indicating that the variance of the residuals is stable over time.

**ACF of Residuals**: The ACF plot for the residuals displays no significant spikes outside the confidence bounds (blue dashed lines), which implies that the residuals are white noise.

**Q-Q Plot of Std Residuals**: which assesses the normality of the residuals, shows an acceptable alignment

**Ljung-Box test**: The p-values are very low (close to 0). This would suggest there is evidence of autocorrelation in the residuals.

**Model Coefficients**: The coefficients are significant all appear to be statistically significant, as indicated by their p-values being well below the 0.05 threshold. This indicates that all the coefficients contribute meaningfully to the model.

Considering the log likelihood and the information criteria (AIC, AICc, BIC), the model seems to be well-fitted. A higher log likelihood and lower information criteria values generally indicate a better model fit.

Based on these diagnostics, the ARIMA(3,1,3) model seems to be adequately fitting the data.



:::


# Auto.Arima()


::: panel-tabset

## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

#Apply auto.arima 
oil_auto <- auto.arima(oil_log_ts)

#Summary
summary(oil_auto)

```

We used auto.arima() to pick the best model, and it chose ARIMA(2,1,0)(1,0,0)[12] which includes a seasonal component.



## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

#Apply auto.arima 
gas_auto <- auto.arima(gas_log_ts)

#Summary
summary(gas_auto)
```

We used auto.arima() to pick the best model, and it chose ARIMA(0,1,2). 


## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

auto.arima(electricity_log_ts)

```

We used auto.arima() to pick the best model, and it chose ARIMA(0,1,1)(0,1,1)[12] which includes a seasonal component.


## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true


auto.arima(gdp_log_ts)

```

We used auto.arima() to pick the best model, and it chose ARIMA(0,1,0). 

## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

auto.arima((cpi_log_ts))

```

We used auto.arima() to pick the best model, and it chose ARIMA(1,1,2)(0,0,1)[12] which includes a seasonal component.

:::

# Forecasting



::: panel-tabset


## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting'
#| warning: false
#| output: true

#Forecasting the next 36
forecast_crudeoil2 <- forecast(fit_crudeoil2, h=36)

#Plotting the forecast
autoplot(forecast_crudeoil2) + 
  labs(title="Forecast for Crude Oil Prices", 
       x="Year", y="Logged Prices", 
       caption="Forecast using ARIMA(2,1,2)") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

The actual forecast (the line within the shaded area) appears to stay around the level of the most recent actual observations, which suggests that the model does not anticipate any significant changes or trends in the near future. However, the wide confidence intervals imply that while the expected value is stable, the actual future prices could vary widely around this expected value.


## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting'
#| warning: false
#| output: true

#Forecasting the next 36
forecast_ng2 <- forecast(fit_NaturalGas, h=36)

#Plotting the forecast
autoplot(forecast_ng2) + 
  labs(title="Forecast for Natural Gas Prices", 
       x="Year", y="Logged Prices", 
       caption="Forecast using ARIMA(3,1,1)") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


```

The actual forecast (the line within the shaded area) appears to stay around the level of the most recent actual observations, which suggests that the model does not anticipate any significant changes or trends in the near future. However, the wide confidence intervals imply that while the expected value is stable, the actual future prices could vary widely around this expected value.



## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting'
#| warning: false
#| output: true


#Forecasting the next 36
forecast_elect2 <- forecast(fit_electricity, h=36)

#Plotting the forecast
autoplot(forecast_elect2) + 
  labs(title="Forecast for Electricty Prices", 
       x="Year", y="Logged Prices", 
       caption="Forecast using ARIMA(2,1,3)") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))


```


The forecast line (within the shaded area) staying around the level of the most recent observations indicates that the ARIMA(2,1,3) model doesn't project any significant increase or decrease in prices. The wide confidence intervals suggest there is a considerable amount of uncertainty in the forecast. This uncertainty might arise from the natural volatility in electricity prices or seasonality.


## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting'
#| warning: false
#| output: true


#Forecasting the next 36
forecast_gdp_a <- forecast(fit_gdp_a, h=36)

#Plotting the forecast
autoplot(forecast_gdp_a) + 
  labs(title="Forecast for GDP", 
       x="Year", y="Logged Prices", 
       caption="Forecast using ARIMA(1,1,1)") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```

The forecast line (within the shaded area) staying around the level of the most recent observations indicates that the ARIMA(1,1,1) model doesn't project any significant increase or decrease in prices. 




## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting'
#| warning: false
#| output: true



#Forecasting the next 36
forecast_cpi_a <- forecast(fit_cpi_a2, h=36)

#Plotting the forecast
autoplot(forecast_cpi_a) + 
  labs(title="Forecast for CPI", 
       x="Year", y="Logged Prices", 
       caption="Forecast using ARIMA(3,1,3)") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```

The forecast line (within the shaded area) staying around the level of the most recent observations indicates that the ARIMA(1,1,1) model doesn't project any significant increase or decrease in prices. 


:::


The forecasting model has been designed to predict future trends for our datasets. Utilizing historical data, the model, foresee market movements up to the period we have defined, identifying patterns to craft reliable future forecasts. The predictions are visualized through the charts above, showcasing the range of possible outcomes, providing a clear, comprehensive view of upcoming market dynamics. While our model offers insightful foresight, it's built on the principle that future patterns will follow the past. 


# Comparing with Benchmark Methods


::: panel-tabset

## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing Benchmark'
#| warning: false
#| output: true

#Generate forecasts
forecast_meanf <- meanf(oil_log_ts, h=36)
forecast_naive <- naive(oil_log_ts, h=36)
forecast_snaive <- snaive(oil_log_ts, h=36)
forecast_rwf <- rwf(oil_log_ts, drift=TRUE, h=36) 
forecast_arima <- forecast(fit_crudeoil2, h=36)

#Plotting the forecasts
autoplot(oil_log_ts, series="Data") +
  autolayer(forecast_meanf, series="Mean Forecast", PI=FALSE) +
  autolayer(forecast_naive, series="Naive Forecast", PI=FALSE) +
  autolayer(forecast_snaive, series="Seasonal Naive Forecast", PI=FALSE) +
  autolayer(forecast_rwf, series="Drift", PI=FALSE) +
  autolayer(forecast_arima, series="ARIMA(2,1,2) Forecast", PI=FALSE) +
  labs(title="Comparative Forecast for Crude Oil Prices",
       x="Year", y="Logged Prices",
       caption="Comparing ARIMA(2,1,2) with Benchmark Methods") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(colour=guide_legend(title="Forecast Method")) +
  scale_color_manual(values=c("Data"="#000000", "Mean Forecast"="red",
                              "Naive Forecast"="green", "Seasonal Naive Forecast"="purple",
                              "Drift"="orange", "ARIMA(2,1,2) Forecast"="blue"),
                     breaks=c("Data", "Mean Forecast", "Naive Forecast",
                              "Seasonal Naive Forecast", "Drift", "ARIMA(2,1,2) Forecast"))


accuracy(forecast_crudeoil2)

```

We can observe the forecasts generated by different methods are quite close to each other, with only slight variations in their predicted future values. 

- The mean forecast (Mean Forecast) is a flat line, indicating no change is expected in the future, which can be a result of averaging past data. 

- The naïve forecast (Naive Forecast) and seasonal naïve forecast (Seasonal Naive Forecast) suggest a continuation of the last observed patterns. 

- The random walk with drift forecast (Drift) shows a slight upward trend, indicating an expected gradual increase over time, likely accounting for the upward trend seen in the historical data. 

The ARIMA(2,1,2) model's forecast also predicts future values that follow the patterns observed in the recent past but with a confidence interval that widens over time.



## Natural Gas

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing Benchmark'
#| warning: false
#| output: true

#Generate forecasts
forecast_meanf_ng <- meanf(gas_log_ts, h=36)
forecast_naive_ng <- naive(gas_log_ts, h=36)
forecast_snaive_ng <- snaive(gas_log_ts, h=36)
forecast_rwf_ng <- rwf(gas_log_ts, drift=TRUE, h=36) 
forecast_arima_ng <- forecast(fit_NaturalGas, h=36)

#Plotting the forecasts
autoplot(gas_log_ts, series="Data") +
  autolayer(forecast_meanf_ng, series="Mean Forecast", PI=FALSE) +
  autolayer(forecast_naive_ng, series="Naive Forecast", PI=FALSE) +
  autolayer(forecast_snaive_ng, series="Seasonal Naive Forecast", PI=FALSE) +
  autolayer(forecast_rwf_ng, series="Drift", PI=FALSE) +
  autolayer(forecast_arima_ng, series="ARIMA(3,1,1) Forecast", PI=FALSE) +
  labs(title="Comparative Forecast for Natural Gas Prices",
       x="Year", y="Logged Prices",
       caption="Comparing ARIMA(3,1,1) with Benchmark Methods") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(colour=guide_legend(title="Forecast Method")) +
  scale_color_manual(values=c("Data"="#000000", "Mean Forecast"="red",
                              "Naive Forecast"="green", "Seasonal Naive Forecast"="purple",
                              "Drift"="orange", "ARIMA(3,1,1) Forecast"="blue"),
                     breaks=c("Data", "Mean Forecast", "Naive Forecast",
                              "Seasonal Naive Forecast", "Drift", "ARIMA(3,1,1) Forecast"))


accuracy(forecast_arima_ng)

```


We can observe the forecasts generated by different methods are quite close to each other, with only slight variations in their predicted future values. 

- The mean forecast (Mean Forecast) is a horizontal line, indicating a projection that the future prices will remain on average at the last observed level of the historical data. This method does not account for trends or seasonality.

- The naïve forecast (Naive Forecast) and seasonal naïve forecast (Seasonal Naive Forecast) suggest a continuation of the last observed patterns. 

- The random walk with drift forecast (Drift) shows a slight upward trend, indicating an expected gradual increase over time, likely accounting for the upward trend seen in the historical data. 

These accuracy measures indicate that the ARIMA(3,1,1) model is providing forecasts that are consistent with the historical data, not deviating significantly from the patterns already observed, and is performing well against standard benchmarks.



## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing Benchmark'
#| warning: false
#| output: true


#Generate forecasts
forecast_meanf_elec <- meanf(electricity_log_ts, h=36)
forecast_naive_elec <- naive(electricity_log_ts, h=36)
forecast_snaive_elec <- snaive(electricity_log_ts, h=36)
forecast_rwf_elec <- rwf(electricity_log_ts, drift=TRUE, h=36) 
forecast_arima_elec <- forecast(fit_electricity, h=36)

#Plotting the forecasts
autoplot(electricity_log_ts, series="Data") +
  autolayer(forecast_meanf_elec, series="Mean Forecast", PI=FALSE) +
  autolayer(forecast_naive_elec, series="Naive Forecast", PI=FALSE) +
  autolayer(forecast_snaive_elec, series="Seasonal Naive Forecast", PI=FALSE) +
  autolayer(forecast_rwf_elec, series="Drift", PI=FALSE) +
  autolayer(forecast_arima_elec, series="ARIMA(2,1,3) Forecast", PI=FALSE) +
  labs(title="Comparative Forecast for Electricity Prices",
       x="Year", y="Logged Prices",
       caption="Comparing ARIMA(2,1,3) with Benchmark Methods") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(colour=guide_legend(title="Forecast Method")) +
  scale_color_manual(values=c("Data"="#000000", "Mean Forecast"="red",
                              "Naive Forecast"="green", "Seasonal Naive Forecast"="purple",
                              "Drift"="orange", "ARIMA(2,1,3) Forecast"="blue"),
                     breaks=c("Data", "Mean Forecast", "Naive Forecast",
                              "Seasonal Naive Forecast", "Drift", "ARIMA(2,1,3) Forecast"))


accuracy(forecast_arima_elec)

```

We can observe the forecasts generated by different methods are quite close to each other, with only slight variations in their predicted future values. 

- The mean forecast (Mean Forecast) is a flat red line suggesting that,electricity prices will remain at the last observed average level.

- The naïve forecast (Naive Forecast) similar to the mean forecast, the naive method carries forward the last observed value. This forecast is also flat, indicating no change from the last observed price level.

- Seasonal naïve forecast (Seasonal Naive Forecast) suggest a continuation of the last observed patterns. 

- The random walk with drift forecast (Drift) shows a slight upward trend, indicating an expected gradual increase over time, likely accounting for the upward trend seen in the historical data. 

ACF1 close to zero would indicate no autocorrelation; however, a value of 0.223077 suggests some autocorrelation in the model residuals.

The ARIMA(2,1,3) model generally captures the central tendency of the historical data, the presence of autocorrelation in the residuals indicates that there may be more information in the data.


## GDP

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing Benchmark'
#| warning: false
#| output: true


#Generate forecasts
forecast_meanf_gdp <- meanf(gdp_log_ts, h=36)
forecast_naive_gdp <- naive(gdp_log_ts, h=36)
forecast_snaive_gdp <- snaive(gdp_log_ts, h=36)
forecast_rwf_gdp <- rwf(gdp_log_ts, drift=TRUE, h=36) 
forecast_arima_gdp <- forecast(fit_gdp_a, h=36)

#Plotting the forecasts
autoplot(gdp_log_ts, series="Data") +
  autolayer(forecast_meanf_gdp, series="Mean Forecast", PI=FALSE) +
  autolayer(forecast_naive_gdp, series="Naive Forecast", PI=FALSE) +
  autolayer(forecast_snaive_gdp, series="Seasonal Naive Forecast", PI=FALSE) +
  autolayer(forecast_rwf_gdp, series="Drift", PI=FALSE) +
  autolayer(forecast_arima_gdp, series="ARIMA(1,1,1) Forecast", PI=FALSE) +
  labs(title="Comparative Forecast for GDP",
       x="Year", y="Logged Prices",
       caption="Comparing ARIMA(1,1,1) with Benchmark Methods") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(colour=guide_legend(title="Forecast Method")) +
  scale_color_manual(values=c("Data"="#000000", "Mean Forecast"="red",
                              "Naive Forecast"="green", "Seasonal Naive Forecast"="purple",
                              "Drift"="orange", "ARIMA(1,1,1) Forecast"="blue"),
                     breaks=c("Data", "Mean Forecast", "Naive Forecast",
                              "Seasonal Naive Forecast", "Drift", "ARIMA(1,1,1) Forecast"))


accuracy(forecast_arima_gdp)


```



## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing Benchmark'
#| warning: false
#| output: true

#Generate forecasts
forecast_meanf_cpi <- meanf(cpi_log_ts, h=36)
forecast_naive_cpi <- naive(cpi_log_ts, h=36)
forecast_snaive_cpi <- snaive(cpi_log_ts, h=36)
forecast_rwf_cpi <- rwf(cpi_log_ts, drift=TRUE, h=36) 
forecast_arima_cpi <- forecast(fit_cpi_a2, h=36)

#Plotting the forecasts
autoplot(cpi_log_ts, series="Data") +
  autolayer(forecast_meanf_cpi, series="Mean Forecast", PI=FALSE) +
  autolayer(forecast_naive_cpi, series="Naive Forecast", PI=FALSE) +
  autolayer(forecast_snaive_cpi, series="Seasonal Naive Forecast", PI=FALSE) +
  autolayer(forecast_rwf_cpi, series="Drift", PI=FALSE) +
  autolayer(forecast_arima_cpi, series="ARIMA(3,1,3) Forecast", PI=FALSE) +
  labs(title="Comparative Forecast for CPI",
       x="Year", y="Logged Prices",
       caption="Comparing ARIMA(3,1,3) with Benchmark Methods") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(colour=guide_legend(title="Forecast Method")) +
  scale_color_manual(values=c("Data"="#000000", "Mean Forecast"="red",
                              "Naive Forecast"="green", "Seasonal Naive Forecast"="purple",
                              "Drift"="orange", "ARIMA(3,1,3) Forecast"="blue"),
                     breaks=c("Data", "Mean Forecast", "Naive Forecast",
                              "Seasonal Naive Forecast", "Drift", "ARIMA(3,1,3) Forecast"))


accuracy(forecast_arima_cpi)

```




:::


# SARIMA

We can observe that in some of the datasets, there is clear seasonality. The three seasonal datasets are: Crude Oil, Electricity, and CPI. For these datasets, we need seasonal differencing and thus we will use SARIMA model.


# SARIMA ACF and PACF Plots


::: panel-tabset

## Crude Oil


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'SARIMA ACF-PACF'
#| warning: false
#| output: true

#Seasonal differencing
seasonally_diff_oil_log_ts <- diff(oil_log_ts, lag=12)
#ACF and PACF plots
ggtsdisplay(seasonally_diff_oil_log_ts, main='Seasonally Differenced Crude Oil Prices')

```

Based on these graphs and examining the ACF and PACF plots, we can suggest a range of values for P and Q:

P = 1, 2
D = 1
Q = 1, 2




## Electricity


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'SARIMA ACF-PACF'
#| warning: false
#| output: true

#Seasonal differencing
seasonally_diff_elec_log_ts <- diff(electricity_log_ts, lag=12)
#ACF and PACF plots
ggtsdisplay(seasonally_diff_elec_log_ts, main='Seasonally Differenced Electricity Prices')

```

Based on these graphs and examining the ACF and PACF plots, we can suggest a range of values for P and Q:

P = 1, 2
D = 1
Q = 1, 2


## CPI


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'SARIMA ACF-PACF'
#| warning: false
#| output: true

#Seasonal differencing
seasonally_diff_cpi_log_ts <- diff(cpi_log_ts, lag=12)
#ACF and PACF plots
ggtsdisplay(seasonally_diff_cpi_log_ts, main='Seasonally Differenced CPI')


```

Based on these graphs and examining the ACF and PACF plots, we can suggest a range of values for P and Q:

P = 1, 2
D = 1
Q = 1, 2





:::



# Finding SARIMA Parameters (p, d, q)



::: panel-tabset

## Crude Oil

p = 1, 2
q = 1, 2, 3
d = 1
D = 1
P = 1, 2
Q = 1, 2

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Parameters'
#| warning: false
#| output: true

SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){
  
  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)
  temp=c()
  d = 1
  D = 1
  s = 12
  
  i=1
  temp= data.frame()
  ls=matrix(rep(NA,9*19),nrow=19)
  
  
  for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          if(p+d+q+P+D+Q<=9)
          {
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
            #print(i)
            
          }
          
        }
      }
    }
    
  }
  
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  
  temp
}

output=SARIMA.c(p1=1,p2=2,q1=1,q2=3,P1=1,P2=2,Q1=1,Q2=2,data=oil_log_ts)


knitr::kable(output)

output[which.min(output$AIC),]
output[which.min(output$BIC),]
output[which.min(output$AICc),]

```



The model with the lowest AIC and AICc is ARIMA(0,1,2)[0,1,1]12.
and lowest BIC is ARIMA(0,1,1)[0,1,1]12.



## Electricity

p = 1, 2, 3
q = 1 to 7
d = 1
D = 1
P = 1, 2
Q = 1, 2

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Parameters'
#| warning: false
#| output: true

SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){
  
  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)
  temp=c()
  d = 1
  D = 1
  s = 12
  
  i=1
  temp= data.frame()
  ls=matrix(rep(NA,9*24),nrow=24)
  
  
  for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          if(p+d+q+P+D+Q<=9)
          {
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
            #print(i)
            
          }
          
        }
      }
    }
    
  }
  
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  
  temp
}

output=SARIMA.c(p1=1,p2=3,q1=1,q2=7,P1=1,P2=2,Q1=1,Q2=2,data=electricity_log_ts)


knitr::kable(output)

output[which.min(output$AIC),]
output[which.min(output$BIC),]
output[which.min(output$AICc),]

```



The model with the lowest AIC and AICc is ARIMA(1,1,0)(0,1,1)[12].
and lowest BIC is ARIMA(0,1,0)(0,1,1)[12].



## CPI

p = 1 to 12
q = 1 to 9
d = 1
D = 1
P = 1, 2
Q = 1, 2

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Parameters'
#| warning: false
#| output: true

SARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){
  
  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)
  temp=c()
  d = 1
  D = 1
  s = 12
  
  i=1
  temp= data.frame()
  ls=matrix(rep(NA,9*25),nrow=25)
  
  
  for (p in p1:p2)
  {
    for(q in q1:q2)
    {
      for(P in P1:P2)
      {
        for(Q in Q1:Q2)
        {
          if(p+d+q+P+D+Q<=9)
          {
            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))
            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)
            i=i+1
            #print(i)
            
          }
          
        }
      }
    }
    
  }
  
  temp= as.data.frame(ls)
  names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")
  
  temp
}

output=SARIMA.c(p1=1,p2=12,q1=1,q2=9,P1=1,P2=2,Q1=1,Q2=2,data=cpi_log_ts)


knitr::kable(output)

output[which.min(output$AIC),]
output[which.min(output$BIC),]
output[which.min(output$AICc),]

```



The model with the lowest AIC and AICc is ARIMA(0,1,2)[0,1,1]12.
and lowest BIC is ARIMA(0,1,1)[0,1,1]12.




:::




# SARIMA Full Model Diagnostics

::: panel-tabset


## Crude Oil


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'SARIMA Fit'
#| warning: false
#| output: true
set.seed(222)
model_output_s <- capture.output(sarima(oil_log_ts, 0,1,2,0,1,1,12))
#model_output_s
cat(model_output_s[25:37], model_output_s[length(model_output_s)], sep = "\n") 

```

- The Standardized Residuals plot doesn’t show any apparent trends or seasonality, indicating that the model residuals are randomly distributed around zero


- The ACF of Residuals plot exhibits all bars within the confidence bounds, suggesting that there's no autocorrelation left in the residuals after fitting the model.


- The Normal Q-Q Plot of Std Residuals displays points that mostly align suggesting that the residuals are approximately normally distributed.


- The p-values for the Ljung-Box statistic are all above the significance threshold of 0.05, implying that there's no significant autocorrelation at any lag


- The coefficients for the non-seasonal terms ma1 and ma2 are 0.5483 and 0.1238, respectively. Both are statistically significant as their p-values are less than 0.05. The seasonal moving average term sma1 is -1.0000, which indicates a strong seasonal pattern has been identified and incorporated into the model.


These criteria suggests that the model strikes a good balance between fitting the historical data well and not overfitting.

Equation:


$$
x_t = 0.5483 a_{t-1} + 0.1238 a_{t-2} - 1.00 a_{t-12} + a_t
$$




## Electricity
ARIMA(1,1,0)(0,1,1)[12].

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'SARIMA Fit'
#| warning: false
#| output: true

set.seed(222)
model_output_e <- capture.output(sarima(electricity_log_ts, 1,1,0,0,1,1,12))
#model_output_e
cat(model_output_e[21:32], model_output_e[length(model_output_e)], sep = "\n") 

```

- The Standardized Residuals plot doesn’t show any apparent trends or seasonality, indicating that the model residuals are randomly distributed around zero


- The ACF of Residuals plot exhibits bars within the confidence bounds for the majority of lags, suggesting that there’s minimal autocorrelation left in the residuals after fitting the model.


- The Normal Q-Q Plot of Std Residuals displays points that mostly align suggesting that the residuals are approximately normally distributed.

- The p-values for the Ljung-Box statistic are all above the significance threshold of 0.05, implying that there's no significant autocorrelation at any lag

- The model's coefficients are statistically significant (except for ar1, which is marginally insignificant with a p-value slightly above 0.05). The coefficient for ar1 is -0.0944, showing a small but negative effect, and for sma1, it is -0.7530, which is quite substantial.


These criteria suggests that the model strikes a good balance between fitting the historical data well and not overfitting.

Equation:

$$
x_t = - 0.0944 x_{t-1} - 0.7530 a_{t-12} + a_t
$$



## CPI


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'SARIMA Fit'
#| warning: false
#| output: true
set.seed(222)
model_output_c <- capture.output(sarima(cpi_log_ts, 0,1,2,0,1,1,12))
#model_output_c
cat(model_output_c[22:34], model_output_c[length(model_output_c)], sep = "\n") 

```

- The Standardized Residuals plot doesn’t show any apparent trends or seasonality, indicating that the model residuals are randomly distributed around zero


- The ACF of Residuals plot exhibits bars within the confidence bounds for the majority of lags, suggesting that there’s minimal autocorrelation left in the residuals after fitting the model.


- The Normal Q-Q Plot of Std Residuals displays points that mostly align suggesting that the residuals are approximately normally distributed.


- The p-values for the Ljung-Box statistic are not above the significance threshold of 0.05, implying that there's still some autocorrelation left


- The coefficients for the non-seasonal terms ma1 and ma2 are 0.5566 and 0.1172, respectively. Both are statistically significant as their p-values are less than 0.05. The seasonal moving average term sma1 is -0.9997, which indicates a seasonal pattern has been identified and incorporated into the model.


These criteria suggests that the model strikes a good balance between fitting the historical data well and not overfitting.

Equation:


$$
x_t = 0.5566 a_{t-1} + 0.1172 a_{t-2} + a_t - 0.9997 a_{t-12}
$$



:::


# Auto ARIMA

::: panel-tabset


## Crude Oil


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

auto.arima(oil_log_ts)

```




## Electricity


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

auto.arima(electricity_log_ts)

```


## CPI


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Auto Arima'
#| warning: false
#| output: true

auto.arima(cpi_log_ts)

```




:::


# Forecasting SARIMA

::: panel-tabset


## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting SARIMA'
#| warning: false
#| output: true


#Forecasting the next 36 months
fit_oil_s <- Arima(oil_log_ts, order=c(0,1,2), seasonal=c(0,1,1))
forecast_oil_s <- forecast(fit_oil_s, h=36)

#Plotting the forecast
autoplot(forecast_oil_s) + 
  labs(title="Forecast for Crude Oil Prices - SARIMA", 
       x="Year", y="Logged Prices", 
       caption="Forecast using SARIMA(0,1,2)(0,1,1)[12]") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```


The forecast generated by the SARIMA(0,1,2)(0,1,1)[12] model appears as a continuation of the last known pattern. This suggests that the model doesn't expect drastic changes in the price levels immediately after the last known data point, with a slightly upward trend.


## Electricity


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting SARIMA'
#| warning: false
#| output: true

#Forecasting the next 36 months
fit_elec_s <- Arima(electricity_log_ts, order=c(1,1,0), seasonal=c(0,1,1))
forecast_elect_s <- forecast(fit_elec_s, h=36)

#Plotting the forecast
autoplot(forecast_elect_s) + 
  labs(title="Forecast for Electricty Prices - SARIMA", 
       x="Year", y="Logged Prices", 
       caption="Forecast using SARIMA(1,1,0)(0,1,1)[12]") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```


The forecast generated by the SARIMA(1,1,0)(0,1,1)[12] model appears as a continuation of the last known pattern. This suggests that the model doesn't expect drastic changes in the price levels immediately after the last known data point, with an upward trend.



## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Forecasting SARIMA'
#| warning: false
#| output: true


#Forecasting the next 36 months
fit_cpi_s <- Arima(cpi_log_ts, order=c(0,1,2), seasonal=c(0,1,1))
forecast_cpi_s <- forecast(fit_cpi_s, h=36)

#Plotting the forecast
autoplot(forecast_cpi_s) + 
  labs(title="Forecast for CPI - SARIMA", 
       x="Year", y="Logged Prices", 
       caption="Forecast using SARIMA(0,1,2)(0,1,1)[12]") +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

The forecast generated by the SARIMA(0,1,2)(0,1,1)[12] model appears as a continuation of the last known pattern. This suggests that the model doesn't expect drastic changes in the price levels immediately after the last known data point, with an upward trend.


:::



# Comparing SARIMA With Benchmark

::: panel-tabset


## Crude Oil

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing SARIMA Benchmark'
#| warning: false
#| output: true


#Plot
autoplot(oil_log_ts) +
  autolayer(meanf(oil_log_ts, h=36), series="Mean", PI=FALSE) +
  autolayer(naive(oil_log_ts, h=36), series="Naïve", PI=FALSE) +
  autolayer(snaive(oil_log_ts, h=36), series="SNaïve", PI=FALSE) +
  autolayer(rwf(oil_log_ts, h=36, drift=TRUE), series="Drift", PI=FALSE) +
  autolayer(forecast(fit_oil_s, 36), series="SARIMA", PI=FALSE) +
  labs(title="Comparing SARIMA with Benchmark Forecasts for Crude Oil Prices",
       x="Time", y="Logged Prices",
       caption="Forecasting Methods") +
  guides(colour=guide_legend(title="Forecast")) +
  theme_bw()

```

- Mean Forecast (Yellow): The flat line suggests that if we consider the average of the past, we would expect the future to hold steady at this average price level.
  

- Naive Forecast (Light Green): Also a flat line suggests that if the future simply repeats the last known price, there won't be any change moving forward.


- Seasonal Naive Forecast (Purple): This method extends the last observed seasonal pattern into the future.


- Drift Forecast (Red): Includes a trend, show an upward trend over time.


- SARIMA Forecast (Blue): This forecast captures both the recent levels and the seasonal patterns. It seems to predict a slight increase followed by seasonal variations.


SARIMA model outperformed most of the benchmark methods




## Electricity

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing SARIMA Benchmark'
#| warning: false
#| output: true

autoplot(electricity_log_ts) +
  autolayer(meanf(electricity_log_ts, h=36), series="Mean", PI=FALSE) +
  autolayer(naive(electricity_log_ts, h=36), series="Naïve", PI=FALSE) +
  autolayer(snaive(electricity_log_ts, h=36), series="SNaïve", PI=FALSE) +
  autolayer(rwf(electricity_log_ts, h=36, drift=TRUE), series="Drift", PI=FALSE) +
  autolayer(forecast(fit_elec_s, 36), series="SARIMA", PI=FALSE) +
  labs(title="Comparing SARIMA with Benchmark Forecasts for Electricity Prices",
       x="Time", y="Logged Prices",
       caption="Forecasting Methods") +
  guides(colour=guide_legend(title="Forecast")) +
  theme_bw()

```

- Mean Forecast (Yellow): The flat line suggests that if we consider the average of the past, we would expect the future to hold steady at this average price level.
  

- Naive Forecast (Light Green): Also a flat line suggests that if the future simply repeats the last known price, there won't be any change moving forward.

- Seasonal Naive Forecast (Purple): This method extends the last observed seasonal pattern into the future.

- Drift Forecast (Red): Includes a trend, show an upward trend over time.

- SARIMA Forecast (Blue): This forecast captures both the recent levels and the seasonal patterns. It seems to predict a slight increase followed by seasonal variations.

SARIMA model outperformed most of the benchmark methods




## CPI

```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'Comparing SARIMA Benchmark'
#| warning: false
#| output: true


#Plot
autoplot(cpi_log_ts) +
  autolayer(meanf(cpi_log_ts, h=36), series="Mean", PI=FALSE) +
  autolayer(naive(cpi_log_ts, h=36), series="Naïve", PI=FALSE) +
  autolayer(snaive(cpi_log_ts, h=36), series="SNaïve", PI=FALSE) +
  autolayer(rwf(cpi_log_ts, h=36, drift=TRUE), series="Drift", PI=FALSE) +
  autolayer(forecast(fit_cpi_s, 36), series="SARIMA", PI=FALSE) +
  labs(title="Comparing SARIMA with Benchmark Forecasts for CPI",
       x="Time", y="Logged Prices",
       caption="Forecasting Methods") +
  guides(colour=guide_legend(title="Forecast")) +
  theme_bw()

```

- Mean Forecast (Yellow): The flat line suggests that if we consider the average of the past, we would expect the future to hold steady at this average price level.
  

- Naive Forecast (Light Green): Also a flat line suggests that if the future simply repeats the last known price, there won't be any change moving forward.


- Seasonal Naive Forecast (Purple): This method extends the last observed seasonal pattern into the future.


- Drift Forecast (Red): Includes a trend, show an upward trend over time.


- SARIMA Forecast (Blue): This forecast captures both the recent levels and the seasonal patterns. It seems to predict a slight increase followed by seasonal variations.


SARIMA model outperformed most of the benchmark methods







:::




# Cross-Validation

::: panel-tabset


## Crude Oil

The best model for this dataset using manual selection is ARIMA(0,1,2)(0,1,1)[12], while the auto arima model is ARIMA(2,1,0)(1,0,0)[12].


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'CV'
#| warning: false
#| output: true

x=oil_log_ts
  
set.seed(222)
k <- 72 # first training set length (6 seasonal lags - 6*12)
n <- length(x) #408
n-k #336 rest of the observations

#n-k=336; 336/12=28; k=72


mae1 <- matrix(NA, 28,12) 
mae2 <- matrix(NA,28,12)

st <- tsp(oil_log_ts)[1]+(k-1)/12
for(i in 1:28)
{
  #xtrain <- window(a10, start=st+(i-k+1)/12, end=st+i/12)
  xtrain <- window(oil_log_ts, end=st + i-1)
  xtest <- window(oil_log_ts, start=st + (i-1) + 1/12, end=st + i)
  # Manual model
  fit <- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12),
                method="ML")
  fcast <- forecast(fit, h=12)
  # Auto Arima
  fit2 <- Arima(xtrain, order=c(2,1,0), seasonal=list(order=c(1,0,0), period=12),
                method="ML")
  fcast2 <- forecast(fit2, h=12)
  
  mae1[i,] <- abs(fcast$mean-xtest)
  mae2[i,] <- abs(fcast2$mean-xtest)
  
}

plot(1:12, colMeans(mae1,na.rm=TRUE), type="l", col=2, xlab="horizon", ylab="MAE")
lines(1:12, colMeans(mae2,na.rm=TRUE), type="l",col=3)
legend("topleft",legend=c("Manual Model","Auto Arima"),col=2:4,lty=1)
```

Initially, both models perform similarly, indicating that both models have similar forecasting accuracy in the short term. Then as we move ahead the model begins to diverge, but the two lines remain close, with manual always having lower values. 
Suggesting that both models return to similar levels of forecasting accuracy but because of lower MAE we would chose Manual Model ARIMA(2,1,0)(1,0,0)[12].


## Electricity

The best model for this dataset using manual selection is ARIMA(1,1,0)(0,1,1)[12], while the auto arima model is ARIMA(0,1,1)(0,1,1)[12].


```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'CV'
#| warning: false
#| output: true


x=electricity_log_ts
  
set.seed(222)
k <- 48 # first training set length (4 seasonal lags - 4*12)
n <- length(x) #407
n-k #359 rest of the observations

#n-k=360; 360/12=30; k=48

mae1 <- matrix(NA, 29,12) 
mae2 <- matrix(NA,29,12)

st <- tsp(electricity_log_ts)[1]+(k-1)/12
for(i in 1:29)
{
  xtrain <- window(electricity_log_ts, end=st + i-1)
  xtest <- window(electricity_log_ts, start=st + (i-1) + 1/12, end=st + i)
  # Manual model
  fit <- Arima(xtrain, order=c(1,1,0), seasonal=list(order=c(0,1,1), period=12), 
               method="ML")
  fcast <- forecast(fit, h=12)
  # Auto Arima
  fit2 <- Arima(xtrain, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12),
                method="ML")
  fcast2 <- forecast(fit2, h=12)
  
  mae1[i,] <- abs(fcast$mean-xtest)
  mae2[i,] <- abs(fcast2$mean-xtest)
  
}

plot(1:12, colMeans(mae1,na.rm=TRUE), type="l", col=2, xlab="horizon", ylab="MAE")
lines(1:12, colMeans(mae2,na.rm=TRUE), type="l",col=3)
legend("topleft",legend=c("Manual Model","Auto Arima"),col=2:4,lty=1)
```

Initially, both models perform similarly, as indicated by the overlapping lines. As the horizon increases, there is some divergence between the models. However, the difference in their performance remains quite small throughout.
Suggesting that both models return to similar levels of forecasting accuracy.


## CPI

The best model for this dataset using manual selection is ARIMA(0,1,2)[0,1,1]12, while the auto arima model is ARIMA(1,1,2)(0,0,1)[12].



```{r, warning=FALSE, message=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: 'CV'
#| warning: false
#| output: true

x=cpi_log_ts
  
set.seed(222)
k <- 72 # first training set length (6 seasonal lags - 6*12)
n <- length(x) #410
#n-k #338 rest of the observations

#n-k=338; 338/12=28; k=72


mae1 <- matrix(NA, 28,12) 
mae2 <- matrix(NA,28,12)

st <- tsp(cpi_log_ts)[1]+(k-1)/12
for(i in 1:28)
{
  #xtrain <- window(a10, start=st+(i-k+1)/12, end=st+i/12)
  xtrain <- window(cpi_log_ts, end=st + i-1)
  xtest <- window(cpi_log_ts, start=st + (i-1) + 1/12, end=st + i)
  # Manual model
  fit <- Arima(xtrain, order=c(0,1,2), seasonal=list(order=c(0,1,1), period=12),
                method="ML")
  fcast <- forecast(fit, h=12)
  # Auto Arima
  fit2 <- Arima(xtrain, order=c(2,1,0), seasonal=list(order=c(1,0,0), period=12),
                method="ML")
  fcast2 <- forecast(fit2, h=12)
  
  mae1[i,] <- abs(fcast$mean-xtest)
  mae2[i,] <- abs(fcast2$mean-xtest)
  
}

plot(1:12, colMeans(mae1,na.rm=TRUE), type="l", col=2, xlab="horizon", ylab="MAE")
lines(1:12, colMeans(mae2,na.rm=TRUE), type="l",col=3)
legend("topleft",legend=c("Manual Model","Auto Arima"),col=2:4,lty=1)
```

Auto Arima appears to consistently have a higher MAE compared to the Manual Model across all forecast horizons, indicating that the manual model has, on average, lower errors and hence is making more accurate predictions.




:::

<a href="exploratory-data-analysis.qmd" class="previous-page-link" style="float: left;">&larr; Previous Page: EDA</a>



<a href="multivariate-ts-models.qmd" class="next-page-link" style="float: right;">Next Page: Multivariate &rarr;</a>




